"""
Life Science Content Gap Analysis Pipeline
==========================================
Inputs:
  --keywords   keywords.csv       : keyword, volume (opt), impressions, clicks, position
  --serp       serp_data.csv      : keyword, title_1..3, url_1..3, paa_1..3 (opt),
                                    snippet (opt), date_1..3 (opt)
  --output     ./output

Outputs:
  clusters.csv        — every keyword fully enriched + scored
  gaps.csv            — keywords flagged as specific gap types
  recommendations.csv — research areas ranked by opportunity
  report.html         — interactive visual summary

Pipeline stages:
  1. Load & merge all inputs
  2. Enrich: keyword + SERP titles → combined embedding text
  3. Embed enriched text (free local model)
  4. Map to research area taxonomy (cosine similarity)
  5. Sub-topic clustering (Agglomerative / KMeans)
  6. Competitor landscape (domain diversity, academic lock-in)
  7. Gap analysis (absence, performance, freshness, format, PAA)
  8. Opportunity scoring & ranking
  9. Report generation

Dependencies:
  pip install sentence-transformers scikit-learn pandas numpy plotly tldextract
"""

import argparse
import os
import re
import warnings
from collections import Counter

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


# ═══════════════════════════════════════════════════════════════════════
# RESEARCH AREA TAXONOMY
# ═══════════════════════════════════════════════════════════════════════

RESEARCH_AREAS = [

    # ── TIER 1: Primary research areas ────────────────────────────────

    {
        "name": "Neuroscience",
        "tier": 1,
        "description": (
            "neuroscience brain neuron synapse neural circuit neurodegeneration "
            "Alzheimer disease Parkinson disease ALS amyotrophic lateral sclerosis "
            "multiple sclerosis epilepsy stroke dementia cognitive function memory "
            "learning plasticity blood brain barrier glia astrocyte microglia "
            "oligodendrocyte neuroinflammation CNS peripheral nervous system "
            "axon dendrite action potential neurotransmitter dopamine serotonin "
            "GABA glutamate acetylcholine optogenetics electrophysiology patch clamp "
            "fMRI EEG brain imaging connectome neural network pain nociception "
            "spinal cord sensory motor cortex hippocampus cerebellum thalamus "
            "neurogenesis synaptic plasticity long-term potentiation LTP "
            "tau amyloid alpha-synuclein TDP-43 FUS neuronal death "
            "depression anxiety schizophrenia ADHD autism spectrum disorder"
        ),
    },

    {
        "name": "Oncology",
        "tier": 1,
        "description": (
            "oncology cancer tumor malignancy carcinoma sarcoma lymphoma leukemia "
            "melanoma metastasis invasion angiogenesis tumor microenvironment "
            "cancer stem cell clonal evolution drug resistance "
            "chemotherapy radiation targeted therapy kinase inhibitor "
            "KRAS EGFR HER2 BRAF ALK RET FGFR PIK3CA PTEN TP53 RB1 "
            "breast cancer lung cancer colorectal cancer prostate cancer "
            "ovarian cancer pancreatic cancer glioblastoma GBM hepatocellular "
            "bladder cancer renal cell carcinoma thyroid cancer "
            "tumor suppressor oncogene cell proliferation apoptosis senescence "
            "liquid biopsy ctDNA circulating tumor cell cancer biomarker "
            "CDK4 CDK6 mTOR VEGF RAF MEK ERK PI3K AKT Wnt Notch Hedgehog "
            "PARP inhibitor proteasome ubiquitin DNA damage repair homologous recombination"
        ),
    },

    {
        "name": "Immunology",
        "tier": 1,
        "description": (
            "immunology immune system innate immunity adaptive immunity "
            "T cell B cell NK cell dendritic cell macrophage neutrophil "
            "monocyte mast cell basophil eosinophil plasma cell "
            "antibody immunoglobulin IgG IgM IgE IgA "
            "cytokine interleukin TNF interferon chemokine "
            "IL-1 IL-2 IL-4 IL-6 IL-10 IL-12 IL-17 IL-23 TGF-beta "
            "MHC HLA antigen presentation TCR BCR "
            "CD4 CD8 CD19 CD20 CD28 CTLA-4 PD-1 LAG-3 TIM-3 "
            "regulatory T cell Treg Th1 Th2 Th17 follicular helper T cell "
            "germinal center affinity maturation somatic hypermutation "
            "complement system toll-like receptor NLR inflammasome NLRP3 "
            "autoimmunity rheumatoid arthritis lupus SLE inflammatory bowel disease "
            "allergy atopic dermatitis asthma type 1 diabetes multiple sclerosis "
            "immunosuppression transplant rejection GVHD "
            "vaccine adjuvant humoral immunity cellular immunity memory"
        ),
    },

    # ── TIER 2: Secondary research areas ──────────────────────────────

    {
        "name": "Immuno-Oncology",
        "tier": 2,
        "description": (
            "immuno-oncology cancer immunotherapy immune checkpoint "
            "PD-1 PD-L1 CTLA-4 LAG-3 TIM-3 TIGIT checkpoint blockade "
            "CAR-T cell therapy chimeric antigen receptor adoptive cell therapy "
            "tumor infiltrating lymphocyte TIL natural killer cell NK cell therapy "
            "bispecific antibody T cell engager BiTE blinatumomab "
            "tumor microenvironment immunosuppression myeloid derived suppressor "
            "cancer vaccine neoantigen personalized immunotherapy "
            "anti-tumor immunity immune evasion antigen presentation MHC "
            "response rate overall survival progression free survival "
            "combination therapy anti-PD1 anti-CTLA4 nivolumab pembrolizumab "
            "ipilimumab atezolizumab durvalumab avelumab cemiplimab "
            "cytokine release syndrome immune related adverse events irAE "
            "solid tumor hematologic malignancy melanoma NSCLC renal cell "
            "MSI-H TMB tumor mutational burden dMMR mismatch repair"
        ),
    },

    {
        "name": "Immunology & Infectious Disease",
        "tier": 2,
        "description": (
            "infectious disease host immune response pathogen infection "
            "bacteria virus fungus parasite antimicrobial resistance "
            "HIV AIDS influenza SARS-CoV-2 COVID-19 tuberculosis TB "
            "malaria dengue Zika Ebola RSV hepatitis sepsis bacteremia "
            "antibiotic resistance AMR MRSA carbapenem-resistant "
            "innate immune response pattern recognition receptor "
            "toll-like receptor TLR NOD-like receptor inflammasome "
            "interferon antiviral response NK cell macrophage activation "
            "vaccine efficacy adjuvant mucosal immunity IgA neutralizing antibody "
            "microbiome gut flora dysbiosis colonization resistance "
            "antifungal antiviral antiparasitic treatment "
            "epidemiology outbreak transmission zoonotic spillover "
            "T cell response CD8 cytotoxic lymphocyte viral clearance "
            "humoral immunity B cell memory immune evasion pathogen virulence"
        ),
    },

    {
        "name": "Metabolism",
        "tier": 2,
        "description": (
            "metabolism metabolic pathway energy homeostasis "
            "glucose metabolism glycolysis gluconeogenesis glycogen "
            "lipid metabolism fatty acid oxidation lipogenesis lipolysis "
            "cholesterol bile acid triglyceride HDL LDL VLDL "
            "amino acid metabolism protein catabolism urea cycle "
            "mitochondria oxidative phosphorylation electron transport chain "
            "TCA cycle citric acid Krebs cycle ATP production "
            "insulin signaling insulin resistance type 2 diabetes "
            "obesity adipose tissue adipogenesis adipokine leptin adiponectin "
            "mTOR AMPK SIRT1 PPARgamma PGC-1alpha FoxO "
            "liver metabolism hepatic gluconeogenesis NAFLD NASH "
            "gut microbiome metabolite short chain fatty acid bile acid "
            "cancer metabolism Warburg effect aerobic glycolysis glutamine "
            "one-carbon metabolism serine folate methionine "
            "nutrient sensing caloric restriction fasting ketogenesis "
            "metabolomics mass spectrometry flux analysis isotope tracing"
        ),
    },

    {
        "name": "Epigenetics",
        "tier": 2,
        "description": (
            "epigenetics epigenomics chromatin remodeling "
            "DNA methylation CpG island methyltransferase DNMT TET "
            "histone modification histone acetylation histone methylation "
            "H3K4me3 H3K27me3 H3K9me3 H3K27ac H3K4me1 H3K36me3 "
            "histone acetyltransferase HAT histone deacetylase HDAC "
            "histone methyltransferase HMT histone demethylase "
            "chromatin accessibility ATAC-seq DNase-seq "
            "ChIP-seq CUT&RUN CUT&TAG chromatin immunoprecipitation "
            "polycomb group trithorax group PRC1 PRC2 EZH2 "
            "bromodomain BET BRD4 JMJD KDM "
            "non-coding RNA lncRNA microRNA miRNA piRNA "
            "X-inactivation genomic imprinting "
            "3D genome topologically associating domain TAD loop "
            "Hi-C cohesin CTCF enhancer promoter "
            "epigenetic reprogramming cell fate pluripotency "
            "cancer epigenetics epigenetic therapy HDAC inhibitor "
            "single cell epigenomics scATAC-seq multiomics"
        ),
    },

    {
        "name": "Cardiovascular",
        "tier": 2,
        "description": (
            "cardiovascular heart cardiac vascular endothelium "
            "coronary artery disease myocardial infarction heart attack "
            "heart failure cardiomyopathy atrial fibrillation arrhythmia "
            "hypertension blood pressure atherosclerosis plaque "
            "stroke cerebrovascular thrombosis clot coagulation "
            "lipid cholesterol PCSK9 statin LDL HDL triglyceride "
            "platelet aggregation anticoagulant antiplatelet "
            "troponin BNP NT-proBNP cardiac biomarker "
            "cardiac fibrosis remodeling ventricular hypertrophy "
            "valve disease aortic stenosis mitral regurgitation "
            "peripheral artery disease aortic aneurysm "
            "cardiac metabolism energy substrate fatty acid glucose "
            "cardiomyocyte contractility calcium handling "
            "endothelial dysfunction nitric oxide eNOS "
            "RAAS angiotensin ACE inhibitor ARB beta blocker "
            "cardiac regeneration progenitor cell "
            "single cell RNA-seq spatial transcriptomics heart"
        ),
    },

    {
        "name": "Cellular Organization & Processes",
        "tier": 2,
        "description": (
            "cell biology cellular organization organelle "
            "cell membrane cytoskeleton actin tubulin microtubule "
            "nucleus nuclear pore lamina chromatin organization "
            "endoplasmic reticulum ER stress unfolded protein response UPR "
            "Golgi apparatus vesicle trafficking secretory pathway "
            "lysosome autophagy mitophagy selective autophagy "
            "mitochondria fission fusion dynamics biogenesis "
            "proteasome ubiquitin proteasome system UPS "
            "cell signaling pathway kinase phosphatase signal transduction "
            "MAPK PI3K AKT mTOR Wnt Notch Hedgehog Hippo YAP TAZ "
            "cell cycle G1 S G2 M phase CDK cyclin checkpoint "
            "apoptosis caspase BCL2 BAX cytochrome c "
            "necroptosis pyroptosis ferroptosis cell death "
            "cell migration invasion actin dynamics focal adhesion "
            "cell polarity tight junction adherens junction "
            "extracellular matrix ECM collagen fibronectin integrin "
            "mechanobiology mechanotransduction force sensing "
            "phase separation liquid-liquid condensate biomolecular "
            "protein quality control chaperone HSP70 HSP90 "
            "super resolution microscopy live cell imaging CRISPR screen"
        ),
    },

    {
        "name": "Stem Cells",
        "tier": 2,
        "description": (
            "stem cell pluripotent embryonic stem cell ESC "
            "induced pluripotent stem cell iPSC reprogramming Yamanaka "
            "Oct4 Sox2 Klf4 c-Myc pluripotency transcription factor "
            "differentiation lineage commitment cell fate specification "
            "hematopoietic stem cell HSC bone marrow transplant "
            "mesenchymal stem cell MSC stromal cell "
            "neural stem cell progenitor neurogenesis "
            "intestinal stem cell Lgr5 organoid gut epithelium "
            "cancer stem cell tumor initiating cell "
            "self-renewal symmetric asymmetric division niche "
            "Wnt Notch Hedgehog BMP FGF signaling stem cell "
            "epigenetic regulation of stem cell H3K27me3 bivalent "
            "single cell RNA sequencing scRNA-seq trajectory pseudotime "
            "organoid 3D culture patient-derived organoid PDO "
            "regenerative medicine cell therapy gene therapy "
            "CRISPR base editing prime editing stem cell engineering "
            "tissue engineering scaffold biomaterial "
            "clonal dynamics lineage tracing barcode"
        ),
    },
]

# ── Academic / journal signals detected from SERP titles (no URLs needed)
ACADEMIC_TITLE_SIGNALS = [
    # Journals & publishers
    "nature", "science", "cell", "lancet", "nejm", "new england journal",
    "jama", "bmj", "plos", "pnas", "pubmed", "ncbi", "nih", "nci",
    "frontiers", "wiley", "springer", "elsevier", "oxford", "cambridge",
    "annals", "journal of", "american journal", "european journal",
    "proceedings", "clinical cancer research", "cancer research",
    # Institutional
    "fda", "cdc", "who", "ema", "mayo clinic", "cleveland clinic",
    "harvard", "stanford", "mit", "johns hopkins", "oxford university",
    "cancer center", "medical center", "university hospital",
    # Content signals
    "systematic review", "meta-analysis", "randomized controlled",
    "clinical trial", "case report", "cohort study", "phase 1", "phase 2", "phase 3",
]

FORMAT_SIGNALS = {
    "listicle":  r"\b(top \d+|\d+ best|\d+ ways|\d+ tips|list of)\b",
    "guide":     r"\b(guide|how to|tutorial|walkthrough|step.by.step)\b",
    "overview":  r"\b(overview|introduction|what is|explained|understanding)\b",
    "comparison":r"\b(vs\.?|versus|compare|comparison|difference between)\b",
    "clinical":  r"\b(clinical|trial|study|results|efficacy|safety|phase [123])\b",
    "news":      r"\b(new|latest|recent|update|breakthrough|announces?)\b",
}


# ═══════════════════════════════════════════════════════════════════════
# 0. CLI
# ═══════════════════════════════════════════════════════════════════════

def parse_args():
    p = argparse.ArgumentParser(description="Life Science Content Gap Analysis Pipeline")
    p.add_argument("--keywords", required=True,
                   help="CSV: keyword, [volume], [impressions], [clicks], [position]")
    p.add_argument("--serp",     required=True,
                   help="CSV: keyword, title_1..3, [paa_1..3], [snippet]")
    p.add_argument("--output",   default="./output")
    p.add_argument("--kw-col",   default="keyword",
                   help="Keyword column name in keywords CSV (default: 'keyword')")
    p.add_argument("--threshold", type=float, default=0.20,
                   help="Min cosine sim to assign research area (default: 0.20)")
    p.add_argument("--min-cluster", type=int, default=3,
                   help="Min keywords per sub-topic cluster (default: 3)")
    p.add_argument("--top-n",    type=int, default=20,
                   help="Top N research areas in recommendations (default: 20)")
    return p.parse_args()


# ═══════════════════════════════════════════════════════════════════════
# 1. LOAD & MERGE
# ═══════════════════════════════════════════════════════════════════════

def _normalise_keyword(s: str) -> str:
    """
    Robust keyword normalisation for reliable joining.
    Handles: mixed case, extra whitespace, smart quotes, unicode punctuation,
    trailing punctuation, and common encoding artifacts.
    """
    if not isinstance(s, str):
        s = str(s)
    s = s.lower().strip()
    # Replace smart quotes and curly apostrophes
    s = s.replace("\u2018", "'").replace("\u2019", "'")
    s = s.replace("\u201c", '"').replace("\u201d", '"')
    # Collapse multiple spaces/tabs
    s = re.sub(r"\s+", " ", s)
    # Remove trailing punctuation that GSC sometimes appends
    s = s.rstrip(".,;:")
    return s


# ═══════════════════════════════════════════════════════════════════════
# KEYWORD FILTER
# ═══════════════════════════════════════════════════════════════════════
#
# Add terms to filter out here. Three filter types:
#
# BRAND_TERMS   — exact brand names and common misspellings.
#                 Any keyword CONTAINING one of these strings is removed.
#                 All matching is case-insensitive after normalisation.
#
# NAVIGATIONAL  — site names, databases, tools people navigate to directly.
#                 Same substring match logic as BRAND_TERMS.
#
# REGEX_FILTERS — full regex patterns for anything more complex
#                 (e.g. product codes, catalog numbers, email addresses).
#
# To add more: just append to the relevant list.
# ─────────────────────────────────────────────────────────────────────

BRAND_TERMS = [
    # Abcam and misspellings
    "abcam", "abcamm", "abcam's", "abcamn", "ab cam",
    # Add competitor brands below as you find them:
    # "thermo fisher", "thermofisher", "thermosfisher",
    # "cell signaling", "cst antibody",
    # "biolegend", "bio-legend",
    # "miltenyi", "milteny",
    # "r&d systems", "r and d systems",
    # "santa cruz", "santacruz",
]

NAVIGATIONAL_TERMS = [
    # Databases and portals people search by name
    "pubmed", "ncbi", "uniprot", "ensembl", "kegg", "reactome",
    "gene ontology", "gene cards", "genecards",
    "clinicaltrials.gov", "clinical trials gov",
    "pdb", "rcsb",
    # Add more as needed:
    # "string db", "biogrid", "intact",
]

REGEX_FILTERS = [
    # Product/catalog codes: letters + 4+ digits e.g. ab12345, PA5-12345
    r"^[a-z]{1,4}[\-]?\d{4,}$",
    # Pure numeric strings
    r"^\d+$",
    # Email addresses
    r"[a-z0-9._%+\-]+@[a-z0-9.\-]+\.[a-z]{2,}",
    # Add more patterns as needed:
    # r"^lot\s+\d+",          # lot numbers
    # r"cas\s+\d+[\-]\d+",    # CAS registry numbers
]

# Compile regex filters once at import time
_COMPILED_REGEX = [re.compile(p) for p in REGEX_FILTERS]


def _should_filter_keyword(kw: str) -> tuple[bool, str]:
    """
    Returns (True, reason) if keyword should be filtered, (False, "") otherwise.
    kw must already be normalised (lowercase, stripped).
    """
    # Brand/navigational substring match
    for term in BRAND_TERMS:
        if term in kw:
            return True, f"brand: '{term}'"

    for term in NAVIGATIONAL_TERMS:
        if term in kw:
            return True, f"navigational: '{term}'"

    # Regex patterns
    for pattern in _COMPILED_REGEX:
        if pattern.search(kw):
            return True, f"pattern: {pattern.pattern}"

    return False, ""


def apply_keyword_filter(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """
    Apply all keyword filters to a dataframe with a 'keyword' column.
    Logs what was removed and why.
    """
    before = len(df)

    filter_results = df["keyword"].apply(_should_filter_keyword)
    filtered_mask  = filter_results.apply(lambda x: x[0])
    reasons        = filter_results.apply(lambda x: x[1])

    removed = df[filtered_mask].copy()
    kept    = df[~filtered_mask].copy()

    if verbose and len(removed) > 0:
        reason_counts = reasons[filtered_mask].value_counts()
        print(f"    → Filtered {len(removed):,} keywords ({before - len(removed):,} remain)")
        print("    → Filter breakdown:")
        for reason, count in reason_counts.head(10).items():
            print(f"         {count:>5}  {reason}")
        if len(removed) <= 20:
            print(f"    → Removed: {', '.join(removed['keyword'].tolist())}")
        else:
            print(f"    → Sample removed: "
                  f"{', '.join(removed['keyword'].head(10).tolist())} ...")
    elif verbose:
        print(f"    → No keywords filtered ({before:,} remain)")

    return kept


def load_and_merge(kw_path: str, serp_path: str, kw_col: str) -> pd.DataFrame:
    print("\n[1/10] Loading and merging inputs...")

    # ── Keywords (GSC data — contains impressions, clicks, position)
    kw = pd.read_csv(kw_path, dtype=str)   # read as str first — avoids numeric coercion
    if kw_col not in kw.columns:
        raise ValueError(f"Column '{kw_col}' not found in keywords file. "
                         f"Available: {list(kw.columns)}")
    kw = kw.rename(columns={kw_col: "keyword"})
    kw["keyword"]     = kw["keyword"].apply(_normalise_keyword)
    kw["keyword_raw"] = kw["keyword"]   # keep for diagnostics
    kw = kw.drop_duplicates("keyword").reset_index(drop=True)

    # ── Apply keyword filter (brands, navigational, product codes)
    print("    → Applying keyword filters...")
    kw = apply_keyword_filter(kw, verbose=True)

    # Coerce numeric columns NOW before merge so we don't lose them
    for col, default in [("impressions", 0), ("clicks", 0),
                          ("position", 999.0), ("volume", 0)]:
        if col in kw.columns:
            kw[col] = pd.to_numeric(kw[col], errors="coerce").fillna(default)
        else:
            kw[col] = default

    print(f"    → {len(kw):,} keywords loaded")
    print(f"    → GSC coverage: "
          f"{(kw['impressions'] > 0).sum():,} with impressions, "
          f"{(kw['impressions'] == 0).sum():,} with zero impressions")

    # ── SERP data
    serp = pd.read_csv(serp_path, dtype=str)
    serp.columns = [c.strip().lower().replace(" ", "_") for c in serp.columns]

    for candidate in ("keyword", "query", "search_term", "term"):
        if candidate in serp.columns:
            serp = serp.rename(columns={candidate: "keyword"})
            break

    if "keyword" not in serp.columns:
        raise ValueError(f"No keyword column found in SERP file. "
                         f"Available: {list(serp.columns)}")

    serp["keyword"] = serp["keyword"].apply(_normalise_keyword)
    serp = serp.drop_duplicates("keyword").reset_index(drop=True)
    print(f"    → {len(serp):,} SERP rows loaded")

    # ── Diagnose join overlap BEFORE merging
    kw_set   = set(kw["keyword"])
    serp_set = set(serp["keyword"])
    overlap  = kw_set & serp_set
    kw_only  = kw_set - serp_set
    serp_only= serp_set - kw_set

    print(f"\n    Join diagnostics:")
    print(f"      ✓ {len(overlap):,} keywords match in both files")
    print(f"      ✗ {len(kw_only):,} keywords in GSC with no SERP data")
    print(f"      ~ {len(serp_only):,} SERP keywords not in GSC list")

    if len(overlap) == 0:
        print("\n    ⚠️  WARNING: Zero keyword overlap — check column names and encoding.")
        print(f"      GSC sample:  {list(kw['keyword'])[:5]}")
        print(f"      SERP sample: {list(serp['keyword'])[:5]}")
    elif len(overlap) < len(kw_set) * 0.5:
        print(f"\n    ⚠️  WARNING: Only {len(overlap)/len(kw_set)*100:.0f}% match rate.")
        print(f"      GSC sample:  {list(kw['keyword'])[:3]}")
        print(f"      SERP sample: {list(serp['keyword'])[:3]}")

    # ── Merge — left join keeps all GSC keywords
    df = kw.merge(serp, on="keyword", how="left")

    # ── Verify impressions survived the merge
    with_impressions_before = (kw["impressions"] > 0).sum()
    with_impressions_after  = (df["impressions"] > 0).sum()
    if with_impressions_after < with_impressions_before * 0.95:
        print(f"\n    ⚠️  Impression loss detected: "
              f"{with_impressions_before:,} → {with_impressions_after:,} after merge. "
              f"Check for duplicate keywords in keywords.csv causing row multiplication.")

    serp_col_check = "title_1" if "title_1" in df.columns else \
                     ("enriched_text" if "enriched_text" in df.columns else None)
    n_with_serp = df[serp_col_check].notna().sum() if serp_col_check else 0
    print(f"\n    → {len(df):,} rows after merge | "
          f"{n_with_serp:,} with SERP data | "
          f"{(df['impressions'] > 0).sum():,} with impressions")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 2. ENRICH: keyword + SERP titles → embedding text
# ═══════════════════════════════════════════════════════════════════════

def enrich_text(df: pd.DataFrame) -> pd.DataFrame:
    """
    Two modes depending on what your SERP CSV contains:

    MODE A — Pre-built from BigQuery (preferred):
      If your serp_data.csv already has an 'enriched_text' column
      (keyword + all page-2 titles concatenated in BQ), use it directly.
      Also uses 'all_paa_questions' column if present.

    MODE B — Build from wide columns (fallback):
      If your CSV has title_1, title_2 ... title_N columns, concatenates
      ALL of them (no 3-title cap) plus all paa_1 ... paa_N columns.

    The enriched_text is what gets embedded — it encodes Google's full
    interpretation of each keyword across up to 20 organic results.
    """
    print("\n[2/8] Enriching keywords with SERP titles...")

    # ── MODE A: pre-built enriched_text column from BigQuery
    if "enriched_text" in df.columns:
        # Fill any blanks with bare keyword
        df["enriched_text"] = df.apply(
            lambda row: row["enriched_text"]
            if pd.notna(row["enriched_text"]) and str(row["enriched_text"]).strip()
            else row["keyword"],
            axis=1
        )
        enriched = (df["enriched_text"].str.count(r"\|") > 0).sum()
        print(f"    → Using pre-built enriched_text column (BigQuery mode)")
        print(f"    → {enriched}/{len(df)} keywords have SERP title context")

        # PAA: use pre-built all_paa_questions column if present
        if "all_paa_questions" in df.columns:
            df["paa_questions"] = df["all_paa_questions"].fillna("")
            paa_count = (df["paa_questions"].str.strip() != "").sum()
            print(f"    → {paa_count}/{len(df)} keywords have PAA questions")
        else:
            df["paa_questions"] = ""

        return df

    # ── MODE B: build from wide title_N / paa_N columns
    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)],
                        key=lambda c: int(re.search(r"\d+", c).group()))
    paa_cols   = sorted([c for c in df.columns if re.match(r"paa_\d+",   c)],
                        key=lambda c: int(re.search(r"\d+", c).group()))

    print(f"    → Building from {len(title_cols)} title columns + {len(paa_cols)} PAA columns")

    def build_text(row):
        parts = [str(row["keyword"])]
        for col in title_cols:          # ALL title columns, no cap
            val = row.get(col, "")
            if pd.notna(val) and str(val).strip():
                parts.append(str(val).strip())
        return " | ".join(parts)

    def build_paa(row):
        questions = []
        for col in paa_cols:            # ALL PAA columns, no cap
            val = row.get(col, "")
            if pd.notna(val) and str(val).strip():
                questions.append(str(val).strip())
        return " | ".join(questions)

    df["enriched_text"] = df.apply(build_text, axis=1)
    df["paa_questions"]  = df.apply(build_paa,  axis=1)

    enriched  = (df["enriched_text"].str.count(r"\|") > 0).sum()
    paa_count = (df["paa_questions"].str.strip() != "").sum()
    print(f"    → {enriched}/{len(df)} keywords enriched with SERP titles")
    print(f"    → {len(df)-enriched} keywords using bare keyword only")
    print(f"    → {paa_count}/{len(df)} keywords have PAA questions")
    return df


# ═══════════════════════════════════════════════════════════════════════
# 3. EMBED
# ═══════════════════════════════════════════════════════════════════════

def embed_all(df: pd.DataFrame):
    print("\n[3/8] Generating embeddings...")
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        raise ImportError("Run: pip install sentence-transformers")

    model = SentenceTransformer("all-MiniLM-L6-v2")

    print("      Embedding enriched keyword texts...")
    kw_emb = model.encode(
        df["enriched_text"].tolist(),
        show_progress_bar=True,
        batch_size=64,
        normalize_embeddings=True
    )

    print("      Embedding research area taxonomy...")
    area_emb = model.encode(
        [a["description"] for a in RESEARCH_AREAS],
        show_progress_bar=False,
        normalize_embeddings=True
    )

    print(f"    → Keyword embeddings: {kw_emb.shape} | Area embeddings: {area_emb.shape}")
    return kw_emb, area_emb


# ═══════════════════════════════════════════════════════════════════════
# 4. RESEARCH AREA MAPPING
# ═══════════════════════════════════════════════════════════════════════

def map_research_areas(df: pd.DataFrame, kw_emb: np.ndarray,
                        area_emb: np.ndarray, threshold: float) -> pd.DataFrame:
    print(f"\n[4/8] Mapping to research areas (threshold={threshold})...")

    sim        = kw_emb @ area_emb.T          # (n_kw, n_areas)
    best_idx   = sim.argmax(axis=1)
    best_score = sim.max(axis=1)

    sim2       = sim.copy()
    sim2[np.arange(len(sim2)), best_idx] = -1
    second_idx = sim2.argmax(axis=1)

    area_names = [a["name"] for a in RESEARCH_AREAS]

    df["research_area"]       = [
        area_names[i] if s >= threshold else "Unclassified / General Methods"
        for i, s in zip(best_idx, best_score)
    ]
    df["research_area_score"] = best_score.round(4)
    df["research_area_2"]     = [area_names[i] for i in second_idx]

    classified = (df["research_area"] != "Unclassified / General Methods").sum()
    print(f"    → {classified}/{len(df)} keywords classified")

    print("\n    Research area keyword distribution:")
    for area, cnt in df["research_area"].value_counts().items():
        bar = "█" * min(35, max(1, cnt * 35 // len(df)))
        print(f"      {cnt:>5}  {bar}  {area}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 5. SUB-TOPIC CLUSTERING
# ═══════════════════════════════════════════════════════════════════════

def cluster_keywords(df: pd.DataFrame, kw_emb: np.ndarray,
                     min_cluster: int) -> pd.DataFrame:
    print("\n[5/8] Sub-topic clustering...")

    from sklearn.decomposition import TruncatedSVD
    from sklearn.cluster import AgglomerativeClustering, KMeans
    from sklearn.manifold import TSNE
    from sklearn.preprocessing import normalize
    from sklearn.metrics import pairwise_distances_argmin_min

    n      = len(df)
    n_comp = min(50, kw_emb.shape[1] - 1, n - 1)
    reduced = normalize(TruncatedSVD(n_comp, random_state=42).fit_transform(kw_emb))

    k = max(5, min(80, n // 8))
    print(f"    → k={k} clusters for {n} keywords")

    labels = (
        AgglomerativeClustering(n_clusters=k, metric="cosine", linkage="average")
        .fit_predict(reduced)
        if n <= 5000 else
        KMeans(n_clusters=k, random_state=42, n_init="auto").fit_predict(reduced)
    )

    # Merge tiny clusters
    counts    = pd.Series(labels).value_counts()
    small     = counts[counts < min_cluster].index.tolist()
    if small:
        vm = ~np.isin(labels, small)
        sm =  np.isin(labels, small)
        if vm.sum() > 0 and sm.sum() > 0:
            nearest, _ = pairwise_distances_argmin_min(
                reduced[sm], reduced[vm], metric="cosine")
            labels = labels.copy()
            labels[sm] = labels[vm][nearest]
        print(f"    → Merged {len(small)} tiny clusters")

    # Label each cluster by centroid-closest keyword
    label_map = {}
    for cid in np.unique(labels):
        mask     = labels == cid
        embs     = reduced[mask]
        centroid = embs.mean(axis=0)
        closest  = df.loc[mask, "keyword"].iloc[(embs @ centroid).argmax()]
        label_map[cid] = closest

    df["cluster_id"]    = labels
    df["cluster_label"] = [label_map[c] for c in labels]
    print(f"    → {len(label_map)} sub-topic clusters produced")

    # t-SNE for visualization
    print("    → Running t-SNE...")
    perp = min(30, max(5, n // 10))
    from sklearn.manifold import TSNE
    xy = TSNE(2, perplexity=perp, learning_rate="auto",
              init="pca", random_state=42).fit_transform(reduced)
    df["tsne_x"] = xy[:, 0]
    df["tsne_y"] = xy[:, 1]

    return df


# ═══════════════════════════════════════════════════════════════════════
# 6. COMPETITOR LANDSCAPE
# ═══════════════════════════════════════════════════════════════════════

def analyze_competitor_landscape(df: pd.DataFrame) -> pd.DataFrame:
    """
    Infer competitor landscape from SERP titles alone — no URLs needed.

    Per keyword computes:
      academic_lock    : 0-1 — how many of the 3 titles signal journal/institutional content
                         High = PubMed/Nature/clinical trial dominated = hard to displace
      dominant_format  : content format the SERP rewards (guide/overview/clinical/news/other)
      serp_diversity   : are titles semantically varied (good) or all saying the same thing?
                         Proxy: count distinct first words across titles

    Why titles are better than URLs here:
      - "Nature Communications | EZH2 inhibition in cancer" tells you more than a URL ever would
      - Titles are clean, always present, and directly reflect what Google is rewarding
      - Academic lock-in is highly visible in titles: journal name, study type, institution
    """
    print("\n[6/8] Analyzing competitor landscape from SERP titles...")

    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])

    academic_lock_list   = []
    dominant_format_list = []
    serp_diversity_list  = []

    for _, row in df.iterrows():
        titles = [
            str(row.get(col, "")).strip().lower()
            for col in title_cols[:3]
            if pd.notna(row.get(col)) and str(row.get(col, "")).strip()
        ]

        if not titles:
            academic_lock_list.append(0.5)    # unknown — neutral
            dominant_format_list.append("unknown")
            serp_diversity_list.append(0.5)
            continue

        all_titles_text = " ".join(titles)

        # ── Academic lock-in: count how many titles contain academic signals
        acad_hits = sum(
            1 for title in titles
            if any(sig in title for sig in ACADEMIC_TITLE_SIGNALS)
        )
        academic_lock_list.append(round(acad_hits / len(titles), 2))

        # ── Dominant format from title language
        fmt_hits = {
            fmt: bool(re.search(pat, all_titles_text))
            for fmt, pat in FORMAT_SIGNALS.items()
        }
        # clinical takes priority for life science (most distinctive)
        dominant = "other"
        for fmt in ("clinical", "guide", "overview", "listicle", "comparison", "news"):
            if fmt_hits.get(fmt):
                dominant = fmt
                break
        dominant_format_list.append(dominant)

        # ── SERP diversity: distinct first words = titles covering different angles
        first_words = {t.split()[0] for t in titles if t}
        serp_diversity_list.append(round(len(first_words) / max(len(titles), 1), 2))

    df["academic_lock"]   = academic_lock_list
    df["dominant_format"] = dominant_format_list
    df["serp_diversity"]  = serp_diversity_list

    avg_lock = df["academic_lock"].mean()
    pct_clinical = (df["dominant_format"] == "clinical").mean() * 100
    print(f"    → Avg academic lock-in:  {avg_lock:.2f}  (0=open, 1=journal dominated)")
    print(f"    → Clinical format SERPs: {pct_clinical:.1f}%")
    print(f"    → Format breakdown:")
    for fmt, cnt in df["dominant_format"].value_counts().items():
        print(f"         {fmt:<15} {cnt:>5}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 7. GAP ANALYSIS
# ═══════════════════════════════════════════════════════════════════════

def analyze_gaps(df: pd.DataFrame) -> pd.DataFrame:
    """
    Four gap types scored at keyword level (URLs and dates removed):

    ABSENCE GAP     — impressions == 0 → no content presence at all
    PERFORMANCE GAP — impressions > 0 but position > 10 OR CTR < 2%
    FORMAT GAP      — SERP rewards accessible content (guide/overview)
                      but academic lock is low and you're not ranking
                      → opportunity to win with well-structured content
    PAA GAP         — PAA questions exist for this keyword but you're
                      not in top 10 → specific content angle opportunity
    SNIPPET GAP     — featured snippet exists but you don't own it

    Composite gap_score = weighted sum (0-1).
    """
    print("\n[7/8] Running gap analysis...")

    # ── CTR
    df["ctr"] = np.where(
        df["impressions"] > 0,
        (df["clicks"] / df["impressions"]).round(4),
        0.0
    )

    # ── Absence gap: truly no content presence
    # position == 999 means keyword never appeared in GSC at all
    # (our default for keywords with no GSC data after merge)
    # Zero impressions alone is NOT absence — GSC sometimes shows position
    # for keywords with 0 impressions due to aggregation thresholds
    df["gap_absence"] = df["position"] >= 999

    # ── Performance gap: Google knows you exist but you're losing
    # Covers: ranking 11-100, OR visible but CTR too low to matter
    df["gap_performance"] = (
        (df["position"] < 999) &           # GSC has seen you for this keyword
        (df["position"] > 10) |            # but you're off page 1
        (
            (df["impressions"] > 0) &      # OR you're getting impressions
            (df["ctr"] < 0.02) &           # but very low CTR
            (df["position"] <= 10)         # even though you're on page 1
        )
    )

    # ── Format gap: SERP rewards accessible formats (guide/overview)
    #    AND academic lock is low (meaning format actually matters here)
    #    AND you're not ranking — so a well-structured article could win
    df["gap_format"] = (
        (df["dominant_format"].isin(["guide", "overview", "listicle"])) &
        (df["academic_lock"] < 0.4) &
        (df["position"] > 10)
    )

    # ── PAA gap: uses pre-built paa_questions column from enrich_text stage
    #    (covers all PAA questions from BQ or all paa_N columns — no cap)
    if "paa_questions" not in df.columns:
        df["paa_questions"] = ""

    has_paa = df["paa_questions"].str.strip() != ""
    df["gap_paa"] = has_paa & (df["position"] > 10)

    # ── Snippet gap: snippet exists but you don't own it
    if "snippet" in df.columns:
        df["gap_snippet"] = (
            df["snippet"].notna() &
            (df["snippet"].astype(str).str.strip() != "") &
            (df["position"] > 5)
        )
    else:
        df["gap_snippet"] = False

    # ── Composite gap score
    weights = {
        "gap_absence":     0.40,   # highest — no presence = biggest gap
        "gap_performance": 0.30,   # second — visible but losing
        "gap_paa":         0.15,   # question angle opportunity
        "gap_format":      0.10,   # content type opportunity
        "gap_snippet":     0.05,   # featured snippet opportunity
    }
    df["gap_score"] = sum(
        df[col].astype(float) * w for col, w in weights.items()
    ).round(3)

    # ── Primary gap label for quick reading
    def primary_gap(row):
        if row["gap_absence"]:     return "Absence"
        if row["gap_performance"]: return "Performance"
        if row["gap_paa"]:         return "PAA"
        if row["gap_format"]:      return "Format"
        if row["gap_snippet"]:     return "Snippet"
        return "None"

    df["primary_gap"] = df.apply(primary_gap, axis=1)

    # ── Summary
    gap_counts = {
        "Absence":     int(df["gap_absence"].sum()),
        "Performance": int(df["gap_performance"].sum()),
        "PAA":         int(df["gap_paa"].sum()),
        "Format":      int(df["gap_format"].sum()),
        "Snippet":     int(df["gap_snippet"].sum()),
    }
    print("\n    Gap type breakdown:")
    for gtype, cnt in gap_counts.items():
        pct = cnt / len(df) * 100
        bar = "█" * min(30, int(pct / 2))
        print(f"      {gtype:<15} {cnt:>5}  ({pct:5.1f}%)  {bar}")

    return df

    df["paa_questions"] = df[paa_cols].apply(
        lambda row: " | ".join(
            str(v) for v in row if pd.notna(v) and str(v).strip()
        ), axis=1
    ) if paa_cols else ""

    df["gap_paa"] = has_paa & (df["position"] > 10)

    # ── Snippet gap: featured snippet exists but we don't own it
    if "snippet" in df.columns:
        df["gap_snippet"] = (
            df["snippet"].notna() &
            (df["snippet"].astype(str).str.strip() != "") &
            (df["position"] > 5)
        )
    else:
        df["gap_snippet"] = False

    # ── Composite gap score (weighted)
    # Absence weighted highest — no presence at all is the biggest gap
    weights = {
        "gap_absence":     0.40,
        "gap_performance": 0.30,
        "gap_format":      0.15,
        "gap_paa":         0.10,
        "gap_snippet":     0.05,
    }
    df["gap_score"] = sum(
        df[col].astype(float) * w for col, w in weights.items()
    ).round(3)

    # ── Gap type label (primary gap for quick reading)
    def primary_gap(row):
        if row["gap_absence"]:     return "Absence"
        if row["gap_performance"]: return "Performance"
        if row["gap_format"]:      return "Format"
        if row["gap_paa"]:         return "PAA"
        if row["gap_snippet"]:     return "Snippet"
        return "None"

    df["primary_gap"] = df.apply(primary_gap, axis=1)

    # ── Summary stats
    gap_counts = {
        "Absence":     df["gap_absence"].sum(),
        "Performance": df["gap_performance"].sum(),
        "Format":      df["gap_format"].sum(),
        "PAA":         df["gap_paa"].sum(),
        "Snippet":     df["gap_snippet"].sum(),
    }
    print("\n    Gap type breakdown:")
    for gtype, cnt in gap_counts.items():
        pct = cnt / len(df) * 100
        bar = "█" * min(30, int(pct / 2))
        print(f"      {gtype:<15} {cnt:>5}  ({pct:5.1f}%)  {bar}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 8. OPPORTUNITY SCORING & RECOMMENDATIONS
# ═══════════════════════════════════════════════════════════════════════

def score_and_recommend(df: pd.DataFrame, top_n: int) -> tuple:
    """
    Roll up to research area level.

    Opportunity = demand × gap × winnability

      demand      = keyword count (or total volume if available)
      gap         = avg gap_score across keywords in area
      winnability = (1 - avg academic_lock) × avg serp_diversity
                    — low academic lock + high SERP diversity = easier to compete

    Separate absence_gap% and performance_gap% drive recommended action:
      if absence_gap% > 60%     → "Create"
      if performance_gap% > 40% → "Optimise"
      else                      → "Create & Optimise"
    """
    print("\n[8/8] Scoring and ranking research areas...")

    work = df[df["research_area"] != "Unclassified / General Methods"].copy()

    # ── Volume: coerce everything to numeric first, then decide which column to use
    for col in ["volume", "impressions"]:
        if col in work.columns:
            work[col] = pd.to_numeric(work[col], errors="coerce").fillna(0)

    if "volume" in work.columns and work["volume"].sum() > 0:
        vol_col = "volume"
        print(f"    → Using search volume for demand scoring (total: {int(work['volume'].sum()):,})")
    elif "impressions" in work.columns and work["impressions"].sum() > 0:
        vol_col = "impressions"
        print(f"    → Using GSC impressions for demand scoring (total: {int(work['impressions'].sum()):,})")
    else:
        vol_col = None
        print("    → No volume/impressions data — using keyword count for demand")

    agg_dict = dict(
        keyword_count        = ("keyword",            "count"),
        avg_similarity       = ("research_area_score","mean"),
        avg_gap_score        = ("gap_score",          "mean"),
        avg_academic_lock    = ("academic_lock",      "mean"),
        avg_serp_diversity   = ("serp_diversity",     "mean"),
        absence_gap_count    = ("gap_absence",        "sum"),
        performance_gap_count= ("gap_performance",   "sum"),
        format_gap_count     = ("gap_format",         "sum"),
        paa_gap_count        = ("gap_paa",            "sum"),
        snippet_gap_count    = ("gap_snippet",        "sum"),
        sample_keywords      = ("keyword",            lambda x: " | ".join(list(x)[:8])),
        sub_topics           = ("cluster_label",      lambda x: " · ".join(
                                 x.value_counts().head(5).index.tolist())),
    )

    if vol_col:
        agg_dict["total_volume"] = (vol_col, "sum")

    area_df = work.groupby("research_area").agg(**agg_dict).reset_index()


    # ── Normalise demand
    if vol_col:
        area_df["demand_score"] = area_df["total_volume"] / area_df["total_volume"].max()
    else:
        area_df["demand_score"] = area_df["keyword_count"] / area_df["keyword_count"].max()

    # ── Winnability: rescale serp_diversity to 0-1 first, then combine with academic lock
    # serp_diversity raw range is ~0.33-1.0 (min 1 distinct / 3 titles)
    # rescaling ensures winnability can actually reach 1.0
    sd_min = area_df["avg_serp_diversity"].min()
    sd_max = area_df["avg_serp_diversity"].max()
    if sd_max > sd_min:
        area_df["avg_serp_diversity_scaled"] = (
            (area_df["avg_serp_diversity"] - sd_min) / (sd_max - sd_min)
        )
    else:
        area_df["avg_serp_diversity_scaled"] = 0.5

    area_df["winnability"] = (
        (1 - area_df["avg_academic_lock"]) * area_df["avg_serp_diversity_scaled"]
    ).clip(0, 1).round(3)

    # ── Opportunity score
    area_df["opportunity_score"] = (
        area_df["demand_score"]  * 0.45 +
        area_df["avg_gap_score"] * 0.35 +
        area_df["winnability"]   * 0.20
    ).round(3)

    # ── Gap percentages
    area_df["absence_gap_pct"]    = (
        area_df["absence_gap_count"]    / area_df["keyword_count"] * 100).round(1)
    area_df["performance_gap_pct"]= (
        area_df["performance_gap_count"]/ area_df["keyword_count"] * 100).round(1)

    # ── Recommended action
    def action(row):
        if row["absence_gap_pct"] >= 60:
            return "🆕 Create"
        if row["performance_gap_pct"] >= 40:
            return "🔧 Optimise"
        return "🆕+🔧 Create & Optimise"

    area_df["recommended_action"] = area_df.apply(action, axis=1)

    # ── Priority tier
    def tier(s):
        if s >= 0.55: return "🔴 High"
        if s >= 0.30: return "🟡 Medium"
        return                "🟢 Monitor"

    area_df["priority"] = area_df["opportunity_score"].apply(tier)

    # ── Add tier from taxonomy
    tier_lookup = {a["name"]: a.get("tier", 2) for a in RESEARCH_AREAS}
    area_df["tier"] = area_df["research_area"].map(tier_lookup).fillna(2).astype(int)
    area_df["tier_label"] = area_df["tier"].map({1: "⭐ Tier 1", 2: "Tier 2"})

    # Sort: Tier 1 areas first, then by opportunity score within each tier
    recs = (
        area_df
        .sort_values(["tier", "opportunity_score"], ascending=[True, False])
        .head(top_n)
        .reset_index(drop=True)
    )
    recs.index += 1
    recs.index.name = "rank"

    print(f"    → {len(area_df)} research areas scored")
    return area_df, recs


# ═══════════════════════════════════════════════════════════════════════
# 9. HTML REPORT
# ═══════════════════════════════════════════════════════════════════════

def generate_report(df: pd.DataFrame, recs: pd.DataFrame, output_dir: str,
                    briefs_html: str = "", serp_gaps_html: str = "",
                    visibility_html: str = ""):
    print("\n    → Generating HTML report...")

    # ── Scatter coloured by research area
    try:
        import plotly.express as px
        import plotly.io as pio

        hover = ["keyword", "research_area", "research_area_2", "primary_gap",
                 "gap_score", "position", "cluster_label"]
        hover = [c for c in hover if c in df.columns]

        fig = px.scatter(
            df, x="tsne_x", y="tsne_y",
            color="research_area",
            symbol="primary_gap",
            hover_data=hover,
            title="Keyword Map — Research Areas & Gap Types",
            labels={"tsne_x": "t-SNE 1", "tsne_y": "t-SNE 2",
                    "research_area": "Research Area",
                    "primary_gap":   "Primary Gap"},
            height=800, template="plotly_white"
        )
        fig.update_traces(marker=dict(size=6, opacity=0.75))
        scatter_html = pio.to_html(fig, full_html=False, include_plotlyjs="cdn")

        # Gap breakdown bar chart
        gap_data = pd.DataFrame({
            "Gap Type": ["Absence", "Performance", "Format", "PAA", "Snippet"],
            "Count":    [
                int(df["gap_absence"].sum()),
                int(df["gap_performance"].sum()),
                int(df["gap_format"].sum()),
                int(df["gap_paa"].sum()),
                int(df["gap_snippet"].sum()),
            ]
        })
        fig2 = px.bar(
            gap_data, x="Gap Type", y="Count",
            title="Gap Type Distribution Across All Keywords",
            color="Gap Type", template="plotly_white", height=350
        )
        bar_html = pio.to_html(fig2, full_html=False, include_plotlyjs=False)

    except ImportError:
        scatter_html = "<p><em>pip install plotly for charts</em></p>"
        bar_html     = ""

    # ── Recommendations table
    col_order = [
        "research_area", "keyword_count", "absence_gap_pct",
        "performance_gap_pct", "winnability", "opportunity_score",
        "priority", "recommended_action", "top_competitors", "sub_topics"
    ]
    col_order = [c for c in col_order if c in recs.columns]

    rec_rows = ""
    for rank, row in recs.iterrows():
        rec_rows += f"""
        <tr>
          <td style='text-align:center;font-weight:700'>{rank}</td>
          <td>{row.get('tier_label','')}</td>
          <td><strong>{row['research_area']}</strong></td>
          <td style='text-align:center'>{row.get('keyword_count','')}</td>
          <td style='text-align:center'>{row.get('absence_gap_pct','')}%</td>
          <td style='text-align:center'>{row.get('performance_gap_pct','')}%</td>
          <td style='text-align:center'>{row.get('avg_academic_lock','')}</td>
          <td style='text-align:center'>{row.get('winnability','')}</td>
          <td style='text-align:center'>{row.get('opportunity_score','')}</td>
          <td>{row.get('priority','')}</td>
          <td>{row.get('recommended_action','')}</td>
          <td style='font-size:0.8em;color:#444'>{row.get('sub_topics','')}</td>
        </tr>"""

    # ── Summary stats
    n_kw      = len(df)
    n_areas   = (df["research_area"] != "Unclassified / General Methods").nunique() - 1
    n_absence = int(df["gap_absence"].sum())
    n_perf    = int(df["gap_performance"].sum())
    n_unclass = int((df["research_area"] == "Unclassified / General Methods").sum())

    html = f"""<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Life Science Content Gap Analysis</title>
  <style>
    body   {{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',sans-serif;
            margin:0;padding:24px 44px;background:#f0f4f8;color:#212529;}}
    h1     {{color:#0d1b2a;border-bottom:3px solid #0077b6;padding-bottom:10px;}}
    h2     {{color:#0077b6;margin-top:36px;}}
    .stats {{display:flex;gap:16px;flex-wrap:wrap;margin:20px 0;}}
    .card  {{background:white;border-radius:10px;padding:14px 22px;
            box-shadow:0 2px 8px rgba(0,0,0,.08);min-width:130px;}}
    .card .num {{font-size:1.9em;font-weight:700;color:#0077b6;}}
    .card .lbl {{font-size:0.82em;color:#6c757d;margin-top:3px;}}
    table  {{width:100%;border-collapse:collapse;background:white;
            border-radius:10px;overflow:hidden;
            box-shadow:0 2px 8px rgba(0,0,0,.08);font-size:0.86em;}}
    th     {{background:#0077b6;color:white;padding:10px 8px;text-align:left;}}
    td     {{padding:9px 8px;border-bottom:1px solid #e9ecef;}}
    tr:hover td {{background:#e8f4fd;}}
    .plot  {{background:white;border-radius:10px;padding:16px;
            box-shadow:0 2px 8px rgba(0,0,0,.08);margin:16px 0;}}
    .note  {{background:#d1ecf1;border-left:4px solid #0077b6;
            padding:12px 16px;border-radius:4px;margin:16px 0;font-size:0.88em;}}
    .legend {{display:flex;gap:24px;flex-wrap:wrap;margin:12px 0;font-size:0.85em;}}
    .leg-item {{display:flex;align-items:center;gap:6px;}}
  </style>
</head>
<body>
  <h1>🔬 Life Science Content Gap Analysis</h1>

  <div class="stats">
    <div class="card"><div class="num">{n_kw}</div><div class="lbl">Total Keywords</div></div>
    <div class="card"><div class="num">{n_areas}</div><div class="lbl">Research Areas</div></div>
    <div class="card"><div class="num">{n_absence}</div><div class="lbl">Absence Gaps</div></div>
    <div class="card"><div class="num">{n_perf}</div><div class="lbl">Performance Gaps</div></div>
    <div class="card"><div class="num">{n_unclass}</div><div class="lbl">Unclassified</div></div>
  </div>

  <div class="note">
    <strong>How to read this:</strong>
    <strong>Absence gap</strong> = zero GSC impressions — no content presence at all (weight: 40%).
    <strong>Performance gap</strong> = impressions exist but position &gt;10 or CTR &lt;2% (weight: 30%).
    <strong>Format gap</strong> = SERP rewards guides/overviews, academic lock is low, you're not ranking (weight: 15%).
    <strong>PAA gap</strong> = People Also Ask questions exist but you're not in top 10 (weight: 10%).
    <strong>Snippet gap</strong> = featured snippet exists that you don't own (weight: 5%).
    <strong>Winnability</strong> = (1 − academic lock) × SERP diversity — higher means easier to compete.
    <strong>Opportunity Score</strong> = 45% demand + 35% gap score + 20% winnability.
    Only gaps with score ≥ 0.15 appear in gaps.csv.
  </div>

  <h2>📊 Keyword Map</h2>
  <div class="plot">{scatter_html}</div>

  <h2>📉 Gap Distribution</h2>
  <div class="plot">{bar_html}</div>

  <h2>🎯 Research Area Recommendations</h2>
  <table>
    <thead><tr>
      <th>#</th><th>Tier</th><th>Research Area</th><th>Keywords</th>
      <th>Absence Gap%</th><th>Perf Gap%</th>
      <th>Academic Lock</th><th>Winnability</th><th>Opp Score</th>
      <th>Priority</th><th>Action</th><th>Key Sub-topics</th>
    </tr></thead>
    <tbody>{rec_rows}</tbody>
  </table>

  {briefs_html}

  {serp_gaps_html}

  <h2>📈 Visibility Improvement Stages</h2>
  <p style='font-size:0.87em;color:#555;margin:0 0 20px 0'>
    The following analyses focus on moving existing and new content from
    page 2 to page 1 — covering intent alignment, fast optimisation wins,
    freshness priorities, and internal linking structure.
  </p>
  {visibility_html}

  <p style="margin-top:40px;color:#aaa;font-size:0.78em">
    Model: all-MiniLM-L6-v2 &nbsp;|&nbsp;
    {len(RESEARCH_AREAS)} research areas (Tier 1: Neuroscience · Oncology · Immunology) &nbsp;|&nbsp;
    Agglomerative clustering + t-SNE &nbsp;|&nbsp;
    Gap types: Absence · Performance · Freshness · Format · PAA · Snippet
  </p>
</body>
</html>"""

    path = os.path.join(output_dir, "report.html")
    with open(path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"    → report.html saved")


# ═══════════════════════════════════════════════════════════════════════
# 9. CONTENT BRIEF GENERATOR
# ═══════════════════════════════════════════════════════════════════════

CONTENT_TYPE_MAP = {
    "clinical":   "Clinical Deep-Dive",
    "guide":      "Practical Guide",
    "overview":   "Educational Overview",
    "listicle":   "Curated Explainer",
    "comparison": "Comparative Analysis",
    "news":       "Research Spotlight",
    "other":      "Research Overview",
    "unknown":    "Research Overview",
}

# Keywords that should never be used as a cluster label
LABEL_BLOCKLIST = {
    # Generic lab terms
    "kit", "panel", "assay", "protocol", "reagent", "antibody", "buffer",
    "solution", "product", "catalog", "item", "sample", "control", "standard",
    "test", "tool", "service", "data", "analysis", "method", "technique",
    "research", "study", "review", "paper", "article", "publication",
    # Short noise
    "a", "an", "the", "of", "in", "for", "and", "or", "with",
    # Numeric / ID patterns handled separately via regex
}

def _is_valid_label(kw: str) -> bool:
    """Return False if keyword looks like a product ID or generic noise."""
    kw = kw.strip().lower()
    # Too short
    if len(kw) < 5:
        return False
    # Blocklisted
    if kw in LABEL_BLOCKLIST:
        return False
    # Looks like a product code: has digits mixed with letters, or is all caps short
    if re.match(r"^[a-z]{0,3}\d+", kw):           # e.g. ab12345, 9876
        return False
    if re.match(r"^[A-Z]{2,6}-?\d{3,}", kw):       # e.g. AB-1234, FLWPNL007
        return False
    if re.match(r"^\d", kw):                         # starts with digit
        return False
    # Contains suspicious patterns
    if re.search(r"\d{4,}", kw):                     # long number sequence
        return False
    return True


def _best_cluster_label(cluster: pd.DataFrame, c_emb: np.ndarray,
                         centroid: np.ndarray) -> str:
    """
    Pick the cluster label keyword using centroid similarity,
    but skip any keyword that fails the quality filter.
    Falls back to the longest valid keyword if all centroid candidates fail.
    """
    sims  = c_emb @ centroid
    order = sims.argsort()[::-1]   # highest similarity first
    kws   = cluster["keyword"].tolist()

    for idx in order:
        kw = kws[idx]
        if _is_valid_label(kw):
            return kw

    # Fallback: longest valid keyword in cluster
    valid = [k for k in kws if _is_valid_label(k)]
    if valid:
        return max(valid, key=len)

    # Last resort: just return the most common word in the cluster
    all_words = " ".join(kws).split()
    counts    = Counter(w for w in all_words if len(w) > 4)
    if counts:
        return counts.most_common(1)[0][0]
    return kws[0]


def _extract_topic_phrase(keywords: list[str]) -> str:
    """
    Extract the most informative 2-3 word phrase from a list of keywords
    to use as the content angle. Prefers multi-word, biology-specific phrases.
    """
    # Score each keyword: longer multi-word phrases score higher,
    # penalise single words and product-like strings
    scored = []
    for kw in keywords:
        if not _is_valid_label(kw):
            continue
        words  = kw.split()
        n      = len(words)
        score  = n * 2                          # reward multi-word
        score += min(len(kw), 30)               # reward longer strings
        score -= sum(1 for w in words           # penalise generic words
                     if w.lower() in LABEL_BLOCKLIST) * 3
        scored.append((score, kw))

    if not scored:
        return keywords[0] if keywords else "Research Topic"

    scored.sort(reverse=True)
    return scored[0][1]


def _generate_title(topic: str, paa_questions: list[str], primary_gap: str,
                    content_type: str, research_area: str,
                    top_keywords: list[str], used_titles: set) -> str:
    """
    Generate a specific, non-repetitive title using multiple strategies:
    1. PAA-first: turn the most specific PAA question into a title
    2. Mechanism: "X: Mechanisms, Regulation & [Research|Clinical] Implications"
    3. Comparative: "X vs Y: ..."
    4. Application: how/why/what structure based on top keywords
    Tracks used titles to avoid repetition.
    """
    area = research_area.split("/")[0].strip()
    topic_title = topic.title()

    # ── Strategy 1: PAA-first (most specific, reader-intent driven)
    if paa_questions:
        # Prefer questions that contain specific scientific terms
        specific_qs = [
            q for q in paa_questions
            if len(q.split()) >= 5
            and not any(g in q.lower() for g in ["what is", "what are", "define"])
        ]
        generic_qs = [q for q in paa_questions if q not in specific_qs]

        for q in (specific_qs + generic_qs)[:5]:
            q_clean = q.strip().rstrip("?")
            if len(q_clean) > 20:
                candidate = f"{q_clean}: Current Evidence and Research Directions"
                if candidate not in used_titles:
                    used_titles.add(candidate)
                    return candidate

    # ── Strategy 2: detect comparative angle from keywords
    comparators = [k for k in top_keywords[:10]
                   if " vs " in k.lower() or " versus " in k.lower()]
    if comparators and primary_gap in ("Absence", "Performance"):
        parts = re.split(r" vs | versus ", comparators[0], flags=re.IGNORECASE)
        if len(parts) == 2:
            candidate = f"{parts[0].strip().title()} vs {parts[1].strip().title()}: Mechanisms, Efficacy & Clinical Considerations"
            if candidate not in used_titles:
                used_titles.add(candidate)
                return candidate

    # ── Strategy 3: mechanism/pathway angle (best for absence gaps in research areas)
    if primary_gap == "Absence":
        templates = [
            f"{topic_title}: Mechanisms, Regulation and Therapeutic Implications",
            f"{topic_title}: From Molecular Biology to Clinical Applications",
            f"The Role of {topic_title} in {area}: Current Research and Emerging Insights",
            f"{topic_title}: Pathways, Biomarkers and Research Advances",
        ]
        for t in templates:
            if t not in used_titles:
                used_titles.add(t)
                return t

    # ── Strategy 4: performance gap — content exists but underperforms
    if primary_gap == "Performance":
        templates = [
            f"{topic_title}: What Researchers Need to Know in {_current_year()}",
            f"Advances in {topic_title}: Key Findings and Clinical Relevance",
            f"{topic_title} Explained: Mechanisms, Methods and Current Evidence",
        ]
        for t in templates:
            if t not in used_titles:
                used_titles.add(t)
                return t

    # ── Strategy 5: PAA gap — question-led content
    if primary_gap == "PAA" and paa_questions:
        q = paa_questions[0].strip().rstrip("?")
        templates = [
            f"{q}: A Researcher's Guide",
            f"{topic_title}: Answering the Key Questions in {area}",
        ]
        for t in templates:
            if t not in used_titles:
                used_titles.add(t)
                return t

    # ── Strategy 6: format gap — accessible explainer
    if primary_gap == "Format":
        templates = [
            f"{topic_title}: A Practical Guide for {area} Research",
            f"Understanding {topic_title}: Key Concepts and Applications",
        ]
        for t in templates:
            if t not in used_titles:
                used_titles.add(t)
                return t

    # ── Fallback with uniqueness guarantee
    fallback = f"{topic_title}: Research Overview, Methods and Key Findings"
    suffix   = 2
    base     = fallback
    while fallback in used_titles:
        fallback = f"{base} (Part {suffix})"
        suffix  += 1
    used_titles.add(fallback)
    return fallback


def _current_year() -> str:
    from datetime import datetime
    return str(datetime.now().year)


def _tone_from_lock(academic_lock: float) -> str:
    if academic_lock >= 0.7:
        return "Technical — written for active researchers and scientists"
    if academic_lock >= 0.4:
        return "Balanced — research-aware professionals and clinicians"
    return "Accessible — broader scientific and clinical audience"


def _wordcount(fmt: str, academic_lock: float) -> str:
    if fmt == "clinical":       return "2,500–3,500"
    if academic_lock >= 0.6:    return "2,000–3,000"
    if fmt in ("guide","listicle"): return "1,500–2,500"
    return "1,800–2,500"


def _recommend_structure(paa_questions: list[str], content_type: str,
                          top_keywords: list[str]) -> str:
    """
    Suggest an article structure (H2 sections) based on PAA questions,
    content type, and keyword themes. This is the core of what makes
    a brief actually useful to a writer.
    """
    sections = []

    # Always open with an intro section
    sections.append("Introduction & Why This Topic Matters")

    # If we have PAA questions, use them as H2s
    if paa_questions:
        for q in paa_questions[:4]:
            q = q.strip().rstrip("?")
            if q and len(q) > 10:
                sections.append(q)

    # Pad with inferred sections based on content type
    if content_type == "Clinical Deep-Dive":
        extras = ["Molecular Mechanisms", "Clinical Evidence & Trial Data",
                  "Biomarkers & Patient Selection", "Current Challenges & Future Directions"]
    elif content_type == "Practical Guide":
        extras = ["Key Principles & Background", "Step-by-Step Methodology",
                  "Troubleshooting & Best Practices", "Applications & Case Examples"]
    elif content_type == "Educational Overview":
        extras = ["Core Concepts & Definitions", "Biological Mechanisms",
                  "Research Applications", "Open Questions in the Field"]
    elif content_type == "Comparative Analysis":
        extras = ["Mechanism of Action Comparison", "Clinical / Experimental Evidence",
                  "Advantages & Limitations", "How to Choose: Decision Framework"]
    else:
        extras = ["Background & Context", "Key Research Findings",
                  "Implications for Research & Practice", "Conclusion & Future Outlook"]

    for e in extras:
        if e not in sections and len(sections) < 7:
            sections.append(e)

    sections.append("Conclusion & Key Takeaways")

    return " → ".join(sections)


def generate_content_briefs(df: pd.DataFrame, kw_emb: np.ndarray,
                             recs: pd.DataFrame, output_dir: str,
                             n_briefs: int = 12) -> pd.DataFrame:
    """
    Stage 9: Generate 10-15 prioritised, specific content briefs.

    Key improvements over v1:
    - Cluster labels use quality-filtered keyword selection (no product IDs)
    - Titles are generated with 6 strategies + deduplication tracker
    - Each brief includes a suggested article structure (H2 sections)
    - Topic phrase extraction prefers specific multi-word biology terms
    - Per-area clustering keeps briefs cleanly scoped
    """
    print("\n[9/9] Generating content briefs...")

    from sklearn.decomposition import TruncatedSVD
    from sklearn.cluster import AgglomerativeClustering, KMeans
    from sklearn.preprocessing import normalize

    # Only classified, meaningfully gapped keywords
    work = df[
        (df["research_area"] != "Unclassified / General Methods") &
        (df["gap_score"] >= 0.15) &
        (df["keyword"].apply(_is_valid_label))   # filter product IDs at source
    ].copy()

    if len(work) == 0:
        print("    → No valid gap keywords above threshold")
        return pd.DataFrame()

    tier_lookup  = {a["name"]: a.get("tier", 2) for a in RESEARCH_AREAS}
    all_briefs   = []
    used_titles  = set()   # deduplication across all briefs

    areas_ordered = (
        recs.sort_values(["tier", "opportunity_score"], ascending=[True, False])
        ["research_area"].tolist()
    )

    for area in areas_ordered:
        area_mask = work["research_area"] == area
        area_df   = work[area_mask].copy()

        if len(area_df) < 3:
            continue

        # ── Cluster within this area only
        area_indices = area_df.index.tolist()
        a_emb        = kw_emb[area_indices]
        n            = len(area_df)
        n_comp       = min(30, a_emb.shape[1] - 1, n - 1)
        reduced      = normalize(
            TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(a_emb)
        )

        k = max(2, min(15, n // 6))
        labels = (
            AgglomerativeClustering(n_clusters=k, metric="cosine", linkage="average")
            .fit_predict(reduced)
            if n <= 500 else
            KMeans(n_clusters=k, random_state=42, n_init="auto").fit_predict(reduced)
        )

        area_df = area_df.copy()
        area_df["area_cluster"] = labels

        for cid in np.unique(labels):
            c_mask  = area_df["area_cluster"] == cid
            cluster = area_df[c_mask]

            if len(cluster) < 2:
                continue

            c_emb    = reduced[c_mask.values]
            centroid = c_emb.mean(axis=0)

            # ── Quality-filtered cluster label
            label_kw = _best_cluster_label(cluster, c_emb, centroid)

            # ── Topic phrase (best multi-word angle from cluster keywords)
            valid_kws = [k for k in cluster["keyword"].tolist() if _is_valid_label(k)]
            topic     = _extract_topic_phrase(valid_kws)

            # ── Gap signals
            avg_gap   = cluster["gap_score"].mean()
            top_gap   = cluster["primary_gap"].value_counts().index[0] \
                        if len(cluster["primary_gap"].value_counts()) else "Absence"

            # ── PAA questions — deduplicated, quality-filtered
            all_paa = []
            for paa_str in cluster["paa_questions"].dropna():
                for q in str(paa_str).split("|"):
                    q = q.strip()
                    if q and len(q) > 10 and q not in all_paa:
                        all_paa.append(q)

            # ── Format and tone
            fmt_counts = cluster["dominant_format"].value_counts()
            dom_fmt    = fmt_counts.index[0] if len(fmt_counts) else "other"
            avg_lock   = cluster["academic_lock"].mean()
            content_type = CONTENT_TYPE_MAP.get(dom_fmt, "Research Overview")

            # ── Target keywords: valid only, sorted by impressions
            if "impressions" in cluster.columns and cluster["impressions"].sum() > 0:
                top_kws = (
                    cluster[cluster["keyword"].apply(_is_valid_label)]
                    .sort_values("impressions", ascending=False)
                    ["keyword"].head(15).tolist()
                )
            else:
                top_kws = valid_kws[:15]

            # ── Snippet and cross-area
            has_snippet = bool(cluster.get("gap_snippet", pd.Series(False)).any())
            cross_area  = ""
            if "research_area_2" in cluster.columns:
                a2 = cluster["research_area_2"].value_counts()
                if len(a2) and a2.index[0] != area:
                    cross_area = a2.index[0]

            # ── Generate non-repetitive title
            title = _generate_title(
                topic, all_paa, top_gap, content_type,
                area, top_kws, used_titles
            )

            # ── Suggested article structure
            structure = _recommend_structure(all_paa, content_type, top_kws)

            # ── Brief score
            paa_bonus     = min(0.15, len(all_paa) * 0.015)
            snippet_bonus = 0.05 if has_snippet else 0.0
            tier_bonus    = 0.10 if tier_lookup.get(area, 2) == 1 else 0.0
            brief_score   = avg_gap + paa_bonus + snippet_bonus + tier_bonus

            all_briefs.append({
                "brief_score":         round(brief_score, 3),
                "research_area":       area,
                "tier":                tier_lookup.get(area, 2),
                "topic":               topic,
                "cross_area":          cross_area,
                "suggested_title":     title,
                "content_type":        content_type,
                "primary_gap":         top_gap,
                "keyword_count":       len(cluster),
                "primary_keyword":     top_kws[0] if top_kws else topic,
                "secondary_keywords":  " | ".join(top_kws[1:8]),
                "all_target_keywords": " | ".join(top_kws),
                "paa_questions":       " | ".join(all_paa[:8]),
                "paa_count":           len(all_paa),
                "suggested_structure": structure,
                "tone":                _tone_from_lock(avg_lock),
                "word_count":          _wordcount(dom_fmt, avg_lock),
                "academic_lock":       round(avg_lock, 2),
                "avg_gap_score":       round(avg_gap, 3),
                "has_snippet_opp":     has_snippet,
                "recommended_action":  (
                    "🆕 Create"          if top_gap == "Absence"     else
                    "🔧 Optimise"        if top_gap == "Performance" else
                    "❓ Add PAA content" if top_gap == "PAA"         else
                    "📋 Reformat"        if top_gap == "Format"      else
                    "⭐ Win Snippet"
                ),
            })

    if not all_briefs:
        print("    → No briefs generated — check gap threshold or keyword quality")
        return pd.DataFrame()

    briefs_df = pd.DataFrame(all_briefs)

    # ── Select top n_briefs: guarantee 1 per area, then fill by score
    selected    = []
    seen_areas  = set()

    for _, row in briefs_df.sort_values(
        ["tier", "brief_score"], ascending=[True, False]
    ).iterrows():
        if row["research_area"] not in seen_areas:
            selected.append(row.to_dict())
            seen_areas.add(row["research_area"])

    selected_titles = {r["suggested_title"] for r in selected}
    for _, row in briefs_df.sort_values("brief_score", ascending=False).iterrows():
        if len(selected) >= n_briefs:
            break
        if row["suggested_title"] not in selected_titles:
            selected.append(row.to_dict())
            selected_titles.add(row["suggested_title"])

    briefs_out = (
        pd.DataFrame(selected)
        .sort_values(["tier", "brief_score"], ascending=[True, False])
        .reset_index(drop=True)
    )
    briefs_out.index += 1
    briefs_out.index.name = "brief_rank"

    briefs_path = os.path.join(output_dir, "content_briefs.csv")
    briefs_out.to_csv(briefs_path)
    print(f"    → {len(briefs_out)} content briefs generated → {briefs_path}")

    print("\n📝  Content Briefs:\n")
    for rank, row in briefs_out.iterrows():
        tier_tag = "⭐" if row["tier"] == 1 else "  "
        print(f"  {rank:>2}. {tier_tag} [{row['research_area']}]")
        print(f"       {row['suggested_title']}")
        print(f"       {row['content_type']} · {row['recommended_action']} · "
              f"{row['keyword_count']} keywords · {row['word_count']} words")
        if row["paa_count"] > 0:
            first_q = str(row["paa_questions"]).split("|")[0].strip()
            print(f"       PAA ({row['paa_count']} Qs): {first_q}...")
        print()

    return briefs_out


def generate_briefs_html(briefs_df: pd.DataFrame, output_dir: str) -> str:
    if briefs_df is None or len(briefs_df) == 0:
        return ""

    cards = ""
    for rank, row in briefs_df.iterrows():
        tier_badge = (
            "<span style='background:#0077b6;color:white;border-radius:4px;"
            "padding:2px 8px;font-size:0.75em;margin-right:6px'>⭐ Tier 1</span>"
            if row["tier"] == 1 else
            "<span style='background:#6c757d;color:white;border-radius:4px;"
            "padding:2px 8px;font-size:0.75em;margin-right:6px'>Tier 2</span>"
        )
        cross_html = (
            f"<span style='font-size:0.8em;color:#6c757d'> · Also: {row['cross_area']}</span>"
            if row.get("cross_area") else ""
        )

        # PAA questions as structured list
        paa_html = ""
        if row.get("paa_questions"):
            items = "".join(
                f"<li style='margin:3px 0'>{q.strip()}</li>"
                for q in str(row["paa_questions"]).split("|")[:6]
                if q.strip()
            )
            if items:
                paa_html = (
                    f"<div style='margin-top:12px'>"
                    f"<strong style='font-size:0.84em;color:#495057'>"
                    f"PAA Questions to Answer ({row['paa_count']} total):</strong>"
                    f"<ul style='margin:4px 0 0 0;padding-left:18px;"
                    f"font-size:0.82em;color:#555'>{items}</ul></div>"
                )

        # Suggested structure
        structure_html = ""
        if row.get("suggested_structure"):
            steps = str(row["suggested_structure"]).split(" → ")
            step_items = "".join(
                f"<span style='background:#f0f4f8;border-radius:4px;padding:3px 8px;"
                f"font-size:0.78em;margin:2px;display:inline-block'>"
                f"<strong>{i+1}.</strong> {s}</span>"
                for i, s in enumerate(steps)
            )
            structure_html = (
                f"<div style='margin-top:12px'>"
                f"<strong style='font-size:0.84em;color:#495057'>Suggested Structure:</strong>"
                f"<div style='margin-top:5px;line-height:1.8'>{step_items}</div></div>"
            )

        # Target keywords as tags
        kws    = str(row.get("all_target_keywords", "")).split("|")
        kw_tags = "".join(
            f"<span style='background:#e8f4fd;border-radius:3px;padding:2px 7px;"
            f"font-size:0.78em;margin:2px;display:inline-block'>{k.strip()}</span>"
            for k in kws[:12] if k.strip()
        )

        gap_colour = {
            "Absence":     "#dc3545",
            "Performance": "#fd7e14",
            "PAA":         "#0077b6",
            "Format":      "#6610f2",
            "Snippet":     "#198754",
        }.get(row["primary_gap"], "#6c757d")

        snippet_html = (
            "<div style='margin-top:10px;font-size:0.8em;background:#fff3cd;"
            "border-radius:4px;padding:6px 10px'>"
            "⭐ Featured snippet opportunity — aim for a direct answer in the opening paragraph"
            "</div>"
            if row.get("has_snippet_opp") else ""
        )

        cards += f"""
        <div style='background:white;border-radius:12px;padding:22px 26px;
                    box-shadow:0 2px 10px rgba(0,0,0,.08);margin-bottom:24px;
                    border-left:5px solid {gap_colour}'>

          <div style='display:flex;align-items:center;justify-content:space-between;
                      flex-wrap:wrap;gap:8px;margin-bottom:8px'>
            <div>
              {tier_badge}
              <span style='font-size:0.83em;color:#495057;font-weight:600'>
                {row['research_area']}</span>{cross_html}
            </div>
            <div style='display:flex;gap:6px;flex-wrap:wrap'>
              <span style='background:{gap_colour};color:white;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['primary_gap']} Gap</span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['content_type']}</span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['word_count']} words</span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['recommended_action']}</span>
            </div>
          </div>

          <h3 style='margin:0 0 6px 0;color:#0d1b2a;font-size:1.08em;line-height:1.4'>
            <span style='color:{gap_colour};margin-right:6px'>#{rank}</span>
            {row['suggested_title']}
          </h3>

          <div style='font-size:0.82em;color:#6c757d;margin-bottom:14px'>
            {row['keyword_count']} target keywords &nbsp;·&nbsp;
            Tone: {row['tone']}
          </div>

          <div>
            <strong style='font-size:0.84em;color:#495057'>Target Keywords:</strong><br>
            <div style='margin-top:5px'>{kw_tags}</div>
          </div>

          {structure_html}
          {paa_html}
          {snippet_html}
        </div>"""

    return f"""
  <h2>📝 Content Briefs ({len(briefs_df)} Recommendations)</h2>
  <div style='margin-bottom:16px;font-size:0.87em;color:#555;
              background:#f8f9fa;border-radius:8px;padding:12px 16px'>
    Briefs are ranked by opportunity score within each tier.
    Each brief includes a suggested article structure derived from PAA questions and content type.
    Border colour = primary gap:
    <span style='color:#dc3545'>■ Absence</span> &nbsp;
    <span style='color:#fd7e14'>■ Performance</span> &nbsp;
    <span style='color:#0077b6'>■ PAA</span> &nbsp;
    <span style='color:#6610f2'>■ Format</span> &nbsp;
    <span style='color:#198754'>■ Snippet</span>
  </div>
  {cards}"""

# ═══════════════════════════════════════════════════════════════════════
# 10. SERP TITLE MINING — CONTENT ANGLE GAP ANALYSIS
# ═══════════════════════════════════════════════════════════════════════

def mine_serp_title_gaps(df: pd.DataFrame, output_dir: str,
                          model, n_gaps: int = 12) -> tuple:
    """
    Stage 10: Data-mine SERP titles to find content angles that are either:
      A) Heavily represented in top-ranking titles but you have no presence
      B) Raised in PAA questions but addressed by NO top-ranking title

    This is fundamentally different from keyword gap analysis (Stage 7-9).
    Stage 7-9 asks: "where are you absent from search results?"
    Stage 10 asks: "what do the search results reveal that nobody has written well?"

    Approach per research area:
      1. Explode enriched_text back into individual SERP titles
      2. Embed all titles as a corpus (not as keyword enrichment — as content units)
      3. Cluster the title corpus to find what themes Google is rewarding
      4. For each title cluster:
           - Summarise what the cluster covers (pattern from titles)
           - Check your GSC presence (avg position for mapped keywords)
           - Find PAA questions that don't match any title cluster centroid
             → these are the uncontested angles
      5. Score each gap by: search demand × your absence × PAA signal strength
      6. Output structured gap briefs: what exists, what's missing, why it wins

    Proxy for "your content": GSC position ≤ 10 on mapped keywords = you have coverage.
    Position > 20 or no impressions = you're absent from that topic.
    """
    print("\n[10/10] Mining SERP titles for content angle gaps...")

    from sklearn.decomposition import TruncatedSVD
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.preprocessing import normalize
    from sklearn.metrics.pairwise import cosine_similarity

    # ── Build the SERP title corpus
    # Each row in df has an enriched_text = "keyword | title1 | title2 | ..."
    # Explode back to individual titles, tagged with their keyword's research area

    title_rows = []

    for _, row in df.iterrows():
        area     = row.get("research_area", "")
        keyword  = row.get("keyword", "")
        position = float(row.get("position", 999))
        impressions = float(row.get("impressions", 0))

        if area == "Unclassified / General Methods" or not area:
            continue

        # Get titles from enriched_text (pipe-separated) or title_N columns
        enriched = str(row.get("enriched_text", ""))
        if "|" in enriched:
            parts  = [p.strip() for p in enriched.split("|")]
            titles = parts[1:]   # skip the keyword itself (first element)
        else:
            title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])
            titles = [str(row.get(c, "")).strip()
                      for c in title_cols if pd.notna(row.get(c)) and str(row.get(c)).strip()]

        # Get PAA questions
        paa_str = str(row.get("paa_questions", ""))
        paas    = [q.strip() for q in paa_str.split("|") if q.strip() and len(q.strip()) > 8]

        for title in titles:
            if title and len(title) > 10 and title.lower() != keyword:
                title_rows.append({
                    "title":       title,
                    "keyword":     keyword,
                    "research_area": area,
                    "position":    position,
                    "impressions": impressions,
                    "paa_questions": paa_str,
                    "paas":        paas,
                })

    if not title_rows:
        print("    → No SERP titles available for mining")
        return pd.DataFrame(), ""

    titles_df = pd.DataFrame(title_rows)
    print(f"    → {len(titles_df)} SERP title instances across "
          f"{titles_df['research_area'].nunique()} research areas")

    # ── Embed the title corpus
    print("    → Embedding SERP title corpus...")
    unique_titles = titles_df["title"].drop_duplicates().tolist()
    title_embs    = model.encode(
        unique_titles,
        show_progress_bar=False,
        batch_size=128,
        normalize_embeddings=True
    )
    title_emb_map = {t: e for t, e in zip(unique_titles, title_embs)}
    titles_df["emb_idx"] = titles_df["title"].map(
        {t: i for i, t in enumerate(unique_titles)}
    )

    all_gaps = []

    for area in titles_df["research_area"].unique():
        area_titles = titles_df[titles_df["research_area"] == area].copy()
        n = len(area_titles)

        if n < 5:
            continue

        # ── Cluster the title corpus for this area
        embs = np.array([title_emb_map[t] for t in area_titles["title"]])
        n_comp   = min(20, embs.shape[1] - 1, n - 1)
        reduced  = normalize(
            TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(embs)
        )

        k = max(3, min(12, n // 8))
        labels = AgglomerativeClustering(
            n_clusters=k, metric="cosine", linkage="average"
        ).fit_predict(reduced)

        area_titles = area_titles.copy()
        area_titles["title_cluster"] = labels

        # ── Collect all PAA questions for this area (deduplicated)
        all_area_paa = []
        for paa_list in area_titles["paas"].tolist():
            for q in paa_list:
                if q and q not in all_area_paa:
                    all_area_paa.append(q)

        # Embed PAA questions if we have them
        paa_embs = None
        if all_area_paa:
            paa_embs = model.encode(
                all_area_paa,
                show_progress_bar=False,
                normalize_embeddings=True
            )

        # ── Analyse each title cluster
        for cid in np.unique(labels):
            c_mask   = area_titles["title_cluster"] == cid
            cluster  = area_titles[c_mask]
            c_embs   = embs[c_mask.values]
            centroid = c_embs.mean(axis=0)
            centroid = centroid / (np.linalg.norm(centroid) + 1e-9)

            # What do these titles cover? Extract representative titles
            sims        = c_embs @ centroid
            top_idx     = sims.argsort()[::-1][:5]
            rep_titles  = cluster["title"].iloc[top_idx].tolist()

            # Pattern: find common words/phrases across rep titles
            def extract_pattern(titles: list) -> str:
                """Find what themes recur across the top titles in a cluster."""
                all_words = Counter()
                for t in titles:
                    words = re.findall(r"\b[a-zA-Z]{4,}\b", t.lower())
                    all_words.update(words)
                # Remove noise words
                noise = {"with", "from", "that", "this", "have", "been",
                         "their", "they", "were", "what", "which", "when",
                         "after", "before", "about", "into", "your", "more"}
                top_words = [w for w, _ in all_words.most_common(8)
                             if w not in noise]
                return ", ".join(top_words[:5]) if top_words else "mixed"

            pattern     = extract_pattern(rep_titles)
            cluster_label = rep_titles[0] if rep_titles else f"Cluster {cid}"

            # ── Your presence: avg position for keywords mapping to this cluster
            # Keywords "map" to this cluster if their position is tracked
            cluster_kws     = cluster.drop_duplicates("keyword")
            your_positions  = cluster_kws[cluster_kws["position"] < 999]["position"]
            your_impressions= cluster_kws["impressions"].sum()

            if len(your_positions) > 0:
                avg_pos = your_positions.mean()
            else:
                avg_pos = 999   # no presence

            you_are_absent = avg_pos > 20 or your_impressions == 0

            # ── Find uncontested PAA angles
            # PAA questions that are semantically distant from ALL title cluster centroids
            uncontested_paa = []
            if paa_embs is not None and len(all_area_paa) > 0:
                # Compute all cluster centroids for this area
                all_centroids = []
                for other_cid in np.unique(labels):
                    other_mask = area_titles["title_cluster"] == other_cid
                    other_embs = embs[other_mask.values]
                    c = other_embs.mean(axis=0)
                    c = c / (np.linalg.norm(c) + 1e-9)
                    all_centroids.append(c)
                all_centroids = np.array(all_centroids)

                # For each PAA question, find its max similarity to any title cluster
                paa_cluster_sims = paa_embs @ all_centroids.T
                max_sims         = paa_cluster_sims.max(axis=1)

                # PAA questions with low similarity to all clusters = uncontested
                for q, max_sim in zip(all_area_paa, max_sims):
                    if max_sim < 0.45:   # no cluster strongly covers this question
                        uncontested_paa.append(q)

            # ── Score this gap
            # Higher score = more valuable to create content for
            absence_score  = 1.0 if avg_pos == 999 else max(0, (avg_pos - 10) / 90)
            demand_score   = min(1.0, your_impressions / 5000) if your_impressions > 0 else 0.3
            paa_score      = min(1.0, len(uncontested_paa) * 0.2)
            title_coverage = len(cluster) / max(len(area_titles), 1)  # how big this theme is

            gap_score = (
                absence_score  * 0.40 +
                title_coverage * 0.25 +
                paa_score      * 0.20 +
                demand_score   * 0.15
            )

            # ── Determine gap type
            if not you_are_absent:
                gap_type = "Angle Gap"   # topic exists, specific angle missing
                gap_explanation = (
                    f"You rank for some keywords in this area (avg position {avg_pos:.0f}) "
                    f"but the SERP shows {len(cluster)} titles on this theme — "
                    f"a more targeted piece could outrank broader coverage."
                )
            elif uncontested_paa:
                gap_type = "Uncontested Angle"
                gap_explanation = (
                    f"This theme has {len(cluster)} competing titles but "
                    f"{len(uncontested_paa)} PAA questions that none of them answer well. "
                    f"Content that directly addresses these questions has a clear opening."
                )
            else:
                gap_type = "Topic Void"
                gap_explanation = (
                    f"You have no presence on this theme ({len(cluster)} SERP titles exist). "
                    f"Creating foundational content here captures demand with no direct competition "
                    f"from your existing pages."
                )

            # ── Generate suggested title from the missing angle
            if uncontested_paa:
                # Title comes from the uncontested PAA question
                best_q   = uncontested_paa[0].rstrip("?")
                title_suggestion = (
                    f"{best_q}: Evidence, Mechanisms and Research Implications"
                )
            else:
                # Title comes from what's missing — deeper angle on the cluster theme
                words = pattern.split(", ")
                topic = " ".join(w.title() for w in words[:3]) if words else cluster_label
                title_suggestion = (
                    f"{topic}: A Deep-Dive into Mechanisms, Current Evidence "
                    f"and Research Directions"
                )

            # What existing titles cover (shown in brief)
            existing_coverage = " · ".join(
                t[:60] + "..." if len(t) > 60 else t
                for t in rep_titles[:4]
            )

            all_gaps.append({
                "gap_score":          round(gap_score, 3),
                "research_area":      area,
                "gap_type":           gap_type,
                "gap_explanation":    gap_explanation,
                "suggested_title":    title_suggestion,
                "theme_pattern":      pattern,
                "serp_title_count":   len(cluster),
                "your_avg_position":  round(avg_pos, 1) if avg_pos < 999 else "No presence",
                "uncontested_paa":    " | ".join(uncontested_paa[:5]),
                "uncontested_paa_count": len(uncontested_paa),
                "existing_coverage":  existing_coverage,
                "rep_titles":         " · ".join(rep_titles[:3]),
                "keywords_in_theme":  " | ".join(cluster["keyword"].drop_duplicates().head(10).tolist()),
            })

    if not all_gaps:
        print("    → No content angle gaps identified")
        return pd.DataFrame(), ""

    gaps_df = (
        pd.DataFrame(all_gaps)
        .sort_values("gap_score", ascending=False)
        .head(n_gaps)
        .reset_index(drop=True)
    )
    gaps_df.index += 1
    gaps_df.index.name = "gap_rank"

    # ── Save
    gaps_path = os.path.join(output_dir, "serp_content_gaps.csv")
    gaps_df.to_csv(gaps_path)
    print(f"    → {len(gaps_df)} content angle gaps identified → {gaps_path}")

    # ── Terminal summary
    print("\n🔍  SERP Content Angle Gaps:\n")
    for rank, row in gaps_df.iterrows():
        gap_icon = {"Uncontested Angle": "🎯", "Topic Void": "⬛",
                    "Angle Gap": "↗️"}.get(row["gap_type"], "•")
        print(f"  {rank:>2}. {gap_icon} [{row['research_area']}] {row['gap_type']}")
        print(f"       {row['suggested_title']}")
        print(f"       SERP coverage: {row['serp_title_count']} titles · "
              f"Your position: {row['your_avg_position']}")
        if row["uncontested_paa_count"] > 0:
            first = str(row["uncontested_paa"]).split("|")[0].strip()
            print(f"       Uncontested: {first}...")
        print()

    # ── Build HTML section
    gap_cards = ""
    for rank, row in gaps_df.iterrows():
        gap_colour = {
            "Uncontested Angle": "#0077b6",
            "Topic Void":        "#dc3545",
            "Angle Gap":         "#fd7e14",
        }.get(row["gap_type"], "#6c757d")

        gap_icon = {
            "Uncontested Angle": "🎯",
            "Topic Void":        "⬛",
            "Angle Gap":         "↗️",
        }.get(row["gap_type"], "•")

        # Uncontested PAA as list
        paa_html = ""
        if row.get("uncontested_paa"):
            items = "".join(
                f"<li style='margin:3px 0;color:#0077b6'>{q.strip()}</li>"
                for q in str(row["uncontested_paa"]).split("|")[:5]
                if q.strip()
            )
            if items:
                paa_html = (
                    f"<div style='margin-top:12px'>"
                    f"<strong style='font-size:0.84em;color:#495057'>"
                    f"❓ Uncontested questions — not answered by any top-ranking page:</strong>"
                    f"<ul style='margin:4px 0 0 0;padding-left:18px;"
                    f"font-size:0.82em'>{items}</ul></div>"
                )

        # What currently ranks
        existing_html = ""
        if row.get("rep_titles"):
            items = "".join(
                f"<li style='margin:2px 0;color:#6c757d'>{t.strip()}</li>"
                for t in str(row["rep_titles"]).split("·")[:3]
                if t.strip()
            )
            existing_html = (
                f"<div style='margin-top:12px'>"
                f"<strong style='font-size:0.84em;color:#495057'>"
                f"📄 What currently ranks on this theme:</strong>"
                f"<ul style='margin:4px 0 0 0;padding-left:18px;"
                f"font-size:0.82em;font-style:italic'>{items}</ul></div>"
            )

        # Keyword tags
        kws = str(row.get("keywords_in_theme", "")).split("|")
        kw_tags = "".join(
            f"<span style='background:#f0f4f8;border-radius:3px;padding:2px 7px;"
            f"font-size:0.78em;margin:2px;display:inline-block'>{k.strip()}</span>"
            for k in kws[:10] if k.strip()
        )

        pos_display = row["your_avg_position"]
        pos_colour  = "#dc3545" if pos_display == "No presence" else (
            "#198754" if isinstance(pos_display, float) and pos_display <= 10 else "#fd7e14"
        )

        gap_cards += f"""
        <div style='background:white;border-radius:12px;padding:22px 26px;
                    box-shadow:0 2px 10px rgba(0,0,0,.08);margin-bottom:24px;
                    border-left:5px solid {gap_colour}'>

          <div style='display:flex;align-items:center;justify-content:space-between;
                      flex-wrap:wrap;gap:8px;margin-bottom:10px'>
            <div>
              <span style='background:{gap_colour};color:white;border-radius:4px;
                           padding:2px 10px;font-size:0.8em;margin-right:8px'>
                {gap_icon} {row["gap_type"]}
              </span>
              <span style='font-size:0.83em;color:#495057;font-weight:600'>
                {row["research_area"]}
              </span>
            </div>
            <div style='display:flex;gap:6px;flex-wrap:wrap;font-size:0.78em'>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;
                           border-radius:4px;padding:2px 8px'>
                {row["serp_title_count"]} competing titles
              </span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;
                           border-radius:4px;padding:2px 8px;color:{pos_colour}'>
                Your position: {pos_display}
              </span>
            </div>
          </div>

          <h3 style='margin:0 0 6px 0;color:#0d1b2a;font-size:1.06em;line-height:1.4'>
            <span style='color:{gap_colour}'># {rank}</span> &nbsp;
            {row["suggested_title"]}
          </h3>

          <div style='font-size:0.83em;color:#555;background:#f8f9fa;
                      border-radius:6px;padding:8px 12px;margin:10px 0;
                      border-left:3px solid {gap_colour}'>
            {row["gap_explanation"]}
          </div>

          <div style='margin-top:10px'>
            <strong style='font-size:0.84em;color:#495057'>Related keywords:</strong>
            <div style='margin-top:5px'>{kw_tags}</div>
          </div>

          {existing_html}
          {paa_html}
        </div>"""

    serp_gaps_html = f"""
  <h2>🔍 SERP Content Angle Gaps ({len(gaps_df)} Identified)</h2>
  <div style='margin-bottom:16px;font-size:0.87em;color:#555;
              background:#f8f9fa;border-radius:8px;padding:12px 16px'>
    These gaps are derived from mining what top-ranking SERP titles cover —
    not from keyword demand alone. They represent topics where either
    no content exists, a specific angle is unaddressed, or PAA questions
    go unanswered by any current top-ranking page.
    <br><br>
    <span style='color:#dc3545'>■ Topic Void</span> — theme has SERP presence but you have none &nbsp;
    <span style='color:#0077b6'>■ Uncontested Angle</span> — PAA questions unanswered by top pages &nbsp;
    <span style='color:#fd7e14'>■ Angle Gap</span> — you rank but a deeper angle is available
  </div>
  {gap_cards}"""

    return gaps_df, serp_gaps_html


# ═══════════════════════════════════════════════════════════════════════
# 11. SEARCH INTENT CLASSIFIER
# ═══════════════════════════════════════════════════════════════════════

INTENT_PATTERNS = {
    "informational": [
        r"\b(what is|what are|how does|how do|why does|why is|explain|"
        r"mechanism|overview|introduction|definition|understand|learn|"
        r"review|guide|tutorial|biology|pathway|function|role of)\b"
    ],
    "commercial": [
        r"\b(best|top|compare|vs\.?|versus|comparison|alternative|"
        r"recommend|rated|ranking|review of|which is better|choose)\b"
    ],
    "transactional": [
        r"\b(buy|purchase|order|price|cost|kit|panel|reagent|antibody|"
        r"catalog|supplier|manufacturer|protocol kit|assay kit|"
        r"recombinant|conjugated|custom)\b"
    ],
    "clinical": [
        r"\b(clinical trial|phase [123]|patient|treatment|therapy|"
        r"diagnosis|prognosis|biomarker|efficacy|safety|dose|dosing|"
        r"indication|fda approved|ema approved|guideline|standard of care)\b"
    ],
    "navigational": [
        r"\b(ncbi|pubmed|uniprot|ensembl|pdb|kegg|reactome|"
        r"abcam|thermo|sigma|bio-techne|r&d systems|biolegend)\b"
    ],
}

def classify_intent(text: str) -> str:
    """Classify search intent from keyword or title text."""
    text = text.lower()
    scores = {}
    for intent, patterns in INTENT_PATTERNS.items():
        score = sum(len(re.findall(p, text)) for p in patterns)
        scores[intent] = score
    top = max(scores, key=scores.get)
    return top if scores[top] > 0 else "informational"


def run_intent_classification(df: pd.DataFrame) -> pd.DataFrame:
    """
    Stage 11: Classify search intent for every keyword.

    Why this matters for page 1-2 visibility:
    If your content is informational but the SERP is 80% transactional,
    Google will not rank you there regardless of content quality.
    This flags those structural mismatches so you don't waste resource.

    Outputs:
      keyword_intent    — dominant intent of the search query
      serp_intent       — dominant intent of top-ranking titles
      intent_mismatch   — True if keyword and SERP intents diverge
      intent_note       — human-readable explanation of the mismatch
    """
    print("\n[11] Classifying search intent...")

    # Classify keyword intent
    df["keyword_intent"] = df["keyword"].apply(classify_intent)

    # Classify SERP intent from titles
    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])

    def serp_intent_from_titles(row):
        titles = " ".join(
            str(row.get(c, "")) for c in title_cols[:5]
            if pd.notna(row.get(c))
        )
        if "enriched_text" in row.index and "|" in str(row.get("enriched_text", "")):
            titles = " ".join(str(row["enriched_text"]).split("|")[1:6])
        return classify_intent(titles) if titles.strip() else "unknown"

    df["serp_intent"] = df.apply(serp_intent_from_titles, axis=1)

    # Flag mismatches
    df["intent_mismatch"] = (
        (df["serp_intent"] != "unknown") &
        (df["keyword_intent"] != df["serp_intent"])
    )

    def intent_note(row):
        if not row["intent_mismatch"]:
            return ""
        ki, si = row["keyword_intent"], row["serp_intent"]
        notes = {
            ("informational", "transactional"):
                "SERP is product/commercial — informational content unlikely to rank here",
            ("informational", "clinical"):
                "SERP favours clinical evidence pages — content needs trial data & citations",
            ("commercial", "informational"):
                "SERP is educational — product-focused content won't rank",
            ("transactional", "informational"):
                "SERP rewards explainer content — pure product pages won't rank",
            ("informational", "navigational"):
                "SERP is database/tool navigation — not rankable with editorial content",
        }
        return notes.get((ki, si),
            f"Intent shift: your content is {ki}, SERP rewards {si}")

    df["intent_note"] = df.apply(intent_note, axis=1)

    n_mismatch = df["intent_mismatch"].sum()
    print(f"    → {n_mismatch:,} intent mismatches detected "
          f"({n_mismatch/len(df)*100:.1f}% of keywords)")

    intent_dist = df["keyword_intent"].value_counts()
    print("    → Intent distribution:")
    for intent, cnt in intent_dist.items():
        bar = "█" * min(30, cnt * 30 // len(df))
        print(f"      {intent:<16} {cnt:>5}  {bar}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 12. PAGE 2 FAST-WIN IDENTIFIER
# ═══════════════════════════════════════════════════════════════════════

def find_page2_fast_wins(df: pd.DataFrame, output_dir: str) -> pd.DataFrame:
    """
    Stage 12: Identify keywords ranking 11-20 (page 2) where small
    content improvements could flip to page 1.

    These are your highest-ROI optimisation opportunities:
    - Content clearly exists (Google has indexed and ranked it)
    - You're close — position 11-20, not position 50+
    - Specific, actionable edits can push you over

    For each fast-win keyword, output:
      - What PAA questions are being asked that you may not answer
      - What format the top results use (that yours may not match)
      - Whether there's a snippet to win
      - Specific optimisation actions ranked by impact

    Grouped by cluster so optimisation briefs address multiple
    related keywords in one content edit.
    """
    print("\n[12] Identifying page 2 fast-wins...")

    # Page 2: position 11-20, has impressions (confirmed Google presence)
    page2 = df[
        (df["position"] >= 11) &
        (df["position"] <= 20) &
        (df["impressions"] > 0) &
        (df["research_area"] != "Unclassified / General Methods")
    ].copy()

    if len(page2) == 0:
        print("    → No page 2 keywords found")
        return pd.DataFrame()

    print(f"    → {len(page2):,} keywords on page 2 across "
          f"{page2['research_area'].nunique()} research areas")

    # Group by cluster for shared optimisation briefs
    fast_wins = []

    for (area, cluster), group in page2.groupby(
        ["research_area", "cluster_label"]
    ):
        if len(group) < 1:
            continue

        # PAA questions unanswered (gap_paa flagged)
        paa_needed = []
        for paa_str in group["paa_questions"].dropna():
            for q in str(paa_str).split("|"):
                q = q.strip()
                if q and len(q) > 8 and q not in paa_needed:
                    paa_needed.append(q)

        # Format signal
        fmt_counts  = group["dominant_format"].value_counts()
        serp_format = fmt_counts.index[0] if len(fmt_counts) else "unknown"

        # Snippet opportunity
        snippet_opp = bool(group.get("gap_snippet", pd.Series(False)).any())

        # Intent alignment
        intent_issues = group[group["intent_mismatch"]]["intent_note"].unique().tolist() \
                        if "intent_mismatch" in group.columns else []

        # Best position in cluster (closest to page 1)
        best_pos  = group["position"].min()
        avg_pos   = group["position"].mean()
        total_imp = group["impressions"].sum()
        top_kw    = group.sort_values("impressions", ascending=False)["keyword"].iloc[0]

        # Build prioritised action list
        actions = []
        if paa_needed:
            actions.append(
                f"Add a FAQ or dedicated section answering: "
                f"{'; '.join(paa_needed[:3])}"
            )
        if serp_format in ("guide", "overview") and not any(
            w in cluster.lower() for w in ["guide", "overview", "how", "what"]
        ):
            actions.append(
                f"Restructure as a {serp_format} — SERP rewards this format here"
            )
        if snippet_opp:
            actions.append(
                "Add a concise 40-60 word direct-answer paragraph near the top "
                "targeting the featured snippet"
            )
        if intent_issues:
            actions.append(f"Intent note: {intent_issues[0]}")
        if not actions:
            actions.append(
                "Deepen content coverage — add more specific sub-topics, "
                "mechanisms, or supporting data to match SERP depth"
            )

        # Win score: closer to p1 + higher impressions = better
        win_score = (
            ((20 - avg_pos) / 10) * 0.6 +
            min(1.0, total_imp / 1000) * 0.4
        )

        fast_wins.append({
            "win_score":          round(win_score, 3),
            "research_area":      area,
            "cluster":            cluster,
            "keyword_count":      len(group),
            "best_position":      round(best_pos, 1),
            "avg_position":       round(avg_pos, 1),
            "total_impressions":  int(total_imp),
            "primary_keyword":    top_kw,
            "all_keywords":       " | ".join(
                group.sort_values("impressions", ascending=False)
                ["keyword"].head(10).tolist()
            ),
            "serp_format":        serp_format,
            "snippet_opportunity":snippet_opp,
            "paa_to_add":         " | ".join(paa_needed[:5]),
            "optimisation_actions": " → ".join(actions),
            "effort":             "Low" if len(actions) <= 2 else "Medium",
        })

    if not fast_wins:
        print("    → No fast-win clusters found")
        return pd.DataFrame()

    wins_df = (
        pd.DataFrame(fast_wins)
        .sort_values("win_score", ascending=False)
        .reset_index(drop=True)
    )
    wins_df.index += 1
    wins_df.index.name = "priority"

    path = os.path.join(output_dir, "page2_fast_wins.csv")
    wins_df.to_csv(path)
    print(f"    → {len(wins_df)} fast-win clusters → {path}")
    print(f"    → Potential: {wins_df['keyword_count'].sum():,} keywords "
          f"could move from page 2 → page 1")

    return wins_df


# ═══════════════════════════════════════════════════════════════════════
# 13. FRESHNESS PRIORITY SCORER
# ═══════════════════════════════════════════════════════════════════════

FRESHNESS_SIGNALS = [
    r"\b(202[3-9]|20[3-9]\d)\b",                          # recent years
    r"\b(new|latest|recent|update[d]?|current|emerging)\b",
    r"\b(phase [23]|trial results|readout|approval|approved|fda|ema)\b",
    r"\b(breakthrough|advance[d]?|novel|first.in.class|next.gen)\b",
]

def score_freshness_priority(df: pd.DataFrame, output_dir: str) -> pd.DataFrame:
    """
    Stage 13: Identify keyword clusters where SERP titles signal
    recency demand — meaning stale content is actively losing ground.

    Logic:
    1. Detect recency signals in SERP titles (year mentions, "new", "latest",
       clinical readout language, approval announcements)
    2. Score each keyword by how strong those signals are
    3. Combine with your position — if you rank 5-15 on a
       freshness-sensitive topic, a content update is urgent
    4. Output a prioritised refresh list by cluster

    This is distinct from the gap analysis — these keywords already
    have content, it's just ageing out of relevance.
    """
    print("\n[13] Scoring freshness priority...")

    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])

    def freshness_score(row) -> float:
        """Count recency signals across SERP titles for this keyword."""
        titles = " ".join(
            str(row.get(c, "")) for c in title_cols[:10]
            if pd.notna(row.get(c))
        )
        if "enriched_text" in row.index and "|" in str(row.get("enriched_text", "")):
            titles = " ".join(str(row["enriched_text"]).split("|")[1:])
        titles = titles.lower()
        hits = sum(
            len(re.findall(p, titles)) for p in FRESHNESS_SIGNALS
        )
        return min(1.0, hits / 5)   # normalise 0-1

    df["freshness_signal"] = df.apply(freshness_score, axis=1)

    # Freshness priority = high signal + you rank (content exists but may be stale)
    # Only flag keywords where you're ranking — no point refreshing content
    # you don't have
    df["freshness_priority"] = (
        df["freshness_signal"] >= 0.4
    ) & (
        df["position"] < 999
    )

    # Roll up to cluster level
    fresh_clusters = []

    for (area, cluster), group in df[df["freshness_priority"]].groupby(
        ["research_area", "cluster_label"]
    ):
        avg_signal = group["freshness_signal"].mean()
        avg_pos    = group["position"].replace(999, np.nan).mean()
        total_imp  = group["impressions"].sum()
        kw_count   = len(group)

        # Sample the freshness signals detected
        sample_titles = []
        for col in title_cols[:3]:
            vals = group[col].dropna().head(2).tolist()
            sample_titles.extend([str(v) for v in vals if str(v).strip()])

        refresh_urgency = (
            "🔴 Urgent" if avg_signal >= 0.7 and (avg_pos or 999) < 15
            else "🟡 Soon"   if avg_signal >= 0.5
            else "🟢 Watch"
        )

        fresh_clusters.append({
            "refresh_score":   round(avg_signal * 0.6 +
                                     (1 - min(1, (avg_pos or 50) / 50)) * 0.4, 3),
            "urgency":         refresh_urgency,
            "research_area":   area,
            "cluster":         cluster,
            "keyword_count":   kw_count,
            "avg_position":    round(avg_pos, 1) if avg_pos else "—",
            "total_impressions": int(total_imp),
            "freshness_signal":round(avg_signal, 2),
            "top_keywords":    " | ".join(
                group.sort_values("impressions", ascending=False)
                ["keyword"].head(8).tolist()
            ),
            "recency_evidence":"; ".join(sample_titles[:3]),
            "recommended_action": (
                "Update with latest clinical data, trial results, or recent publications. "
                "Add publication date and 'Last updated' tag. "
                "Refresh statistics and cited studies."
            ),
        })

    if not fresh_clusters:
        print("    → No freshness-sensitive clusters detected")
        return pd.DataFrame()

    fresh_df = (
        pd.DataFrame(fresh_clusters)
        .sort_values("refresh_score", ascending=False)
        .reset_index(drop=True)
    )
    fresh_df.index += 1
    fresh_df.index.name = "priority"

    path = os.path.join(output_dir, "freshness_priority.csv")
    fresh_df.to_csv(path)

    urgent = (fresh_df["urgency"] == "🔴 Urgent").sum()
    print(f"    → {len(fresh_df)} freshness-sensitive clusters | "
          f"{urgent} urgent → {path}")

    return fresh_df


# ═══════════════════════════════════════════════════════════════════════
# 14. TOPIC CLUSTER MAPPER (HUB & SPOKE)
# ═══════════════════════════════════════════════════════════════════════

def map_topic_clusters(df: pd.DataFrame, kw_emb: np.ndarray,
                        output_dir: str) -> pd.DataFrame:
    """
    Stage 14: Map keywords into hub-and-spoke topic structures
    for internal linking strategy.

    Why this matters for rankings:
    Internal links signal to Google which pages are authoritative
    on a topic. A hub page on "CAR-T cell therapy" that links to
    spokes on "CAR-T manufacturing", "CAR-T toxicity management",
    "CAR-T vs BiTE" etc. builds topical authority much faster
    than isolated pages.

    This stage identifies:
      HUB candidates — broad keywords with high search volume / impression
                        that could serve as pillar pages linking outward
      SPOKE candidates — specific sub-topic keywords that should link
                         back to the hub
      ORPHANS — keywords that have no natural hub in your current
                keyword set (content gaps for hub pages)

    Output:
      topic_clusters.csv — hub-spoke mapping for each research area
                           ready to inform site architecture and
                           internal linking decisions
    """
    print("\n[14] Mapping hub-and-spoke topic clusters...")

    from sklearn.metrics.pairwise import cosine_similarity

    classified = df[
        df["research_area"] != "Unclassified / General Methods"
    ].copy()

    topic_clusters = []

    for area in classified["research_area"].unique():
        area_df = classified[classified["research_area"] == area].copy()

        if len(area_df) < 5:
            continue

        area_indices = area_df.index.tolist()
        a_emb        = kw_emb[area_indices]

        # ── Identify hub candidates
        # Hubs are broad, high-volume keywords — they represent the pillar topic
        # Proxy: shorter keyword (fewer words), higher impressions
        area_df["word_count_kw"] = area_df["keyword"].str.split().str.len()

        # Hub score: fewer words + more impressions = broader pillar candidate
        max_imp = area_df["impressions"].max() or 1
        area_df["hub_score"] = (
            (1 / area_df["word_count_kw"].clip(1, 10)) * 0.5 +
            (area_df["impressions"] / max_imp)           * 0.5
        )

        n_hubs = max(1, min(5, len(area_df) // 10))
        hub_mask   = area_df.nlargest(n_hubs, "hub_score")
        spoke_mask = area_df[~area_df.index.isin(hub_mask.index)]

        hub_embs   = a_emb[[area_indices.index(i) for i in hub_mask.index]]

        # For each hub, find its spokes (closest non-hub keywords)
        for hub_idx, hub_row in hub_mask.iterrows():
            hub_emb_vec = kw_emb[hub_idx].reshape(1, -1)

            # Cosine sim from this hub to all spokes in area
            spoke_indices = [area_indices.index(i)
                             for i in spoke_mask.index if i in area_indices]
            if not spoke_indices:
                continue

            spoke_embs = a_emb[spoke_indices]
            sims       = cosine_similarity(hub_emb_vec, spoke_embs)[0]

            spoke_df = spoke_mask.copy()
            spoke_df["hub_similarity"] = sims

            # Direct spokes: sim > 0.6 (clearly related)
            direct_spokes = spoke_df[spoke_df["hub_similarity"] > 0.60]
            # Weak spokes: 0.4-0.6 (loosely related)
            weak_spokes   = spoke_df[
                (spoke_df["hub_similarity"] >= 0.40) &
                (spoke_df["hub_similarity"] <= 0.60)
            ]

            # Identify orphans: spokes with very low similarity to ANY hub
            orphan_threshold = 0.35
            orphan_mask = spoke_df["hub_similarity"] < orphan_threshold

            topic_clusters.append({
                "research_area":       area,
                "hub_keyword":         hub_row["keyword"],
                "hub_impressions":     int(hub_row["impressions"]),
                "hub_position":        round(hub_row["position"], 1)
                                       if hub_row["position"] < 999 else "Not ranking",
                "hub_recommendation":  (
                    "✓ Strengthen as pillar page — expand coverage and add internal links"
                    if hub_row["position"] < 20 else
                    "⚠ Hub not ranking — create or significantly expand this page first"
                ),
                "direct_spoke_count":  len(direct_spokes),
                "direct_spokes":       " | ".join(
                    direct_spokes.sort_values("impressions", ascending=False)
                    ["keyword"].head(10).tolist()
                ),
                "weak_spoke_count":    len(weak_spokes),
                "weak_spokes":         " | ".join(
                    weak_spokes.sort_values("impressions", ascending=False)
                    ["keyword"].head(5).tolist()
                ),
                "orphan_count":        int(orphan_mask.sum()),
                "orphan_keywords":     " | ".join(
                    spoke_df[orphan_mask]["keyword"].head(5).tolist()
                ),
                "internal_link_action": (
                    f"Link {len(direct_spokes)} spoke pages back to this hub. "
                    f"Ensure hub page explicitly covers: "
                    f"{', '.join(direct_spokes['keyword'].head(3).tolist())}."
                ) if len(direct_spokes) > 0 else
                    "No close spokes found — this may be an isolated topic",
            })

    if not topic_clusters:
        print("    → No topic cluster structure identified")
        return pd.DataFrame()

    tc_df = (
        pd.DataFrame(topic_clusters)
        .sort_values(["research_area", "hub_impressions"], ascending=[True, False])
        .reset_index(drop=True)
    )
    tc_df.index += 1
    tc_df.index.name = "cluster_rank"

    path = os.path.join(output_dir, "topic_clusters.csv")
    tc_df.to_csv(path)

    total_orphans = tc_df["orphan_count"].sum()
    print(f"    → {len(tc_df)} hub-spoke clusters mapped | "
          f"{total_orphans} orphaned keywords → {path}")

    print("\n    Hub-spoke summary by research area:")
    for area, grp in tc_df.groupby("research_area"):
        hubs    = len(grp)
        spokes  = grp["direct_spoke_count"].sum()
        orphans = grp["orphan_count"].sum()
        print(f"      {area:<35} {hubs} hubs · {spokes} spokes · {orphans} orphans")

    return tc_df


def generate_visibility_html(wins_df: pd.DataFrame, fresh_df: pd.DataFrame,
                              tc_df: pd.DataFrame, intent_df: pd.DataFrame) -> str:
    """Generate the HTML section for all visibility stages."""

    sections = []

    # ── Page 2 Fast Wins
    if len(wins_df) > 0:
        rows = ""
        for rank, row in wins_df.head(15).iterrows():
            snippet_badge = (
                "<span style='background:#198754;color:white;border-radius:3px;"
                "padding:1px 6px;font-size:0.75em;margin-left:4px'>⭐ Snippet</span>"
                if row.get("snippet_opportunity") else ""
            )
            rows += f"""
            <tr>
              <td style='font-weight:700;text-align:center'>{rank}</td>
              <td>{row['research_area']}</td>
              <td><strong>{row['primary_keyword']}</strong>{snippet_badge}</td>
              <td style='text-align:center'>{row['avg_position']}</td>
              <td style='text-align:center'>{row['keyword_count']}</td>
              <td style='text-align:center'>{row['total_impressions']:,}</td>
              <td style='font-size:0.8em;color:#495057'>{row['optimisation_actions']}</td>
              <td style='text-align:center'>{row['effort']}</td>
            </tr>"""

        sections.append(f"""
  <h2>⚡ Page 2 Fast Wins ({len(wins_df)} clusters)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Keywords ranking 11–20 where targeted content edits could move to page 1.
    Sorted by proximity to page 1 and impression volume.
  </p>
  <table>
    <thead><tr>
      <th>#</th><th>Area</th><th>Primary Keyword</th>
      <th>Avg Pos</th><th>Keywords</th><th>Impressions</th>
      <th>Optimisation Actions</th><th>Effort</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    # ── Freshness Priority
    if len(fresh_df) > 0:
        rows = ""
        for rank, row in fresh_df.head(15).iterrows():
            urgency_colour = {
                "🔴 Urgent": "#dc3545",
                "🟡 Soon":   "#fd7e14",
                "🟢 Watch":  "#198754",
            }.get(row["urgency"], "#6c757d")
            rows += f"""
            <tr>
              <td style='font-weight:700;text-align:center'>{rank}</td>
              <td><span style='color:{urgency_colour}'>{row['urgency']}</span></td>
              <td>{row['research_area']}</td>
              <td><strong>{row['cluster']}</strong></td>
              <td style='text-align:center'>{row['avg_position']}</td>
              <td style='text-align:center'>{row['keyword_count']}</td>
              <td style='font-size:0.78em;color:#555;font-style:italic'>
                {str(row['recency_evidence'])[:80]}...</td>
            </tr>"""

        sections.append(f"""
  <h2>🔄 Freshness Priority ({len(fresh_df)} clusters)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Topics where SERP titles show strong recency signals — existing content
    may be losing ground to newer pages. Refresh these to defend rankings.
  </p>
  <table>
    <thead><tr>
      <th>#</th><th>Urgency</th><th>Area</th><th>Topic</th>
      <th>Your Pos</th><th>Keywords</th><th>Recency Evidence</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    # ── Intent Mismatches
    if "intent_mismatch" in intent_df.columns:
        mismatches = intent_df[intent_df["intent_mismatch"]].copy()
        if len(mismatches) > 0:
            intent_summary = mismatches.groupby(
                ["keyword_intent", "serp_intent"]
            ).size().reset_index(name="count").sort_values("count", ascending=False)

            rows = ""
            for _, row in intent_summary.head(10).iterrows():
                rows += f"""
                <tr>
                  <td>{row['keyword_intent']}</td>
                  <td>→</td>
                  <td>{row['serp_intent']}</td>
                  <td style='text-align:center'>{row['count']:,}</td>
                  <td style='font-size:0.8em;color:#dc3545'>
                    Content targeting this query type unlikely to rank
                  </td>
                </tr>"""

            sections.append(f"""
  <h2>🎯 Intent Mismatches ({len(mismatches):,} keywords)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Keywords where your likely content type doesn't match what Google
    rewards. These are structural ranking barriers — content quality
    alone cannot overcome intent mismatch.
  </p>
  <table>
    <thead><tr>
      <th>Your Content Intent</th><th></th><th>SERP Rewards</th>
      <th>Keywords Affected</th><th>Implication</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    # ── Topic Clusters
    if len(tc_df) > 0:
        rows = ""
        for rank, row in tc_df.head(20).iterrows():
            hub_colour = (
                "#198754" if str(row["hub_position"]) != "Not ranking"
                and float(str(row["hub_position"]).replace("Not ranking","999")) < 20
                else "#dc3545"
            )
            rows += f"""
            <tr>
              <td style='text-align:center;font-weight:700'>{rank}</td>
              <td>{row['research_area']}</td>
              <td><strong style='color:{hub_colour}'>{row['hub_keyword']}</strong></td>
              <td style='text-align:center'>{row['hub_position']}</td>
              <td style='text-align:center'>{row['direct_spoke_count']}</td>
              <td style='text-align:center;color:#dc3545'>{row['orphan_count']}</td>
              <td style='font-size:0.79em;color:#495057'>
                {row['internal_link_action']}</td>
            </tr>"""

        sections.append(f"""
  <h2>🕸 Topic Cluster Map ({len(tc_df)} hubs)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Hub-and-spoke structure for internal linking strategy.
    Green hubs are ranking and can be strengthened.
    Red hubs need to be created or significantly expanded first.
    Orphans are keywords with no natural hub — consider whether a pillar page is missing.
  </p>
  <table>
    <thead><tr>
      <th>#</th><th>Area</th><th>Hub Keyword</th><th>Hub Position</th>
      <th>Spokes</th><th>Orphans</th><th>Action</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    return "\n".join(sections)


def main():
    args = parse_args()
    os.makedirs(args.output, exist_ok=True)

    # ── Stage 1: Load & merge
    df = load_and_merge(args.keywords, args.serp, args.kw_col)

    # ── Stage 2: Enrich text
    df = enrich_text(df)

    # ── Stage 3: Embed
    kw_emb, area_emb = embed_all(df)

    # ── Stage 4: Research area mapping
    df = map_research_areas(df, kw_emb, area_emb, args.threshold)

    # ── Stage 5: Sub-topic clustering (global — for t-SNE visualization)
    df = cluster_keywords(df, kw_emb, args.min_cluster)

    # ── Stage 6: Competitor landscape
    df = analyze_competitor_landscape(df)

    # ── Stage 7: Gap analysis
    df = analyze_gaps(df)

    # ── Stage 8: Score & recommend
    area_df, recs = score_and_recommend(df, args.top_n)

    # ── Stage 9: Content briefs (per-area clustering)
    briefs_df   = generate_content_briefs(df, kw_emb, recs, args.output, n_briefs=12)
    briefs_html = generate_briefs_html(briefs_df, args.output)

    # ── Stage 10: SERP title mining — content angle gaps
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer("all-MiniLM-L6-v2")
    serp_gaps_df, serp_gaps_html = mine_serp_title_gaps(
        df, args.output, model, n_gaps=12
    )

    # ── Stage 11: Search intent classification
    df = run_intent_classification(df)

    # ── Stage 12: Page 2 fast wins
    wins_df = find_page2_fast_wins(df, args.output)

    # ── Stage 13: Freshness priority
    fresh_df = score_freshness_priority(df, args.output)

    # ── Stage 14: Topic cluster mapper
    tc_df = map_topic_clusters(df, kw_emb, args.output)

    # ── Visibility HTML (stages 11-14)
    visibility_html = generate_visibility_html(
        wins_df   if len(wins_df)  > 0 else pd.DataFrame(),
        fresh_df  if len(fresh_df) > 0 else pd.DataFrame(),
        tc_df     if len(tc_df)    > 0 else pd.DataFrame(),
        df,
    )

    # ── Save outputs
    drop_cols          = ["tsne_x", "tsne_y", "enriched_text"]
    clusters_path      = os.path.join(args.output, "clusters.csv")
    gaps_path          = os.path.join(args.output, "gaps.csv")
    recs_path          = os.path.join(args.output, "recommendations.csv")
    briefs_path        = os.path.join(args.output, "content_briefs.csv")
    serp_gaps_path     = os.path.join(args.output, "serp_content_gaps.csv")
    wins_path          = os.path.join(args.output, "page2_fast_wins.csv")
    fresh_path         = os.path.join(args.output, "freshness_priority.csv")
    tc_path            = os.path.join(args.output, "topic_clusters.csv")

    df.drop(columns=drop_cols, errors="ignore").to_csv(clusters_path, index=False)

    gap_cols = ["keyword", "research_area", "research_area_2", "cluster_label", "primary_gap",
                "gap_score", "gap_absence", "gap_performance",
                "gap_format", "gap_paa", "gap_snippet", "paa_questions",
                "impressions", "clicks", "position", "ctr",
                "academic_lock", "dominant_format", "serp_diversity"]
    gap_cols = [c for c in gap_cols if c in df.columns]
    (
        df[(df["primary_gap"] != "None") & (df["gap_score"] >= 0.15)][gap_cols]
        .sort_values("gap_score", ascending=False)
        .to_csv(gaps_path, index=False)
    )

    recs.to_csv(recs_path)
    generate_report(df, recs, args.output,
                    briefs_html=briefs_html,
                    serp_gaps_html=serp_gaps_html,
                    visibility_html=visibility_html)

    print("\n" + "═" * 62)
    print("✅  Pipeline complete!")
    print(f"   clusters.csv          → {clusters_path}")
    print(f"   gaps.csv              → {gaps_path}")
    print(f"   recommendations.csv   → {recs_path}")
    print(f"   content_briefs.csv    → {briefs_path}")
    print(f"   serp_content_gaps.csv → {serp_gaps_path}")
    print(f"   page2_fast_wins.csv   → {wins_path}")
    print(f"   freshness_priority.csv→ {fresh_path}")
    print(f"   topic_clusters.csv    → {tc_path}")
    print(f"   report.html           → {os.path.join(args.output, 'report.html')}")
    print("═" * 62)

    print("\n📋  Top Research Area Recommendations:\n")
    show_cols = ["research_area", "keyword_count", "absence_gap_pct",
                 "performance_gap_pct", "opportunity_score",
                 "priority", "recommended_action"]
    show_cols = [c for c in show_cols if c in recs.columns]
    print(recs[show_cols].to_string())
    print("═" * 62)

    print("\n📋  Top Research Area Recommendations:\n")
    show_cols = ["research_area", "keyword_count", "absence_gap_pct",
                 "performance_gap_pct", "opportunity_score",
                 "priority", "recommended_action"]
    show_cols = [c for c in show_cols if c in recs.columns]
    print(recs[show_cols].to_string())


if __name__ == "__main__":
    main()


# ═══════════════════════════════════════════════════════════════════════
# EXPECTED INPUT FORMATS
# ═══════════════════════════════════════════════════════════════════════
#
# keywords.csv
# ────────────
# Required:  keyword, impressions, clicks, position
# Optional:  volume  (SEMrush/GSC search volume — improves demand scoring)
#
#   keyword,impressions,clicks,position
#   CAR-T cell therapy,3100,280,4.1
#   PD-1 checkpoint,1800,95,14.7
#
#
# serp_data.csv
# ─────────────
# Required:  keyword, title_1, title_2, title_3
# Optional:  paa_1, paa_2, paa_3   (People Also Ask questions)
#            snippet                (Featured snippet text)
#
# NOTE: URLs and dates are NOT needed and should be excluded.
#       Titles carry all the signal needed for academic lock-in
#       detection, format classification, and enriched embedding.
#
#   keyword,title_1,title_2,title_3,paa_1,paa_2,snippet
#   CAR-T cell therapy,CAR-T Cell Therapy Overview | NCI,How CAR-T Works,CAR-T Side Effects,What cancers does CAR-T treat?,How long does CAR-T last?,CAR-T therapy uses a patient's own T cells to fight cancer.
#
#
# Run:
#   python lifesci_gap_pipeline.py \
#     --keywords keywords.csv \
#     --serp     serp_data.csv \
#     --output   ./results
#
# Optional flags:
#   --threshold 0.20    min cosine similarity to assign a research area
#   --min-cluster 3     min keywords per sub-topic cluster
#   --top-n 9           show all 9 research areas in recommendations
#   --kw-col query      if your keyword column has a different name
# ═══════════════════════════════════════════════════════════════════════
