"""
Life Science Content Gap Analysis Pipeline
==========================================
Inputs:
  --keywords   keywords.csv       : keyword, volume (opt), impressions, clicks, position
  --serp       serp_data.csv      : keyword, title_1..3, url_1..3, paa_1..3 (opt),
                                    snippet (opt), date_1..3 (opt)
  --output     ./output

Outputs:
  clusters.csv        — every keyword fully enriched + scored
  gaps.csv            — keywords flagged as specific gap types
  recommendations.csv — research areas ranked by opportunity
  report.html         — interactive visual summary

Pipeline stages:
  1. Load & merge all inputs
  2. Enrich: keyword + SERP titles → combined embedding text
  3. Embed enriched text (free local model)
  4. Map to research area taxonomy (cosine similarity)
  5. Sub-topic clustering (Agglomerative / KMeans)
  6. Competitor landscape (domain diversity, academic lock-in)
  7. Gap analysis (absence, performance, freshness, format, PAA)
  8. Opportunity scoring & ranking
  9. Report generation

Dependencies:
  pip install sentence-transformers scikit-learn pandas numpy plotly tldextract
"""

import argparse
import os
import re
import warnings
from collections import Counter

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


_EMB_CACHE_DIR: str = "/tmp"   # overridden in main() to args.output


# ═══════════════════════════════════════════════════════════════════════
#
# Caches embeddings to disk keyed by a hash of the input texts.
# On re-run the embed step is skipped entirely — loads from .npy in ~1s
# instead of encoding for 20-40 minutes.
#
# Cache location: <output_dir>/.emb_cache/
# Each cache entry is two files:
#   <hash>.npy   — the embedding matrix
#   <hash>.json  — metadata (text count, model name, timestamp)
#
# Cache is invalidated automatically if the input texts change
# (different keywords, updated SERP data etc.) — the hash won't match.
# ─────────────────────────────────────────────────────────────────────

import hashlib
import json
from pathlib import Path


def _emb_cache_key(texts: list, model_name: str) -> str:
    """SHA-256 of sorted text list + model name → stable cache key."""
    h = hashlib.sha256()
    h.update(model_name.encode())
    for t in texts:
        h.update(str(t).encode())
    return h.hexdigest()[:16]   # 16 hex chars is plenty for uniqueness


def _cache_path(cache_dir: Path, key: str):
    return cache_dir / f"{key}.npy", cache_dir / f"{key}.json"


def cached_encode(texts: list, model, cache_dir: str,
                  label: str = "texts",
                  batch_size: int = 256,
                  show_progress_bar: bool = True) -> np.ndarray:
    """
    Encode texts with the sentence transformer model, caching to disk.

    First call: encodes and saves to cache_dir/.emb_cache/<hash>.npy
    Subsequent calls with same texts: loads from cache, skips encoding entirely.

    Args:
        texts:             list of strings to encode
        model:             SentenceTransformer instance
        cache_dir:         output directory (cache stored in subdir)
        label:             human-readable name for progress messages
        batch_size:        encoding batch size (256 is safe on most hardware)
        show_progress_bar: show tqdm bar during encoding
    """
    cache_root = Path(cache_dir) / ".emb_cache"
    cache_root.mkdir(parents=True, exist_ok=True)

    model_name = getattr(model, "_model_card_data", {}).get("model_name", "unknown") \
                 if hasattr(model, "_model_card_data") else "all-MiniLM-L6-v2"

    key       = _emb_cache_key(texts, model_name)
    npy_path, meta_path = _cache_path(cache_root, key)

    if npy_path.exists() and meta_path.exists():
        embs = np.load(str(npy_path))
        with open(meta_path) as f:
            meta = json.load(f)
        print(f"    → [{label}] Loaded {embs.shape[0]:,} embeddings from cache "
              f"(saved {meta.get('saved_at', '?')})")
        return embs

    print(f"    → [{label}] Encoding {len(texts):,} texts "
          f"(batch_size={batch_size})...")
    embs = model.encode(
        texts,
        show_progress_bar=show_progress_bar,
        batch_size=batch_size,
        normalize_embeddings=True,
    )

    np.save(str(npy_path), embs)
    with open(meta_path, "w") as f:
        json.dump({
            "label":      label,
            "n_texts":    len(texts),
            "model":      model_name,
            "shape":      list(embs.shape),
            "saved_at":   pd.Timestamp.now().isoformat(),
        }, f, indent=2)

    print(f"    → [{label}] Cached {embs.shape[0]:,} embeddings → "
          f"{npy_path.name}")
    return embs




RESEARCH_AREAS = [

    # ── TIER 1: Primary research areas ────────────────────────────────

    {
        "name": "Neuroscience",
        "tier": 1,
        "description": (
            "neuroscience brain neuron synapse neural circuit neurodegeneration "
            "Alzheimer disease Parkinson disease ALS amyotrophic lateral sclerosis "
            "multiple sclerosis epilepsy stroke dementia cognitive function memory "
            "learning plasticity blood brain barrier glia astrocyte microglia "
            "oligodendrocyte neuroinflammation CNS peripheral nervous system "
            "axon dendrite action potential neurotransmitter dopamine serotonin "
            "GABA glutamate acetylcholine optogenetics electrophysiology patch clamp "
            "fMRI EEG brain imaging connectome neural network pain nociception "
            "spinal cord sensory motor cortex hippocampus cerebellum thalamus "
            "neurogenesis synaptic plasticity long-term potentiation LTP "
            "tau amyloid alpha-synuclein TDP-43 FUS neuronal death "
            "depression anxiety schizophrenia ADHD autism spectrum disorder"
        ),
    },

    {
        "name": "Oncology",
        "tier": 1,
        "description": (
            "oncology cancer tumor malignancy carcinoma sarcoma lymphoma leukemia "
            "melanoma metastasis invasion angiogenesis tumor microenvironment "
            "cancer stem cell clonal evolution drug resistance "
            "chemotherapy radiation targeted therapy kinase inhibitor "
            "KRAS EGFR HER2 BRAF ALK RET FGFR PIK3CA PTEN TP53 RB1 "
            "breast cancer lung cancer colorectal cancer prostate cancer "
            "ovarian cancer pancreatic cancer glioblastoma GBM hepatocellular "
            "bladder cancer renal cell carcinoma thyroid cancer "
            "tumor suppressor oncogene cell proliferation apoptosis senescence "
            "liquid biopsy ctDNA circulating tumor cell cancer biomarker "
            "CDK4 CDK6 mTOR VEGF RAF MEK ERK PI3K AKT Wnt Notch Hedgehog "
            "PARP inhibitor proteasome ubiquitin DNA damage repair homologous recombination"
        ),
    },

    {
        "name": "Immunology",
        "tier": 1,
        "description": (
            "immunology immune system innate immunity adaptive immunity "
            "T cell B cell NK cell dendritic cell macrophage neutrophil "
            "monocyte mast cell basophil eosinophil plasma cell "
            "antibody immunoglobulin IgG IgM IgE IgA "
            "cytokine interleukin TNF interferon chemokine "
            "IL-1 IL-2 IL-4 IL-6 IL-10 IL-12 IL-17 IL-23 TGF-beta "
            "MHC HLA antigen presentation TCR BCR "
            "CD4 CD8 CD19 CD20 CD28 CTLA-4 PD-1 LAG-3 TIM-3 "
            "regulatory T cell Treg Th1 Th2 Th17 follicular helper T cell "
            "germinal center affinity maturation somatic hypermutation "
            "complement system toll-like receptor NLR inflammasome NLRP3 "
            "autoimmunity rheumatoid arthritis lupus SLE inflammatory bowel disease "
            "allergy atopic dermatitis asthma type 1 diabetes multiple sclerosis "
            "immunosuppression transplant rejection GVHD "
            "vaccine adjuvant humoral immunity cellular immunity memory"
        ),
    },

    # ── TIER 2: Secondary research areas ──────────────────────────────

    {
        "name": "Immuno-Oncology",
        "tier": 2,
        "description": (
            "immuno-oncology cancer immunotherapy immune checkpoint "
            "PD-1 PD-L1 CTLA-4 LAG-3 TIM-3 TIGIT checkpoint blockade "
            "CAR-T cell therapy chimeric antigen receptor adoptive cell therapy "
            "tumor infiltrating lymphocyte TIL natural killer cell NK cell therapy "
            "bispecific antibody T cell engager BiTE blinatumomab "
            "tumor microenvironment immunosuppression myeloid derived suppressor "
            "cancer vaccine neoantigen personalized immunotherapy "
            "anti-tumor immunity immune evasion antigen presentation MHC "
            "response rate overall survival progression free survival "
            "combination therapy anti-PD1 anti-CTLA4 nivolumab pembrolizumab "
            "ipilimumab atezolizumab durvalumab avelumab cemiplimab "
            "cytokine release syndrome immune related adverse events irAE "
            "solid tumor hematologic malignancy melanoma NSCLC renal cell "
            "MSI-H TMB tumor mutational burden dMMR mismatch repair"
        ),
    },

    {
        "name": "Immunology & Infectious Disease",
        "tier": 2,
        "description": (
            "infectious disease host immune response pathogen infection "
            "bacteria virus fungus parasite antimicrobial resistance "
            "HIV AIDS influenza SARS-CoV-2 COVID-19 tuberculosis TB "
            "malaria dengue Zika Ebola RSV hepatitis sepsis bacteremia "
            "antibiotic resistance AMR MRSA carbapenem-resistant "
            "innate immune response pattern recognition receptor "
            "toll-like receptor TLR NOD-like receptor inflammasome "
            "interferon antiviral response NK cell macrophage activation "
            "vaccine efficacy adjuvant mucosal immunity IgA neutralizing antibody "
            "microbiome gut flora dysbiosis colonization resistance "
            "antifungal antiviral antiparasitic treatment "
            "epidemiology outbreak transmission zoonotic spillover "
            "T cell response CD8 cytotoxic lymphocyte viral clearance "
            "humoral immunity B cell memory immune evasion pathogen virulence"
        ),
    },

    {
        "name": "Metabolism",
        "tier": 2,
        "description": (
            "metabolism metabolic pathway energy homeostasis "
            "glucose metabolism glycolysis gluconeogenesis glycogen "
            "lipid metabolism fatty acid oxidation lipogenesis lipolysis "
            "cholesterol bile acid triglyceride HDL LDL VLDL "
            "amino acid metabolism protein catabolism urea cycle "
            "mitochondria oxidative phosphorylation electron transport chain "
            "TCA cycle citric acid Krebs cycle ATP production "
            "insulin signaling insulin resistance type 2 diabetes "
            "obesity adipose tissue adipogenesis adipokine leptin adiponectin "
            "mTOR AMPK SIRT1 PPARgamma PGC-1alpha FoxO "
            "liver metabolism hepatic gluconeogenesis NAFLD NASH "
            "gut microbiome metabolite short chain fatty acid bile acid "
            "cancer metabolism Warburg effect aerobic glycolysis glutamine "
            "one-carbon metabolism serine folate methionine "
            "nutrient sensing caloric restriction fasting ketogenesis "
            "metabolomics mass spectrometry flux analysis isotope tracing"
        ),
    },

    {
        "name": "Epigenetics",
        "tier": 2,
        "description": (
            "epigenetics epigenomics chromatin remodeling "
            "DNA methylation CpG island methyltransferase DNMT TET "
            "histone modification histone acetylation histone methylation "
            "H3K4me3 H3K27me3 H3K9me3 H3K27ac H3K4me1 H3K36me3 "
            "histone acetyltransferase HAT histone deacetylase HDAC "
            "histone methyltransferase HMT histone demethylase "
            "chromatin accessibility ATAC-seq DNase-seq "
            "ChIP-seq CUT&RUN CUT&TAG chromatin immunoprecipitation "
            "polycomb group trithorax group PRC1 PRC2 EZH2 "
            "bromodomain BET BRD4 JMJD KDM "
            "non-coding RNA lncRNA microRNA miRNA piRNA "
            "X-inactivation genomic imprinting "
            "3D genome topologically associating domain TAD loop "
            "Hi-C cohesin CTCF enhancer promoter "
            "epigenetic reprogramming cell fate pluripotency "
            "cancer epigenetics epigenetic therapy HDAC inhibitor "
            "single cell epigenomics scATAC-seq multiomics"
        ),
    },

    {
        "name": "Cardiovascular",
        "tier": 2,
        "description": (
            "cardiovascular heart cardiac vascular endothelium "
            "coronary artery disease myocardial infarction heart attack "
            "heart failure cardiomyopathy atrial fibrillation arrhythmia "
            "hypertension blood pressure atherosclerosis plaque "
            "stroke cerebrovascular thrombosis clot coagulation "
            "lipid cholesterol PCSK9 statin LDL HDL triglyceride "
            "platelet aggregation anticoagulant antiplatelet "
            "troponin BNP NT-proBNP cardiac biomarker "
            "cardiac fibrosis remodeling ventricular hypertrophy "
            "valve disease aortic stenosis mitral regurgitation "
            "peripheral artery disease aortic aneurysm "
            "cardiac metabolism energy substrate fatty acid glucose "
            "cardiomyocyte contractility calcium handling "
            "endothelial dysfunction nitric oxide eNOS "
            "RAAS angiotensin ACE inhibitor ARB beta blocker "
            "cardiac regeneration progenitor cell "
            "single cell RNA-seq spatial transcriptomics heart"
        ),
    },

    {
        "name": "Cellular Organization & Processes",
        "tier": 2,
        "description": (
            "cell biology cellular organization organelle "
            "cell membrane cytoskeleton actin tubulin microtubule "
            "nucleus nuclear pore lamina chromatin organization "
            "endoplasmic reticulum ER stress unfolded protein response UPR "
            "Golgi apparatus vesicle trafficking secretory pathway "
            "lysosome autophagy mitophagy selective autophagy "
            "mitochondria fission fusion dynamics biogenesis "
            "proteasome ubiquitin proteasome system UPS "
            "cell signaling pathway kinase phosphatase signal transduction "
            "MAPK PI3K AKT mTOR Wnt Notch Hedgehog Hippo YAP TAZ "
            "cell cycle G1 S G2 M phase CDK cyclin checkpoint "
            "apoptosis caspase BCL2 BAX cytochrome c "
            "necroptosis pyroptosis ferroptosis cell death "
            "cell migration invasion actin dynamics focal adhesion "
            "cell polarity tight junction adherens junction "
            "extracellular matrix ECM collagen fibronectin integrin "
            "mechanobiology mechanotransduction force sensing "
            "phase separation liquid-liquid condensate biomolecular "
            "protein quality control chaperone HSP70 HSP90 "
            "super resolution microscopy live cell imaging CRISPR screen"
        ),
    },

    {
        "name": "Stem Cells",
        "tier": 2,
        "description": (
            "stem cell pluripotent embryonic stem cell ESC "
            "induced pluripotent stem cell iPSC reprogramming Yamanaka "
            "Oct4 Sox2 Klf4 c-Myc pluripotency transcription factor "
            "differentiation lineage commitment cell fate specification "
            "hematopoietic stem cell HSC bone marrow transplant "
            "mesenchymal stem cell MSC stromal cell "
            "neural stem cell progenitor neurogenesis "
            "intestinal stem cell Lgr5 organoid gut epithelium "
            "cancer stem cell tumor initiating cell "
            "self-renewal symmetric asymmetric division niche "
            "Wnt Notch Hedgehog BMP FGF signaling stem cell "
            "epigenetic regulation of stem cell H3K27me3 bivalent "
            "single cell RNA sequencing scRNA-seq trajectory pseudotime "
            "organoid 3D culture patient-derived organoid PDO "
            "regenerative medicine cell therapy gene therapy "
            "CRISPR base editing prime editing stem cell engineering "
            "tissue engineering scaffold biomaterial "
            "clonal dynamics lineage tracing barcode"
        ),
    },
]

# ── Academic / journal signals detected from SERP titles (no URLs needed)
ACADEMIC_TITLE_SIGNALS = [
    # Journals & publishers
    "nature", "science", "cell", "lancet", "nejm", "new england journal",
    "jama", "bmj", "plos", "pnas", "pubmed", "ncbi", "nih", "nci",
    "frontiers", "wiley", "springer", "elsevier", "oxford", "cambridge",
    "annals", "journal of", "american journal", "european journal",
    "proceedings", "clinical cancer research", "cancer research",
    # Institutional
    "fda", "cdc", "who", "ema", "mayo clinic", "cleveland clinic",
    "harvard", "stanford", "mit", "johns hopkins", "oxford university",
    "cancer center", "medical center", "university hospital",
    # Content signals
    "systematic review", "meta-analysis", "randomized controlled",
    "clinical trial", "case report", "cohort study", "phase 1", "phase 2", "phase 3",
]

FORMAT_SIGNALS = {
    "listicle":  r"\b(top \d+|\d+ best|\d+ ways|\d+ tips|list of)\b",
    "guide":     r"\b(guide|how to|tutorial|walkthrough|step.by.step)\b",
    "overview":  r"\b(overview|introduction|what is|explained|understanding)\b",
    "comparison":r"\b(vs\.?|versus|compare|comparison|difference between)\b",
    "clinical":  r"\b(clinical|trial|study|results|efficacy|safety|phase [123])\b",
    "news":      r"\b(new|latest|recent|update|breakthrough|announces?)\b",
}


# ═══════════════════════════════════════════════════════════════════════
# 0. CLI
# ═══════════════════════════════════════════════════════════════════════

def parse_args():
    p = argparse.ArgumentParser(description="Life Science Content Gap Analysis Pipeline")
    p.add_argument("--keywords", required=True,
                   help="CSV: keyword, [volume], [impressions], [clicks], [position]")
    p.add_argument("--serp",     required=True,
                   help="CSV: keyword, title_1..3, [paa_1..3], [snippet]")
    p.add_argument("--output",   default="./output")
    p.add_argument("--kw-col",   default="keyword",
                   help="Keyword column name in keywords CSV (default: 'keyword')")
    p.add_argument("--threshold", type=float, default=0.20,
                   help="Min cosine sim to assign research area (default: 0.20)")
    p.add_argument("--min-cluster", type=int, default=3,
                   help="Min keywords per sub-topic cluster (default: 3)")
    p.add_argument("--top-n",    type=int, default=20,
                   help="Top N research areas in recommendations (default: 20)")
    return p.parse_args()


# ═══════════════════════════════════════════════════════════════════════
# 1. LOAD & MERGE
# ═══════════════════════════════════════════════════════════════════════

def _normalise_keyword(s: str) -> str:
    """
    Robust keyword normalisation for reliable joining.
    Handles: mixed case, extra whitespace, smart quotes, unicode punctuation,
    trailing punctuation, and common encoding artifacts.
    """
    if not isinstance(s, str):
        s = str(s)
    s = s.lower().strip()
    # Replace smart quotes and curly apostrophes
    s = s.replace("\u2018", "'").replace("\u2019", "'")
    s = s.replace("\u201c", '"').replace("\u201d", '"')
    # Collapse multiple spaces/tabs
    s = re.sub(r"\s+", " ", s)
    # Remove trailing punctuation that GSC sometimes appends
    s = s.rstrip(".,;:")
    return s


# ═══════════════════════════════════════════════════════════════════════
# KEYWORD FILTER
# ═══════════════════════════════════════════════════════════════════════
#
# Add terms to filter out here. Three filter types:
#
# BRAND_TERMS   — exact brand names and common misspellings.
#                 Any keyword CONTAINING one of these strings is removed.
#                 All matching is case-insensitive after normalisation.
#
# NAVIGATIONAL  — site names, databases, tools people navigate to directly.
#                 Same substring match logic as BRAND_TERMS.
#
# REGEX_FILTERS — full regex patterns for anything more complex
#                 (e.g. product codes, catalog numbers, email addresses).
#
# To add more: just append to the relevant list.
# ─────────────────────────────────────────────────────────────────────

BRAND_TERMS = [
    # Abcam and misspellings
    "abcam", "abcamm", "abcam's", "abcamn", "ab cam",
    # Thermo Fisher / Invitrogen
    "thermo fisher", "thermofisher", "thermosfisher", "thermo scientific",
    "invitrogen", "gibco", "nunc", "pierce",
    # Cell Signaling Technology
    "cell signaling technology", "cell signaling", "cst antibody", "cellsignal",
    # Bio-Techne / R&D Systems
    "bio-techne", "biotechne", "r&d systems", "r and d systems", "rndsystems",
    # Biolegend
    "biolegend", "bio-legend", "bio legend",
    # Novus Biologicals
    "novus biologicals", "novusbio", "novus bio",
    # Santa Cruz Biotechnology
    "santa cruz biotechnology", "santa cruz biotech", "scbt", "sc-biosciences",
    # Sigma-Aldrich / Merck (commercial reagent context)
    "sigma-aldrich", "sigmaaldrich", "sigma aldrich", "millipore", "emd millipore",
    "emdmillipore", "merck millipore",
    # Miltenyi
    "miltenyi biotec", "miltenyi", "milteny",
    # ProteinTech
    "proteintech", "ptglab",
    # BioCompare
    "biocompare",
    # Other common reagent brands
    "bd biosciences", "bd pharmingen", "becton dickinson",
    "biorad", "bio-rad", "bio rad",
    "qiagen", "promega", "neb", "new england biolabs",
    "lifetech", "life technologies",
    "jackson immunoresearch", "jackson immuno",
    "rockland immunochemicals", "rockland antibodies",
    "bethyl laboratories", "bethyl labs",
    "genscript", "gen script",
    "origene", "sino biological",
    "peprotech", "pepro tech",
    "stemcell technologies", "stemcell tech",
    "lonza", "corning life sciences",
    "cayman chemical", "tocris",
    "enzo life sciences", "enzo biochem",
    "epitomics", "abgent", "abnova",
    "bioss antibodies", "bioss",
    "lsbio", "ls bio",
    "gentex", "creative biolabs",
]

NAVIGATIONAL_TERMS = [
    # Databases and portals people navigate to directly
    "pubmed", "ncbi", "ncbi pubmed", "nih pubmed",
    "uniprot", "ensembl", "kegg", "reactome",
    "gene ontology", "gene cards", "genecards",
    "clinicaltrials.gov", "clinical trials gov", "clinicaltrials",
    "pdb", "rcsb pdb", "protein data bank",
    "string db", "string database", "biogrid", "intact",
    "omim", "online mendelian inheritance",
    "cosmic", "cosmic database",
    "tcga", "geo database", "gdc portal",
    "atlas gtex", "gtex", "human protein atlas",
    "biomart", "ensembl biomart",
    "cellxgene", "single cell portal",
    "addgene", "depositor addgene",
]

REGEX_FILTERS = [
    # Abcam-style product codes: ab + 1-6 digits (must be whole keyword)
    r"^ab\d{1,6}$",
    # Generic catalog codes: 1-4 letters + 4+ digits, optionally with dash
    r"^[a-z]{1,4}[\-]?\d{4,}$",
    # Codes like PA5-12345, GTX-12345, bs-1234R
    r"^[a-z]{2,5}[\-]\d{3,}[a-z]{0,3}$",
    # Pure numeric strings
    r"^\d+$",
    # CAS registry numbers: digits-digits-digit
    r"^\d{2,7}[\-]\d{2,3}[\-]\d$",
    # Lot numbers
    r"^lot\s+\d+",
    # Email addresses
    r"[a-z0-9._%+\-]+@[a-z0-9.\-]+\.[a-z]{2,}",
]

# ── Embedding model
# PubMedBERT fine-tuned on MS-MARCO: understands biomedical entity names,
# drug synonyms, gene/protein relationships far better than general English models.
# Same sentence-transformers API — drop-in replacement.
# First run downloads ~440MB; subsequent runs load from local HuggingFace cache.
# Delete .embedding_cache/ before first run to force re-embedding with new model.
EMBEDDING_MODEL = "pritamdeka/S-PubMedBert-MS-MARCO"

# Compile regex filters once at import time
_COMPILED_REGEX = [re.compile(p) for p in REGEX_FILTERS]

# PAA questions matching these patterns are biographical/crime/celebrity junk
# that bleeds into life-science SERP data via tangential keyword associations.
# e.g. "Did Ted Bundy have MAOA gene" for MAOA-related keywords.
_JUNK_PAA_PATTERNS = re.compile(
    r'\b(who (?:is|was|are|were)|did .{1,30} (?:have|kill|die|commit|go to'
    r'|marry|win|lose|meet)|when did .{1,20} (?:die|born|start|end|happen)'
    r'|where (?:is|was) .{1,20} (?:born|from|live|buried)'
    r'|how (?:old|tall|much|many) (?:is|was) [A-Z]'
    r'|what (?:happened|did) .{1,20} do'
    r'|is .{1,30} (?:a serial|a murderer|a criminal|in jail|dead|alive)'
    r'|celebrity|serial killer|murderer|crime|prison|arrested)\b',
    re.IGNORECASE
)


def _should_filter_keyword(kw: str) -> tuple[bool, str]:
    """
    Returns (True, reason) if keyword should be filtered, (False, "") otherwise.
    kw must already be normalised (lowercase, stripped).
    """
    # Brand/navigational substring match
    for term in BRAND_TERMS:
        if term in kw:
            return True, f"brand: '{term}'"

    for term in NAVIGATIONAL_TERMS:
        if term in kw:
            return True, f"navigational: '{term}'"

    # Regex patterns
    for pattern in _COMPILED_REGEX:
        if pattern.search(kw):
            return True, f"pattern: {pattern.pattern}"

    return False, ""


def apply_keyword_filter(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:
    """
    Apply all keyword filters to a dataframe with a 'keyword' column.
    Logs what was removed and why.
    """
    before = len(df)

    filter_results = df["keyword"].apply(_should_filter_keyword)
    filtered_mask  = filter_results.apply(lambda x: x[0])
    reasons        = filter_results.apply(lambda x: x[1])

    removed = df[filtered_mask].copy()
    kept    = df[~filtered_mask].copy()

    if verbose and len(removed) > 0:
        reason_counts = reasons[filtered_mask].value_counts()
        print(f"    → Filtered {len(removed):,} keywords ({before - len(removed):,} remain)")
        print("    → Filter breakdown:")
        for reason, count in reason_counts.head(10).items():
            print(f"         {count:>5}  {reason}")
        if len(removed) <= 20:
            print(f"    → Removed: {', '.join(removed['keyword'].tolist())}")
        else:
            print(f"    → Sample removed: "
                  f"{', '.join(removed['keyword'].head(10).tolist())} ...")
    elif verbose:
        print(f"    → No keywords filtered ({before:,} remain)")

    return kept


def load_and_merge(kw_path: str, serp_path: str, kw_col: str) -> pd.DataFrame:
    print("\n[1/10] Loading and merging inputs...")

    # ── Keywords (GSC data — contains impressions, clicks, position)
    kw = pd.read_csv(kw_path, dtype=str)   # read as str first — avoids numeric coercion
    if kw_col not in kw.columns:
        raise ValueError(f"Column '{kw_col}' not found in keywords file. "
                         f"Available: {list(kw.columns)}")
    kw = kw.rename(columns={kw_col: "keyword"})
    kw["keyword"]     = kw["keyword"].apply(_normalise_keyword)
    kw["keyword_raw"] = kw["keyword"]   # keep for diagnostics
    kw = kw.drop_duplicates("keyword").reset_index(drop=True)

    # ── Apply keyword filter (brands, navigational, product codes)
    print("    → Applying keyword filters...")
    kw = apply_keyword_filter(kw, verbose=True)

    # Coerce numeric columns NOW before merge so we don't lose them
    for col, default in [("impressions", 0), ("clicks", 0),
                          ("position", 999.0), ("volume", 0)]:
        if col in kw.columns:
            kw[col] = pd.to_numeric(kw[col], errors="coerce").fillna(default)
        else:
            kw[col] = default

    print(f"    → {len(kw):,} keywords loaded")
    print(f"    → GSC coverage: "
          f"{(kw['impressions'] > 0).sum():,} with impressions, "
          f"{(kw['impressions'] == 0).sum():,} with zero impressions")

    # ── SERP data
    serp = pd.read_csv(serp_path, dtype=str)
    serp.columns = [c.strip().lower().replace(" ", "_") for c in serp.columns]

    for candidate in ("keyword", "query", "search_term", "term"):
        if candidate in serp.columns:
            serp = serp.rename(columns={candidate: "keyword"})
            break

    if "keyword" not in serp.columns:
        raise ValueError(f"No keyword column found in SERP file. "
                         f"Available: {list(serp.columns)}")

    serp["keyword"] = serp["keyword"].apply(_normalise_keyword)
    serp = serp.drop_duplicates("keyword").reset_index(drop=True)
    print(f"    → {len(serp):,} SERP rows loaded")

    # ── Diagnose join overlap BEFORE merging
    kw_set   = set(kw["keyword"])
    serp_set = set(serp["keyword"])
    overlap  = kw_set & serp_set
    kw_only  = kw_set - serp_set
    serp_only= serp_set - kw_set

    print(f"\n    Join diagnostics:")
    print(f"      ✓ {len(overlap):,} keywords match in both files")
    print(f"      ✗ {len(kw_only):,} keywords in GSC with no SERP data")
    print(f"      ~ {len(serp_only):,} SERP keywords not in GSC list")

    if len(overlap) == 0:
        print("\n    ⚠️  WARNING: Zero keyword overlap — check column names and encoding.")
        print(f"      GSC sample:  {list(kw['keyword'])[:5]}")
        print(f"      SERP sample: {list(serp['keyword'])[:5]}")
    elif len(overlap) < len(kw_set) * 0.5:
        print(f"\n    ⚠️  WARNING: Only {len(overlap)/len(kw_set)*100:.0f}% match rate.")
        print(f"      GSC sample:  {list(kw['keyword'])[:3]}")
        print(f"      SERP sample: {list(serp['keyword'])[:3]}")

    # ── Merge — left join keeps all GSC keywords
    # Strategy: GSC metric columns (impressions, clicks, position, ctr) should always
    # come from the keywords file (ground truth). But if the keywords file is missing
    # those columns (e.g. a plain keyword list), keep them from SERP if present.
    # Only drop SERP-side columns when the kw file ALREADY has real data for them.
    GSC_COLS = {"impressions", "clicks", "position", "ctr", "volume"}
    serp_conflicts = []
    for c in serp.columns:
        if c not in GSC_COLS:
            continue
        # Only protect/drop from SERP if kw has genuinely non-default values
        kw_has_real = c in kw.columns and (
            pd.to_numeric(kw[c], errors="coerce").fillna(0) > 0
        ).any()
        if kw_has_real:
            serp_conflicts.append(c)

    if serp_conflicts:
        print(f"    → Dropping from SERP file (kw file has these): {serp_conflicts}")
        serp = serp.drop(columns=serp_conflicts)
    else:
        print(f"    → Keeping all SERP columns (kw file has no overlapping metrics)")

    df = kw.merge(serp, on="keyword", how="left")

    # Resolve any remaining _x/_y suffixes — always prefer _x (from kw/GSC)
    for col in [c.replace("_x", "") for c in df.columns if c.endswith("_x")]:
        if f"{col}_x" in df.columns:
            df[col] = df[f"{col}_x"]
            df = df.drop(columns=[f"{col}_x", f"{col}_y"], errors="ignore")

    # Coerce GSC metric columns to numeric after merge — catches any stragglers
    for col, default in [("impressions", 0), ("clicks", 0),
                          ("position", 999.0), ("volume", 0), ("ctr", 0.0)]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(default)
        else:
            df[col] = default

    # ── Verify impressions survived the merge
    with_impressions_before = (kw["impressions"] > 0).sum()
    with_impressions_after  = (df["impressions"] > 0).sum()
    if with_impressions_after < with_impressions_before * 0.95:
        print(f"\n    ⚠️  Impression loss detected: "
              f"{with_impressions_before:,} → {with_impressions_after:,} after merge. "
              f"Check for duplicate keywords in SERP file causing row multiplication.")

    serp_col_check = "title_1" if "title_1" in df.columns else \
                     ("enriched_text" if "enriched_text" in df.columns else None)
    n_with_serp = df[serp_col_check].notna().sum() if serp_col_check else 0
    print(f"\n    → {len(df):,} rows after merge | "
          f"{n_with_serp:,} with SERP data | "
          f"{(df['impressions'] > 0).sum():,} with impressions")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 2. ENRICH: keyword + SERP titles → embedding text
# ═══════════════════════════════════════════════════════════════════════


# ═══════════════════════════════════════════════════════════════════════
# TITLE & ENRICHED TEXT CLEANING
# ═══════════════════════════════════════════════════════════════════════
#
# Two levels of cleaning, applied at different stages:
#
# _clean_title(text)
#   Applied to every individual SERP title before it enters the
#   title corpus (cluster_keywords Stage 5, mine_serp_title_gaps Stage 10).
#   Strips publisher suffixes, boilerplate phrases, site names, encoding
#   artifacts, and very short fragments. Returns None if nothing useful
#   remains (caller should skip the title).
#
# _clean_enriched_text(text)
#   Applied to the full pipe-joined enriched_text string in enrich_text()
#   (Stage 2). Splits on pipe, cleans each segment individually with
#   _clean_title, deduplicates near-identical segments, then re-joins.
#   This is what controls keyword-level embedding quality.
# ─────────────────────────────────────────────────────────────────────

# Publisher / site name suffixes commonly appended to SERP titles.
# Matched case-insensitively. Pattern: " - Publisher" or " | Publisher" at end.
_PUBLISHER_SUFFIXES = re.compile(
    r'[\s\|\-\u2013\u2014]+'
    r'(?:'
    # Exact multi-word publishers first (order matters — longer matches first)
    r'new\s+england\s+journal\s+of\s+medicine|'
    r'new\s+engl\s+j\s+med|'
    r'clinical\s+cancer\s+research|'
    r'nature\s+(?:communications?|medicine|methods|genetics|immunology|'
    r'chemical\s+biology|cell\s+biology|reviews?\s+\w+)|'
    r'cancer\s+(?:research|cell|discovery)|'
    r'journal\s+of\s+(?:clinical\s+oncology|immunology|biological\s+chemistry|'
    r'experimental\s+medicine|cell\s+biology|\w+)|'
    r'annals\s+of\s+(?:oncology|internal\s+medicine|\w+)|'
    r'frontiers\s+in\s+\w+(?:\s+\w+)?|'
    r'cell\s+(?:reports|stem\s+cell|metabolism|host\s+microbe)|'
    # Single-word publishers / institutions
    r'nature|science(?:direct)?|lancet|nejm|jama|bmj|'
    r'plos(?:\s+\w+)?|pnas|pubmed|ncbi|nih|nci|'
    r'wiley(?:\s+online\s+library)?|springer(?:\s+link)?|elsevier|'
    r'oxford(?:\s+academic)?|cambridge(?:\s+core)?|'
    # Hospitals and centers
    r'(?:mayo|cleveland)\s+clinic|'
    r'(?:memorial\s+sloan\s+kettering|msk)\s+cancer\s+center|'
    r'(?:md\s+anderson|dana.farber)\s+cancer\s+(?:center|institute)|'
    r'(?:[\w\s]{3,30})\s+cancer\s+center|'
    r'(?:[\w\s]{3,30})\s+medical\s+center|'
    r'(?:[\w\s]{3,30})\s+university(?:\s+hospital)?|'
    # General high-frequency sites
    r'wikipedia|webmd|healthline|medscape|uptodate|'
    r'verywellhealth|medicalnewstoday|drugs\.com|rxlist|emedicine|'
    r'merck\s+manual[s]?|'
    # Government/regulatory
    r'ncbi\.nlm\.nih\.gov|nih\.gov|cdc\.gov|who\.int|fda\.gov|ema\.europa\.eu|'
    # Generic "site.com" catch-all
    r'(?:[\w\-]+\.(?:com|org|net|edu|gov|io))'
    r')'
    r'\s*$',
    re.IGNORECASE,
)

# Boilerplate title suffixes that add no semantic content
_BOILERPLATE_SUFFIXES = re.compile(
    r'[\s\|\-\u2013\u2014:]*'
    r'(?:'
    r'(?:a\s+)?complete\s+guide|'
    r'everything\s+you\s+need\s+to\s+know|'
    r'(?:the\s+)?(?:ultimate|comprehensive|definitive|essential)\s+guide|'
    r'what\s+you\s+need\s+to\s+know|'
    r'all\s+you\s+need\s+to\s+know|'
    r'updated?\s+(?:in\s+)?20\d\d|'
    r'(?:last\s+)?updated\s+20\d\d|'
    r'(?:new\s+in\s+)?20\d\d|'
    # Only match "overview/review/summary" when preceded by at least one qualifier
    r'(?:full\s+|complete\s+|detailed\s+|in.depth\s+)?'
    r'(?:systematic\s+review|literature\s+review|narrative\s+review|'
    r'scoping\s+review|research\s+overview|research\s+article|'
    r'research\s+review|clinical\s+overview|clinical\s+review)|'
    r'current\s+evidence\s+and\s+research\s+directions|'
    r'evidence[,\s]+mechanisms?\s+and\s+research\s+implications|'
    r'mechanisms?,?\s+current\s+evidence\s+and\s+research\s+directions|'
    r'research\s+overview,?\s+methods?\s+and\s+key\s+findings|'
    r'a\s+deep.dive\s+into\s+mechanisms?[,\s]+current\s+evidence[^$]*|'
    r'read\s+more|learn\s+more|click\s+here|find\s+out\s+more|'
    r'(?:free\s+)?(?:pdf|download)'
    r')'
    r'\s*$',
    re.IGNORECASE,
)

# HTML entities
_HTML_ENTITY_MAP = {
    "&amp;": "&", "&lt;": "<", "&gt;": ">",
    "&quot;": '"', "&apos;": "'", "&nbsp;": " ",
}
_HTML_ENTITIES = re.compile(r'&(?:amp|lt|gt|quot|apos|nbsp|#\d+|#x[0-9a-fA-F]+);')

# URL pattern
_URL_RE = re.compile(
    r'https?://\S+|www\.\S+|\S+\.(?:com|org|net|edu|gov|io)/\S*',
    re.IGNORECASE,
)

# Encoding artifacts: raw mojibake and hex escapes in string data
_ENCODING_ARTIFACTS = re.compile(
    r'â€[^\s]|'           # UTF-8 apostrophe mojibake (â€™ etc)
    r'Ã[\x80-\xff]|'      # Latin-1 mojibake
    r'\\x[0-9a-fA-F]{2}'  # literal \x84 style escapes
)

# Numeric-only or short catalog code strings
_CODE_RE = re.compile(r'^(?:\d+|[a-z]{1,3}\d{3,}|[a-z0-9\-]{1,4})$', re.IGNORECASE)

_MIN_SEGMENT_LEN = 12


def _clean_title(text: str):
    """
    Clean a single SERP title for use in the title corpus.

    Returns cleaned string, or None if nothing meaningful remains.

    Steps:
    1. Reject null/nan/empty literals
    2. Decode HTML entities and fix encoding artifacts
    3. Strip URLs
    4. Strip publisher name suffixes (iteratively — can be stacked)
    5. Strip boilerplate suffixes
    6. Normalise whitespace
    7. Return None if result is a catalog code or under _MIN_SEGMENT_LEN chars
    """
    if not text or not isinstance(text, str):
        return None

    t = text.strip()

    if t.lower() in ("null", "nan", "none", "n/a", "na", "", "-"):
        return None

    # Decode HTML entities
    def _entity_replace(m):
        return _HTML_ENTITY_MAP.get(m.group(0), "")
    t = _HTML_ENTITIES.sub(_entity_replace, t)

    # Fix encoding artifacts
    t = _ENCODING_ARTIFACTS.sub("", t)

    # Strip URLs
    t = _URL_RE.sub("", t).strip()

    # Iteratively strip publisher suffixes (can be layered: "Title - NCI | PubMed")
    for _ in range(4):
        prev = t
        t = _PUBLISHER_SUFFIXES.sub("", t).strip(" |-\u2013\u2014:")
        if t == prev:
            break

    # Strip boilerplate suffixes
    t = _BOILERPLATE_SUFFIXES.sub("", t).strip(" |-\u2013\u2014:")

    # Normalise whitespace
    t = re.sub(r'\s+', ' ', t).strip()

    if not t or _CODE_RE.match(t) or len(t) < _MIN_SEGMENT_LEN:
        return None

    # Reject titles where a brand name appears as a full word —
    # these are product/vendor pages, not useful content signals
    t_lower = t.lower()
    if any(
        re.search(r'\b' + re.escape(term) + r'\b', t_lower)
        for term in BRAND_TERMS
        if len(term) > 4
    ):
        return None

    return t


def _normalise_for_dedup(text: str) -> str:
    """Lowercase, strip punctuation, collapse spaces — for near-duplicate detection."""
    t = text.lower()
    t = re.sub(r'[^a-z0-9\s]', ' ', t)
    t = re.sub(r'\s+', ' ', t).strip()
    return t


def _char_trigram_sim(a: str, b: str) -> float:
    """
    Character trigram similarity — handles plurals, stemming variants,
    minor rewordings better than word-level Jaccard.
    Returns 0.0-1.0 (Dice coefficient on trigram sets).
    """
    def trigrams(s):
        s = s.replace(" ", "_")  # treat spaces as char for boundary trigrams
        return set(s[i:i+3] for i in range(len(s) - 2))
    ta, tb = trigrams(a), trigrams(b)
    if not ta or not tb:
        return 0.0
    return 2 * len(ta & tb) / (len(ta) + len(tb))


def _token_overlap(a: str, b: str) -> float:
    """Jaccard similarity on word token sets."""
    sa = set(a.split())
    sb = set(b.split())
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / len(sa | sb)


def _clean_enriched_text(text: str, keyword: str) -> str:
    """
    Clean a full pipe-joined enriched_text string from BigQuery string_agg.

    Steps:
    1. Split on "|"
    2. First segment is the keyword — always keep as anchor
    3. Clean every subsequent segment with _clean_title
    4. Near-duplicate deduplication:
       a. Exact normalised match → skip
       b. Token Jaccard ≥ 0.85 → skip (catches plural/minor variants)
       c. One is ≤50% length of the other and contained within it → skip
    5. Re-join with " | "

    Returns at minimum just the keyword if all SERP segments are cleaned away.
    """
    if not text or not isinstance(text, str):
        return keyword

    parts = [p.strip() for p in text.split("|")]
    if not parts:
        return keyword

    cleaned = [keyword]
    seen_normalised = {_normalise_for_dedup(keyword)}
    # Separate pool for trigram/substring dedup — excludes keyword itself
    # (keyword is always short and would suppress legitimate longer titles
    # e.g. "flow cytometry" would suppress "Flow Cytometry Applications")
    dedup_pool = []

    for part in parts[1:]:
        c = _clean_title(part)
        if c is None:
            continue
        norm = _normalise_for_dedup(c)

        # a. Exact normalised match (checked against all including keyword)
        if norm in seen_normalised:
            continue

        # b. High character trigram similarity against OTHER titles (not keyword)
        # Threshold 0.85 Dice on trigrams catches "Inhibitor" vs "Inhibitors",
        # rewording of the same phrase, etc. while preserving distinct angles.
        if any(_char_trigram_sim(norm, s) >= 0.85 for s in dedup_pool if len(s) > 8):
            continue

        # c. Strict substring containment — one is trivially shorter than the other
        if any(
            (norm in s and len(norm) <= len(s) * 0.5)
            or (s in norm and len(s) <= len(norm) * 0.5)
            for s in dedup_pool
            if len(s) > 8 and len(norm) > 8
        ):
            continue

        cleaned.append(c)
        seen_normalised.add(norm)
        dedup_pool.append(norm)

    return " | ".join(cleaned)



def enrich_text(df: pd.DataFrame) -> pd.DataFrame:
    """
    Two modes depending on what your SERP CSV contains:

    MODE A — Pre-built from BigQuery (preferred):
      If your serp_data.csv already has an 'enriched_text' column
      (keyword + all page-2 titles concatenated in BQ), use it directly.
      Also uses 'all_paa_questions' column if present.

    MODE B — Build from wide columns (fallback):
      If your CSV has title_1, title_2 ... title_N columns, concatenates
      ALL of them (no 3-title cap) plus all paa_1 ... paa_N columns.

    The enriched_text is what gets embedded — it encodes Google's full
    interpretation of each keyword across up to 20 organic results.
    """
    print("\n[2/8] Enriching keywords with SERP titles...")

    # ── MODE A: pre-built enriched_text column from BigQuery
    if "enriched_text" in df.columns:
        # Clean and deduplicate each pipe-joined string_agg field
        # This strips: URLs, publisher suffixes, boilerplate, null/nan segments,
        # encoding artifacts, and near-duplicate titles before embedding
        print(f"    → Using pre-built enriched_text column (BigQuery mode)")
        print(f"    → Cleaning string_agg output (URLs, publisher suffixes, dedup)...")
        df["enriched_text"] = df.apply(
            lambda row: _clean_enriched_text(
                str(row["enriched_text"])
                if pd.notna(row["enriched_text"]) and str(row["enriched_text"]).strip()
                else "",
                row["keyword"],
            ),
            axis=1,
        )
        enriched = (df["enriched_text"].str.count(r"\|") > 0).sum()
        avg_titles = (
            df["enriched_text"].str.count(r"\|").mean()
        )
        print(f"    → {enriched:,}/{len(df):,} keywords have SERP title context "
              f"(avg {avg_titles:.1f} titles per keyword after cleaning)")

        # PAA: use pre-built all_paa_questions column if present
        if "all_paa_questions" in df.columns:
            # Clean PAA questions — strip nulls, encoding artifacts, very short fragments
            def _clean_paa_str(paa_text):
                if not paa_text or not isinstance(paa_text, str) or not paa_text.strip():
                    return ""
                questions = []
                seen = set()
                for q in paa_text.split("|"):
                    q = q.strip()
                    if not q or q.lower() in ("null", "nan", "none", ""):
                        continue
                    # Decode HTML entities
                    q = _HTML_ENTITIES.sub(lambda m: _HTML_ENTITY_MAP.get(m.group(0), ""), q)
                    q = _ENCODING_ARTIFACTS.sub("", q).strip()
                    if len(q) < 10:
                        continue
                    norm = _normalise_for_dedup(q)
                    if norm in seen:
                        continue
                    seen.add(norm)
                    questions.append(q)
                return " | ".join(questions)

            df["paa_questions"] = df["all_paa_questions"].fillna("").apply(_clean_paa_str)
            paa_count = (df["paa_questions"].str.strip() != "").sum()
            print(f"    → {paa_count:,}/{len(df):,} keywords have PAA questions after cleaning")
        else:
            df["paa_questions"] = ""

        return df

    # ── MODE B: build from wide title_N / paa_N columns
    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)],
                        key=lambda c: int(re.search(r"\d+", c).group()))
    paa_cols   = sorted([c for c in df.columns if re.match(r"paa_\d+",   c)],
                        key=lambda c: int(re.search(r"\d+", c).group()))

    print(f"    → Building from {len(title_cols)} title columns + {len(paa_cols)} PAA columns")

    def build_text(row):
        parts = [str(row["keyword"])]
        for col in title_cols:
            val = row.get(col, "")
            cleaned = _clean_title(str(val)) if pd.notna(val) else None
            if cleaned:
                parts.append(cleaned)
        return " | ".join(parts)

    def build_paa(row):
        questions = []
        for col in paa_cols:            # ALL PAA columns, no cap
            val = row.get(col, "")
            if pd.notna(val) and str(val).strip():
                questions.append(str(val).strip())
        return " | ".join(questions)

    df["enriched_text"] = df.apply(build_text, axis=1)
    df["paa_questions"]  = df.apply(build_paa,  axis=1)

    enriched  = (df["enriched_text"].str.count(r"\|") > 0).sum()
    paa_count = (df["paa_questions"].str.strip() != "").sum()
    print(f"    → {enriched}/{len(df)} keywords enriched with SERP titles")
    print(f"    → {len(df)-enriched} keywords using bare keyword only")
    print(f"    → {paa_count}/{len(df)} keywords have PAA questions")
    return df


# ═══════════════════════════════════════════════════════════════════════
# 3. EMBED
# ═══════════════════════════════════════════════════════════════════════

def embed_all(df: pd.DataFrame, cache_dir: str):
    print("\n[3/8] Generating embeddings...")
    try:
        from sentence_transformers import SentenceTransformer
    except ImportError:
        raise ImportError("Run: pip install sentence-transformers")

    print(f"    → Loading model: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)

    kw_emb = cached_encode(
        df["enriched_text"].tolist(), model, cache_dir,
        label="keyword enriched texts", batch_size=64,   # smaller batch — larger model
    )

    area_emb = cached_encode(
        [a["description"] for a in RESEARCH_AREAS], model, cache_dir,
        label="research area taxonomy", batch_size=64,
        show_progress_bar=False,
    )

    print(f"    → Keyword embeddings: {kw_emb.shape} | Area embeddings: {area_emb.shape}")
    return kw_emb, area_emb, model


# ═══════════════════════════════════════════════════════════════════════
# 4. RESEARCH AREA MAPPING
# ═══════════════════════════════════════════════════════════════════════

def map_research_areas(df: pd.DataFrame, kw_emb: np.ndarray,
                        area_emb: np.ndarray, threshold: float) -> pd.DataFrame:
    print(f"\n[4/8] Mapping to research areas (threshold={threshold})...")

    sim        = kw_emb @ area_emb.T          # (n_kw, n_areas)
    best_idx   = sim.argmax(axis=1)
    best_score = sim.max(axis=1)

    sim2       = sim.copy()
    sim2[np.arange(len(sim2)), best_idx] = -1
    second_idx = sim2.argmax(axis=1)

    area_names = [a["name"] for a in RESEARCH_AREAS]

    df["research_area"]       = [
        area_names[i] if s >= threshold else "Unclassified / General Methods"
        for i, s in zip(best_idx, best_score)
    ]
    df["research_area_score"] = best_score.round(4)
    df["research_area_2"]     = [area_names[i] for i in second_idx]

    classified = (df["research_area"] != "Unclassified / General Methods").sum()
    print(f"    → {classified}/{len(df)} keywords classified")

    print("\n    Research area keyword distribution:")
    for area, cnt in df["research_area"].value_counts().items():
        bar = "█" * min(35, max(1, cnt * 35 // len(df)))
        print(f"      {cnt:>5}  {bar}  {area}")

    return df


def _extract_cluster_theme(texts: list) -> str:
    """
    Derive a short descriptive label from a list of titles or snippets
    representing one cluster.

    Strategy:
    1. Find the longest n-gram (2-5 words) that appears in ≥2 of the texts
       AND contains at least one non-stopword — this is the shared theme.
    2. If no recurring n-gram, return the shortest text after stripping
       stop-word-only phrases and brand tokens.
    3. Cap at 60 characters for display.
    """
    import itertools

    STOP = {
        # Articles / determiners
        "a","an","the","this","that","these","those","each","every","both","all",
        "any","some","such","no","neither","either",
        # Prepositions
        "of","in","to","for","on","with","at","by","from","into","onto","upon",
        "about","above","below","between","through","during","within","without",
        "against","among","along","across","around","behind","beyond","beside",
        "before","after","since","until","towards","toward","via","per","as",
        # Conjunctions
        "and","or","but","nor","so","yet","if","than","then","though","although",
        "because","while","when","where","whereas","unless","whether",
        # Pronouns
        "it","its","their","our","your","we","they","he","she","you","i","them",
        "him","her","us","who","which","what","whom","whose",
        # Auxiliary verbs
        "is","are","was","were","be","been","being","have","has","had","do","does",
        "did","can","could","may","might","will","would","should","shall","must",
        "need","ought","used",
        # Question / relative words
        "how","why","when","where","not","never","ever",
        # Common filler adjectives / adverbs
        "new","more","most","also","only","just","very","much","many","few",
        "several","various","different","other","same","own","key","main",
        "major","common","important","potential","current","recent","next",
        "first","second","third","last","long","short","high","low","large",
        "small","early","late","full","whole","certain","specific","general",
        # Common connective phrases (single tokens after split)
        "overview","introduction","review","update","guide","article","paper",
        "study","research","analysis","report","data","results","findings",
        "information","details","summary","notes","based","using","used",
        "include","including","related","associated","compared","known",
        "defined","called","named",
        # Number words
        "one","two","three","four","five","six","seven","eight","nine","ten",
        # Punctuation-like tokens that slip through
        "vs","versus","vs.",
    }

    # Brand tokens — any n-gram containing these as a word is rejected
    # (built from the first word of each BRAND_TERMS entry to avoid full-string matching)
    _BRAND_TOKENS = {
        w.lower()
        for term in BRAND_TERMS
        for w in term.split()
        if len(w) > 2
    } | {
        # Common antibody/reagent brand fragments that wouldn't appear in BRAND_TERMS
        "abcam","thermofisher","thermo","invitrogen","sigma","aldrich",
        "biolegend","novus","scbt","santacruz","rndsystems","ptglab",
        "cellsignal","biocompare","emdmillipore","millipore","qiagen",
        "promega","neb","genscript","origene","peprotech","cayman","tocris",
        "rockland","bethyl","epitomics","abnova","bioss","lsbio","gentex",
    }

    def _is_brand_free(ng: str) -> bool:
        """Return False if any token in the n-gram is a brand token."""
        return not any(w in _BRAND_TOKENS for w in ng.split())

    def _clean_for_display(text: str) -> str:
        """
        Strip leading/trailing stop words from a title so the fallback
        label doesn't start with 'What Is' or 'How To'.
        """
        words = text.split()
        # Strip leading stop words
        while words and words[0].lower() in STOP:
            words = words[1:]
        # Strip trailing stop words
        while words and words[-1].lower() in STOP:
            words = words[:-1]
        # Strip leading brand tokens
        while words and words[0].lower() in _BRAND_TOKENS:
            words = words[1:]
        return " ".join(words)

    texts = [str(t).strip() for t in texts if t and len(str(t).strip()) > 5]
    if not texts:
        return "Unknown cluster"

    # Normalise: lowercase, collapse spaces, normalise hyphens to spaces
    # so "car-t" and "car t" count as the same token for n-gram matching
    normed = [re.sub(r"\s+", " ", t.lower().replace("-", " ")) for t in texts]

    # Extract n-grams (2-5 words) from each text
    # Pattern allows hyphenated bio terms: pd-1, car-t, il-6, kras-g12c
    def ngrams(text, n):
        words = re.findall(r"\b[a-z][a-z0-9\-]*[a-z0-9]\b|\b[a-z]{2,}\b", text)
        words = [w for w in words if len(w) >= 2]
        return [" ".join(words[i:i+n]) for i in range(len(words)-n+1)]

    # Count n-gram frequency across all texts — track both raw count and
    # distinct-text count (how many different texts contain this n-gram)
    ngram_counts    = Counter()   # total occurrences
    ngram_texts     = Counter()   # distinct texts containing n-gram
    for text in normed:
        seen = set()
        for n in range(5, 1, -1):
            for ng in ngrams(text, n):
                if ng not in seen:
                    ngram_texts[ng]  += 1
                    seen.add(ng)
                ngram_counts[ng] += 1

    # Filter: must appear in ≥2 distinct texts, must have ≥1 non-stopword,
    # must not start/end with a stop word, must not contain brand tokens
    candidates = [
        (ng, ngram_texts[ng]) for ng in ngram_texts
        if ngram_texts[ng] >= 2
        and any(w not in STOP for w in ng.split())
        and not all(w in STOP for w in ng.split())
        and ng.split()[0]  not in STOP    # don't start with stop word
        and ng.split()[-1] not in STOP    # don't end with stop word
        and _is_brand_free(ng)            # no brand tokens
    ]

    if candidates:
        # Primary: distinct-text count (how many titles share this phrase)
        # Secondary: n-gram length (more specific = better)
        best  = sorted(candidates, key=lambda x: (x[1], len(x[0])), reverse=True)
        label = _clean_for_display(best[0][0]).title()
        if not label:
            label = best[0][0].title()
        return label[:60] + "..." if len(label) > 60 else label

    # Fallback: clean the shortest text before returning
    fallback = min(texts, key=len)
    fallback = _clean_for_display(fallback)
    if not fallback:
        # Everything was stop words — just return the raw shortest
        fallback = min(texts, key=len)
    return fallback[:60] + "..." if len(fallback) > 60 else fallback


# ═══════════════════════════════════════════════════════════════════════
# 5. TITLE-FIRST CLUSTERING + AI OVERVIEW CLUSTERING
# ═══════════════════════════════════════════════════════════════════════


def _reduce_embeddings(embs: np.ndarray, n_components: int = 15,
                       have_umap: bool = True) -> np.ndarray:
    """
    Reduce high-dimensional embeddings for clustering.

    UMAP (preferred): non-linear, preserves local neighbourhood structure,
    far better than linear SVD at keeping biologically-related concepts close.
    n_components=15 is a good balance — enough for HDBSCAN to find structure,
    not so many that density estimation degrades.

    TruncatedSVD (fallback): linear, faster, but discards fine-grained
    biological distinctions that live in lower-variance dimensions.
    """
    from sklearn.preprocessing import normalize

    n, dim = embs.shape
    n_comp = min(n_components, dim - 1, n - 2)

    if have_umap:
        import umap as umap_lib
        reducer = umap_lib.UMAP(
            n_components    = n_comp,
            metric          = "cosine",
            n_neighbors     = max(5, min(30, n // 20)),
            min_dist        = 0.0,    # tighter clusters — better for HDBSCAN
            random_state    = 42,
            low_memory      = True,
        )
        return reducer.fit_transform(embs)
    else:
        from sklearn.decomposition import TruncatedSVD
        reduced = TruncatedSVD(n_comp, random_state=42).fit_transform(embs)
        return normalize(reduced)


def _cluster_embeddings(reduced: np.ndarray, min_size: int,
                        have_hdbscan: bool = True) -> np.ndarray:
    """
    Cluster reduced embeddings.

    HDBSCAN (preferred): density-based, finds natural cluster count,
    marks genuine outliers as -1 rather than forcing them into a cluster.
    min_cluster_size controls the smallest meaningful cluster.

    AgglomerativeClustering (fallback): requires fixed k, uses
    n // 15 heuristic. Produces reasonable but less semantically
    coherent clusters than HDBSCAN on biomedical text.
    """
    n = len(reduced)

    if have_hdbscan:
        import hdbscan as hdbscan_lib
        clusterer = hdbscan_lib.HDBSCAN(
            min_cluster_size    = max(min_size, 3),
            min_samples         = 2,
            metric              = "euclidean",   # on UMAP output = cosine-aware
            cluster_selection_method = "eom",    # excess of mass — better for varied densities
            prediction_data     = True,
        )
        return clusterer.fit_predict(reduced)
    else:
        from sklearn.cluster import AgglomerativeClustering, KMeans
        k = max(10, min(120, n // 15))
        if n <= 8000:
            return AgglomerativeClustering(
                n_clusters=k, metric="cosine", linkage="average"
            ).fit_predict(reduced)
        else:
            return KMeans(
                n_clusters=k, random_state=42, n_init="auto"
            ).fit_predict(reduced)


def cluster_keywords(df: pd.DataFrame, kw_emb: np.ndarray,
                     min_cluster: int, model) -> pd.DataFrame:
    """
    Title-first clustering — fundamentally different from keyword clustering.

    OLD approach: embed keyword → cluster keywords → one cluster per keyword
    NEW approach: embed each organic title → cluster titles → map clusters
                  back to keywords. One keyword can belong to multiple clusters
                  if its titles span multiple themes.

    Why this is better:
      "KRAS G12C inhibitor" as a keyword is ambiguous — is it about mechanism,
      clinical trials, or resistance? Its titles tell you: title_1 might be about
      mechanism (→ cluster A), title_2 about a trial (→ cluster B), title_3 about
      resistance (→ cluster C). The keyword now has three cluster memberships,
      each representing a distinct content opportunity.

    Output columns added to df:
      title_cluster_ids     — comma-separated cluster IDs for all titles of this keyword
      title_cluster_labels  — comma-separated cluster labels (most rep. title per cluster)
      primary_cluster_id    — cluster of the keyword's most representative title (for t-SNE)
      primary_cluster_label — label of primary cluster
      aio_cluster_id        — AI Overview reference snippet cluster (if data available)
      aio_cluster_label     — label of AI Overview cluster

    Also produces a separate title_clusters.csv mapping table:
      title | keyword | cluster_id | cluster_label
    """
    print("\n[5] Title-first clustering...")

    from sklearn.preprocessing import normalize
    from sklearn.metrics import pairwise_distances_argmin_min
    from sentence_transformers import SentenceTransformer

    print(f"    → Loading model: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)

    try:
        import umap
        _HAVE_UMAP = True
    except ImportError:
        _HAVE_UMAP = False
        print("    ⚠ umap-learn not installed — falling back to TruncatedSVD")
        print("      Install with: pip install umap-learn")

    try:
        import hdbscan as hdbscan_lib
        _HAVE_HDBSCAN = True
    except ImportError:
        _HAVE_HDBSCAN = False
        print("    ⚠ hdbscan not installed — falling back to AgglomerativeClustering")
        print("      Install with: pip install hdbscan")

    # ─────────────────────────────────────────────────────────────────
    # PART A: Organic title clustering
    # ─────────────────────────────────────────────────────────────────

    # Step 1: Extract all individual titles with their parent keyword
    title_cols = sorted(
        [c for c in df.columns if re.match(r"title_\d+", c)],
        key=lambda c: int(re.search(r"\d+", c).group())
    )

    title_rows = []   # list of (title_text, keyword, row_index, title_position)

    for row_idx, row in df.iterrows():
        # BQ mode: enriched_text is "keyword | title1 | title2 | ..."
        if "enriched_text" in df.columns and "|" in str(row.get("enriched_text", "")):
            parts = [p.strip() for p in str(row["enriched_text"]).split("|")]
            titles = parts[1:]   # skip keyword itself
        else:
            titles = [
                str(row.get(c, "")).strip()
                for c in title_cols
                if pd.notna(row.get(c)) and str(row.get(c, "")).strip()
            ]

        for pos, title in enumerate(titles):
            cleaned = _clean_title(title)
            if cleaned:
                title_rows.append({
                    "title":     cleaned,
                    "keyword":   row["keyword"],
                    "row_idx":   row_idx,
                    "title_pos": pos,
                })

    if not title_rows:
        print("    ⚠ No titles found — falling back to keyword-level clustering")
        n      = len(df)
        reduced = _reduce_embeddings(kw_emb, n_components=10, have_umap=_HAVE_UMAP)
        labels  = _cluster_embeddings(reduced, min_cluster, have_hdbscan=_HAVE_HDBSCAN)
        label_map = {}
        for cid in np.unique(labels):
            if cid == -1:
                continue
            mask     = labels == cid
            embs     = reduced[mask]
            centroid = embs.mean(axis=0)
            closest  = df.iloc[np.where(mask)[0]]["keyword"].iloc[
                (embs @ centroid).argmax()
            ]
            label_map[cid] = closest
        # Noise points (-1) get assigned to nearest valid cluster
        if -1 in np.unique(labels):
            noise_mask  = labels == -1
            valid_mask  = labels != -1
            if valid_mask.sum() > 0:
                nearest, _ = pairwise_distances_argmin_min(
                    reduced[noise_mask], reduced[valid_mask]
                )
                labels_arr        = labels.copy()
                labels_arr[noise_mask] = labels[valid_mask][nearest]
                labels = labels_arr
                for cid in np.unique(labels):
                    if cid not in label_map:
                        mask = labels == cid
                        embs = reduced[mask]
                        centroid = embs.mean(axis=0)
                        label_map[cid] = df.iloc[np.where(mask)[0]]["keyword"].iloc[
                            (embs @ centroid).argmax()
                        ]
        df["primary_cluster_id"]    = labels
        df["primary_cluster_label"] = [label_map.get(c, "Unknown") for c in labels]
        df["title_cluster_ids"]     = df["primary_cluster_id"].astype(str)
        df["title_cluster_labels"]  = df["primary_cluster_label"]
        return df

    titles_df = pd.DataFrame(title_rows)
    n_titles  = len(titles_df)
    print(f"    → {n_titles:,} individual titles extracted from "
          f"{len(df):,} keywords")

    # Step 2: Embed unique titles (deduplicate to save compute)
    unique_titles = titles_df["title"].drop_duplicates().tolist()
    title_embs = cached_encode(
        unique_titles, model, _EMB_CACHE_DIR,
        label=f"cluster titles ({len(unique_titles):,} unique)",
        batch_size=256,
    )
    title_emb_map = {t: e for t, e in zip(unique_titles, title_embs)}

    # Step 3: Cluster the title corpus
    all_embs = np.array([title_emb_map[t] for t in titles_df["title"]])

    print(f"    → Reducing {n_titles:,} title embeddings...")
    reduced = _reduce_embeddings(all_embs, n_components=15, have_umap=_HAVE_UMAP)

    print(f"    → Clustering with HDBSCAN (auto k)..." if _HAVE_HDBSCAN else
          f"    → Clustering with AgglomerativeClustering (k={max(10, min(120, n_titles//15))})...")
    labels = _cluster_embeddings(reduced, min_cluster, have_hdbscan=_HAVE_HDBSCAN)

    titles_df["cluster_id"] = labels

    # Assign HDBSCAN noise points (-1) to nearest valid cluster
    # (For AgglomerativeClustering fallback, merge tiny clusters instead)
    if _HAVE_HDBSCAN:
        noise_mask = labels == -1
        if noise_mask.sum() > 0:
            valid_mask = labels != -1
            if valid_mask.sum() > 0:
                nearest, _ = pairwise_distances_argmin_min(
                    reduced[noise_mask], reduced[valid_mask]
                )
                labels_arr           = np.array(labels)
                labels_arr[noise_mask] = labels_arr[valid_mask][nearest]
                titles_df["cluster_id"] = labels_arr
                labels = labels_arr
            print(f"    → {noise_mask.sum()} noise titles assigned to nearest cluster")
    else:
        counts = pd.Series(labels).value_counts()
        small  = counts[counts < min_cluster].index.tolist()
        if small:
            vm = ~np.isin(labels, small)
            sm =  np.isin(labels, small)
            if vm.sum() > 0 and sm.sum() > 0:
                nearest, _ = pairwise_distances_argmin_min(
                    reduced[sm], reduced[vm], metric="cosine"
                )
                labels_arr = np.array(labels)
                labels_arr[sm] = labels_arr[vm][nearest]
                titles_df["cluster_id"] = labels_arr
            print(f"    → Merged {len(small)} tiny clusters")

    # Step 4: Label each cluster by extracting the core theme across its
    # top-3 most centroid-representative titles.
    # Strategy: find the longest common meaningful phrase across top titles;
    # fall back to the single most representative title if no phrase found.
    cluster_label_map = {}
    for cid in titles_df["cluster_id"].unique():
        mask     = titles_df["cluster_id"] == cid
        c_embs   = reduced[mask.values]
        centroid = c_embs.mean(axis=0)
        sims     = c_embs @ centroid
        top_idx  = sims.argsort()[::-1][:5]
        top_titles = titles_df.loc[mask, "title"].iloc[top_idx].tolist()

        # Find common meaningful n-gram across top titles
        label = _extract_cluster_theme(top_titles)
        cluster_label_map[cid] = label

    titles_df["cluster_label"] = titles_df["cluster_id"].map(cluster_label_map)

    n_clusters = titles_df["cluster_id"].nunique()
    print(f"    → {n_clusters} title clusters produced")
    # Separator: " || " (double pipe — single pipe already used in title text)
    # primary_cluster = cluster of the keyword's highest-position title (title_1)
    # cluster_ids / cluster_labels = all clusters spanned, deduped, position-ordered
    kw_clusters = (
        titles_df
        .sort_values("title_pos")
        .drop_duplicates(["keyword", "cluster_id"])
        .groupby("keyword", sort=False)
        .apply(lambda g: list(zip(g["cluster_id"].tolist(),
                                   g["cluster_label"].tolist())))
        .to_dict()
    )

    primary_id_list    = []
    primary_label_list = []
    all_ids_list       = []
    all_labels_list    = []

    for _, row in df.iterrows():
        clusters = kw_clusters.get(row["keyword"], [])
        if not clusters:
            primary_id_list.append(-1)
            primary_label_list.append("No title data")
            all_ids_list.append("")
            all_labels_list.append("")
            continue
        ids    = [str(c[0]) for c in clusters]
        labels_kw = [c[1] for c in clusters]
        primary_id_list.append(int(ids[0]))
        primary_label_list.append(labels_kw[0])
        all_ids_list.append(" || ".join(ids))
        all_labels_list.append(" || ".join(labels_kw))

    df["primary_cluster_id"]    = primary_id_list
    df["primary_cluster_label"] = primary_label_list
    df["cluster_ids"]           = all_ids_list     # e.g. "4 || 12 || 27"
    df["cluster_labels"]        = all_labels_list  # e.g. "CAR-T mechanism || PD-1 trial || ..."

    # Backward-compat aliases used by downstream stages
    df["cluster_id"]    = df["primary_cluster_id"]
    df["cluster_label"] = df["primary_cluster_label"]

    multi = (df["cluster_ids"].str.contains(r"\|\|", na=False)).sum()
    avg_n = df["cluster_ids"].str.count(r"\|\|").add(1).mean()
    print(f"    → {multi:,} keywords span multiple title clusters")
    print(f"    → Avg clusters per keyword: {avg_n:.1f}")

    # ─────────────────────────────────────────────────────────────────
    # PART B: AI Overview / reference snippet clustering
    #
    # Input: a single column of pipe-separated snippet texts per keyword,
    # matching the same concat pattern as enriched_text / titles.
    # e.g. "Immunotherapy uses T cells | PD-1 blocks immune checkpoints | ..."
    #
    # Accepted column names: aio_snippets, ai_overview, aio_snippet,
    #                        reference_snippets, ai_snippets
    # ─────────────────────────────────────────────────────────────────
    print("\n    [PART B] AI Overview clustering...")

    aio_col_candidates = [
        "aio_snippets", "aio_snippet", "ai_overview",
        "reference_snippets", "ai_snippets",
    ]
    aio_text_col = next(
        (c for c in aio_col_candidates if c in df.columns), None
    )
    # Also accept any column matching aio_* or ai_overview_*
    if not aio_text_col:
        aio_text_col = next(
            (c for c in df.columns
             if re.match(r"(aio|ai_overview|reference_snippet)", c)), None
        )

    if not aio_text_col:
        print("    → No AI Overview column found "
              f"(tried: {', '.join(aio_col_candidates)})")
        df["aio_cluster_id"]    = -1
        df["aio_cluster_label"] = "No AI Overview"
    else:
        print(f"    → Using column: '{aio_text_col}'")

        # Explode pipe-separated snippets into individual rows
        # (same approach as title exploding in Part A)
        aio_rows = []
        for row_idx, row in df.iterrows():
            raw = str(row.get(aio_text_col, "")).strip()
            if not raw:
                continue
            snippets = [s.strip() for s in raw.split("|") if s.strip()]
            for pos, snippet in enumerate(snippets):
                if len(snippet) > 15:   # ignore very short fragments
                    aio_rows.append({
                        "snippet":  snippet,
                        "keyword":  row["keyword"],
                        "row_idx":  row_idx,
                        "snip_pos": pos,
                    })

        if len(aio_rows) < 5:
            print(f"    → Too few AIO snippets to cluster ({len(aio_rows)})")
            df["aio_cluster_id"]    = -1
            df["aio_cluster_label"] = "No AI Overview"
        else:
            aio_df   = pd.DataFrame(aio_rows)
            n_snips  = len(aio_df)
            n_kw_aio = aio_df["keyword"].nunique()
            print(f"    → {n_snips:,} snippets from {n_kw_aio:,} keywords")

            # Embed unique snippets
            unique_snips = aio_df["snippet"].drop_duplicates().tolist()
            aio_embs_raw = cached_encode(
                unique_snips, model, _EMB_CACHE_DIR,
                label=f"AIO snippets ({len(unique_snips):,} unique)",
                batch_size=256,
                show_progress_bar=False,
            )
            snip_emb_map = {s: e for s, e in zip(unique_snips, aio_embs_raw)}
            all_aio_embs = np.array([snip_emb_map[s] for s in aio_df["snippet"]])

            n_comp_aio  = min(30, all_aio_embs.shape[1] - 1, n_snips - 1)
            aio_reduced = normalize(
                TruncatedSVD(n_comp_aio, random_state=42).fit_transform(all_aio_embs)
            )
            k_aio = max(3, min(40, n_snips // 10))

            aio_labels = (
                AgglomerativeClustering(
                    n_clusters=k_aio, metric="cosine", linkage="average"
                ).fit_predict(aio_reduced)
                if n_snips <= 5000 else
                KMeans(n_clusters=k_aio, random_state=42, n_init="auto")
                .fit_predict(aio_reduced)
            )
            aio_df["aio_cluster_id"] = aio_labels

            # Label each AIO cluster by extracting the shared theme
            # across the top-3 most centroid-representative snippets
            aio_label_map = {}
            for cid in np.unique(aio_labels):
                mask     = aio_labels == cid
                c_embs   = aio_reduced[mask]
                centroid = c_embs.mean(axis=0)
                sims     = c_embs @ centroid
                top_idx  = sims.argsort()[::-1][:5]
                top_snips = aio_df["snippet"].iloc[
                    np.where(mask)[0][top_idx]
                ].tolist()
                aio_label_map[cid] = _extract_cluster_theme(top_snips)
            aio_df["aio_cluster_label"] = aio_df["aio_cluster_id"].map(aio_label_map)

            # Roll up to keyword level — one row per keyword
            # Primary AIO cluster = cluster of first snippet (snip_pos == 0)
            # All AIO clusters = " || " separated, position-ordered, deduped
            aio_kw = (
                aio_df
                .sort_values("snip_pos")
                .drop_duplicates(["keyword", "aio_cluster_id"])
                .groupby("keyword", sort=False)
                .apply(lambda g: list(zip(
                    g["aio_cluster_id"].tolist(),
                    g["aio_cluster_label"].tolist()
                )))
                .to_dict()
            )

            aio_primary_id_list    = []
            aio_primary_label_list = []
            aio_all_ids_list       = []
            aio_all_labels_list    = []

            for _, row in df.iterrows():
                clusters = aio_kw.get(row["keyword"], [])
                if not clusters:
                    aio_primary_id_list.append(-1)
                    aio_primary_label_list.append("No AI Overview")
                    aio_all_ids_list.append("")
                    aio_all_labels_list.append("")
                    continue
                ids    = [str(c[0]) for c in clusters]
                labels_a = [c[1] for c in clusters]
                aio_primary_id_list.append(int(ids[0]))
                aio_primary_label_list.append(labels_a[0])
                aio_all_ids_list.append(" || ".join(ids))
                aio_all_labels_list.append(" || ".join(labels_a))

            df["aio_cluster_id"]     = aio_primary_id_list
            df["aio_cluster_label"]  = aio_primary_label_list
            df["aio_cluster_ids"]    = aio_all_ids_list    # all AIO clusters, || sep
            df["aio_cluster_labels"] = aio_all_labels_list

            n_aio_clusters = len(aio_label_map)
            aio_multi = (df["aio_cluster_ids"].str.contains(r"\|\|", na=False)).sum()
            print(f"    → {n_aio_clusters} AI Overview clusters produced")
            print(f"    → {aio_multi:,} keywords span multiple AIO clusters")

    return df, titles_df




# ═══════════════════════════════════════════════════════════════════════
# 6. COMPETITOR LANDSCAPE
# ═══════════════════════════════════════════════════════════════════════

def analyze_competitor_landscape(df: pd.DataFrame) -> pd.DataFrame:
    """
    Infer competitor landscape from SERP titles alone — no URLs needed.

    Per keyword computes:
      academic_lock    : 0-1 — how many of the 3 titles signal journal/institutional content
                         High = PubMed/Nature/clinical trial dominated = hard to displace
      dominant_format  : content format the SERP rewards (guide/overview/clinical/news/other)
      serp_diversity   : are titles semantically varied (good) or all saying the same thing?
                         Proxy: count distinct first words across titles

    Why titles are better than URLs here:
      - "Nature Communications | EZH2 inhibition in cancer" tells you more than a URL ever would
      - Titles are clean, always present, and directly reflect what Google is rewarding
      - Academic lock-in is highly visible in titles: journal name, study type, institution
    """
    print("\n[6/8] Analyzing competitor landscape from SERP titles...")

    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])

    academic_lock_list   = []
    dominant_format_list = []
    serp_diversity_list  = []

    for _, row in df.iterrows():
        titles = [
            str(row.get(col, "")).strip().lower()
            for col in title_cols[:3]
            if pd.notna(row.get(col)) and str(row.get(col, "")).strip()
        ]

        if not titles:
            academic_lock_list.append(0.5)    # unknown — neutral
            dominant_format_list.append("unknown")
            serp_diversity_list.append(0.5)
            continue

        all_titles_text = " ".join(titles)

        # ── Academic lock-in: count how many titles contain academic signals
        acad_hits = sum(
            1 for title in titles
            if any(sig in title for sig in ACADEMIC_TITLE_SIGNALS)
        )
        academic_lock_list.append(round(acad_hits / len(titles), 2))

        # ── Dominant format from title language
        fmt_hits = {
            fmt: bool(re.search(pat, all_titles_text))
            for fmt, pat in FORMAT_SIGNALS.items()
        }
        # clinical takes priority for life science (most distinctive)
        dominant = "other"
        for fmt in ("clinical", "guide", "overview", "listicle", "comparison", "news"):
            if fmt_hits.get(fmt):
                dominant = fmt
                break
        dominant_format_list.append(dominant)

        # ── SERP diversity: mean pairwise token dissimilarity across titles
        # Much better than first-word count — "Mechanisms of X" vs "Mechanisms of Y"
        # score 0 on first-word but are semantically different topics.
        if len(titles) >= 2:
            def _title_tokens(t):
                return set(re.findall(r'\b[a-z]{3,}\b', t.lower()))
            pairs = []
            for i in range(len(titles)):
                for j in range(i+1, len(titles)):
                    a, b = _title_tokens(titles[i]), _title_tokens(titles[j])
                    union = len(a | b)
                    overlap = len(a & b) / union if union else 0
                    pairs.append(1 - overlap)   # dissimilarity
            serp_diversity_list.append(round(sum(pairs) / len(pairs), 2))
        else:
            serp_diversity_list.append(0.5)

    df["academic_lock"]   = academic_lock_list
    df["dominant_format"] = dominant_format_list
    df["serp_diversity"]  = serp_diversity_list

    avg_lock = df["academic_lock"].mean()
    pct_clinical = (df["dominant_format"] == "clinical").mean() * 100
    print(f"    → Avg academic lock-in:  {avg_lock:.2f}  (0=open, 1=journal dominated)")
    print(f"    → Clinical format SERPs: {pct_clinical:.1f}%")
    print(f"    → Format breakdown:")
    for fmt, cnt in df["dominant_format"].value_counts().items():
        print(f"         {fmt:<15} {cnt:>5}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 7. GAP ANALYSIS
# ═══════════════════════════════════════════════════════════════════════

def analyze_gaps(df: pd.DataFrame) -> pd.DataFrame:
    """
    Four gap types scored at keyword level (URLs and dates removed):

    ABSENCE GAP     — impressions == 0 → no content presence at all
    PERFORMANCE GAP — impressions > 0 but position > 10 OR CTR < 2%
    FORMAT GAP      — SERP rewards accessible content (guide/overview)
                      but academic lock is low and you're not ranking
                      → opportunity to win with well-structured content
    PAA GAP         — PAA questions exist for this keyword but you're
                      not in top 10 → specific content angle opportunity
    SNIPPET GAP     — featured snippet exists but you don't own it

    Composite gap_score = weighted sum (0-1).
    """
    print("\n[7/8] Running gap analysis...")

    # ── CTR
    df["ctr"] = np.where(
        df["impressions"] > 0,
        (df["clicks"] / df["impressions"]).round(4),
        0.0
    )

    # ── Absence gap: truly no content presence
    # position == 999 means keyword never appeared in GSC at all
    # (our default for keywords with no GSC data after merge)
    # Zero impressions alone is NOT absence — GSC sometimes shows position
    # for keywords with 0 impressions due to aggregation thresholds
    df["gap_absence"] = df["position"] >= 999

    # ── Performance gap: Google knows you exist but you're losing
    # Covers: ranking 11-100, OR visible but CTR too low to matter
    df["gap_performance"] = (
        (df["position"] < 999) &           # GSC has seen you for this keyword
        (df["position"] > 10) |            # but you're off page 1
        (
            (df["impressions"] > 0) &      # OR you're getting impressions
            (df["ctr"] < 0.02) &           # but very low CTR
            (df["position"] <= 10)         # even though you're on page 1
        )
    )

    # ── Format gap: SERP rewards accessible formats (guide/overview)
    #    AND academic lock is low (meaning format actually matters here)
    #    AND you're not ranking — so a well-structured article could win
    df["gap_format"] = (
        (df["dominant_format"].isin(["guide", "overview", "listicle"])) &
        (df["academic_lock"] < 0.4) &
        (df["position"] > 10)
    )

    # ── PAA gap: uses pre-built paa_questions column from enrich_text stage
    #    (covers all PAA questions from BQ or all paa_N columns — no cap)
    if "paa_questions" not in df.columns:
        df["paa_questions"] = ""

    has_paa = df["paa_questions"].str.strip() != ""
    df["gap_paa"] = has_paa & (df["position"] > 10)

    # ── Snippet gap: snippet exists but you don't own it
    if "snippet" in df.columns:
        df["gap_snippet"] = (
            df["snippet"].notna() &
            (df["snippet"].astype(str).str.strip() != "") &
            (df["position"] > 5)
        )
    else:
        df["gap_snippet"] = False

    # ── Composite gap score
    # Binary gap signal (0-1) weighted by gap type importance
    weights = {
        "gap_absence":     0.40,   # highest — no presence = biggest gap
        "gap_performance": 0.30,   # second — visible but losing
        "gap_paa":         0.15,   # question angle opportunity
        "gap_format":      0.10,   # content type opportunity
        "gap_snippet":     0.05,   # featured snippet opportunity
    }
    binary_gap = sum(
        df[col].astype(float) * w for col, w in weights.items()
    )

    # Weight by impression demand — a 50k-impression gap is more valuable
    # than a 100-impression gap of the same type. Log-scale to avoid
    # very large keywords dominating entirely.
    if df["impressions"].max() > 0:
        imp_weight = (
            np.log1p(df["impressions"]) /
            np.log1p(df["impressions"].max())
        )
        # Blend: 65% gap type, 35% demand signal
        df["gap_score"] = (binary_gap * 0.65 + imp_weight * 0.35).round(3)
    else:
        df["gap_score"] = binary_gap.round(3)

    # ── Primary gap label for quick reading
    def primary_gap(row):
        if row["gap_absence"]:     return "Absence"
        if row["gap_performance"]: return "Performance"
        if row["gap_paa"]:         return "PAA"
        if row["gap_format"]:      return "Format"
        if row["gap_snippet"]:     return "Snippet"
        return "None"

    df["primary_gap"] = df.apply(primary_gap, axis=1)

    # ── Summary
    gap_counts = {
        "Absence":     int(df["gap_absence"].sum()),
        "Performance": int(df["gap_performance"].sum()),
        "PAA":         int(df["gap_paa"].sum()),
        "Format":      int(df["gap_format"].sum()),
        "Snippet":     int(df["gap_snippet"].sum()),
    }
    print("\n    Gap type breakdown:")
    for gtype, cnt in gap_counts.items():
        pct = cnt / len(df) * 100
        bar = "█" * min(30, int(pct / 2))
        print(f"      {gtype:<15} {cnt:>5}  ({pct:5.1f}%)  {bar}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 8. OPPORTUNITY SCORING & RECOMMENDATIONS
# ═══════════════════════════════════════════════════════════════════════

def score_and_recommend(df: pd.DataFrame, top_n: int) -> tuple:
    """
    Roll up to research area level.

    Opportunity = demand × gap × winnability

      demand      = keyword count (or total volume if available)
      gap         = avg gap_score across keywords in area
      winnability = (1 - avg academic_lock) × avg serp_diversity
                    — low academic lock + high SERP diversity = easier to compete

    Separate absence_gap% and performance_gap% drive recommended action:
      if absence_gap% > 60%     → "Create"
      if performance_gap% > 40% → "Optimise"
      else                      → "Create & Optimise"
    """
    print("\n[8/8] Scoring and ranking research areas...")

    work = df[df["research_area"] != "Unclassified / General Methods"].copy()

    # ── Volume: coerce everything to numeric first, then decide which column to use
    for col in ["volume", "impressions"]:
        if col in work.columns:
            work[col] = pd.to_numeric(work[col], errors="coerce").fillna(0)

    if "volume" in work.columns and work["volume"].sum() > 0:
        vol_col = "volume"
        print(f"    → Using search volume for demand scoring (total: {int(work['volume'].sum()):,})")
    elif "impressions" in work.columns and work["impressions"].sum() > 0:
        vol_col = "impressions"
        print(f"    → Using GSC impressions for demand scoring (total: {int(work['impressions'].sum()):,})")
    else:
        vol_col = None
        print("    → No volume/impressions data — using keyword count for demand")

    agg_dict = dict(
        keyword_count        = ("keyword",            "count"),
        avg_similarity       = ("research_area_score","mean"),
        avg_gap_score        = ("gap_score",          "mean"),
        avg_academic_lock    = ("academic_lock",      "mean"),
        avg_serp_diversity   = ("serp_diversity",     "mean"),
        absence_gap_count    = ("gap_absence",        "sum"),
        performance_gap_count= ("gap_performance",   "sum"),
        paa_gap_count        = ("gap_paa",            "sum"),
        sample_keywords      = ("keyword",            lambda x: " | ".join(list(x)[:8])),
        sub_topics           = ("cluster_label",      lambda x: " · ".join(
                                 x.value_counts().head(5).index.tolist())),
    )

    if vol_col:
        agg_dict["total_volume"] = (vol_col, "sum")

    area_df = work.groupby("research_area").agg(**agg_dict).reset_index()

    # ── Impression-weighted gap score
    # Simple mean of gap_score ignores that a keyword with 50k impressions and
    # an absence gap is worth far more than one with 10 impressions.
    # Weighted: sum(gap_score × impressions) / sum(impressions)
    if "impressions" in work.columns and work["impressions"].sum() > 0:
        work["_gap_x_imp"] = work["gap_score"] * work["impressions"].clip(lower=1)
        imp_weighted = work.groupby("research_area").agg(
            _sum_gap_x_imp = ("_gap_x_imp",   "sum"),
            _sum_imp       = ("impressions",   "sum"),
        )
        imp_weighted["imp_weighted_gap"] = (
            imp_weighted["_sum_gap_x_imp"] / imp_weighted["_sum_imp"].clip(lower=1)
        ).round(3)
        area_df = area_df.merge(
            imp_weighted[["imp_weighted_gap"]], on="research_area", how="left"
        )
    else:
        area_df["imp_weighted_gap"] = area_df["avg_gap_score"]

    # ── PAA coverage rate — what % of keywords have PAA questions
    # High coverage = strong question demand = informational content is wanted
    area_df["paa_coverage"] = (
        area_df["paa_gap_count"] / area_df["keyword_count"] * 100
    ).round(1)

    # ── Position distribution signals
    pos_stats = (
        work[work["position"] < 999]
        .groupby("research_area")["position"]
        .agg(
            median_position = "median",
            pct_page2       = lambda x: (((x >= 11) & (x <= 20)).sum() / len(x) * 100),
        )
        .round(1)
    )
    area_df = area_df.merge(pos_stats, on="research_area", how="left")
    area_df["median_position"] = area_df["median_position"].fillna(999).round(1)
    area_df["pct_page2"]       = area_df["pct_page2"].fillna(0).round(1)

    # ── Keyword concentration index — top 3 keywords' share of area impressions
    # High concentration = area dominated by 1-2 head terms, risky if they slip
    if vol_col:
        def _top3_share(grp):
            total = grp[vol_col].sum()
            if total == 0:
                return 0.0
            return round(grp.nlargest(3, vol_col)[vol_col].sum() / total * 100, 1)
        conc = work.groupby("research_area").apply(_top3_share).rename("top3_imp_share")
        area_df = area_df.merge(conc.reset_index(), on="research_area", how="left")
    else:
        area_df["top3_imp_share"] = None

    # ── AIO displacement risk — % of area keywords with AI Overview data
    if "aio_cluster_id" in work.columns:
        aio_stats = (
            work.groupby("research_area")
            .agg(_has_aio = ("aio_cluster_id", lambda x: (x >= 0).sum()))
            .reset_index()
        )
        aio_stats["aio_coverage_pct"] = (
            aio_stats["_has_aio"] /
            area_df.set_index("research_area").loc[aio_stats["research_area"], "keyword_count"].values
            * 100
        ).round(1)
        area_df = area_df.merge(
            aio_stats[["research_area", "aio_coverage_pct"]], on="research_area", how="left"
        )
    else:
        area_df["aio_coverage_pct"] = None


    # ── Normalise demand
    if vol_col:
        area_df["demand_score"] = area_df["total_volume"] / area_df["total_volume"].max()
    else:
        area_df["demand_score"] = area_df["keyword_count"] / area_df["keyword_count"].max()

    # ── Winnability: PAA coverage rate normalised 0-1
    # Replaces the old academic_lock × serp_diversity formula which was noise.
    # High PAA coverage = Google wants question-driven informational content here
    # = easier to win with well-structured FAQ-heavy content.
    area_df["winnability"] = (area_df["paa_coverage"] / 100).clip(0, 1).round(3)

    # ── Opportunity score
    # Uses impression-weighted gap score in place of simple mean
    area_df["opportunity_score"] = (
        area_df["demand_score"]       * 0.45 +
        area_df["imp_weighted_gap"]   * 0.35 +
        area_df["winnability"]        * 0.20
    ).round(3)

    # ── Gap percentages
    area_df["absence_gap_pct"]    = (
        area_df["absence_gap_count"]    / area_df["keyword_count"] * 100).round(1)
    area_df["performance_gap_pct"]= (
        area_df["performance_gap_count"]/ area_df["keyword_count"] * 100).round(1)

    # ── Recommended action
    def action(row):
        if row["absence_gap_pct"] >= 60:
            return "Create"
        if row["performance_gap_pct"] >= 40:
            return "🔧 Optimise"
        return "🆕+🔧 Create & Optimise"

    area_df["recommended_action"] = area_df.apply(action, axis=1)

    # ── Priority tier
    def tier(s):
        if s >= 0.55: return "🔴 High"
        if s >= 0.30: return "🟡 Medium"
        return                "🟢 Monitor"

    area_df["priority"] = area_df["opportunity_score"].apply(tier)

    # ── Add tier from taxonomy
    tier_lookup = {a["name"]: a.get("tier", 2) for a in RESEARCH_AREAS}
    area_df["tier"] = area_df["research_area"].map(tier_lookup).fillna(2).astype(int)
    area_df["tier_label"] = area_df["tier"].map({1: "⭐ Tier 1", 2: "Tier 2"})

    # Sort: Tier 1 areas first, then by opportunity score within each tier
    recs = (
        area_df
        .sort_values(["tier", "opportunity_score"], ascending=[True, False])
        .head(top_n)
        .reset_index(drop=True)
    )
    recs.index += 1
    recs.index.name = "rank"

    print(f"    → {len(area_df)} research areas scored")
    return area_df, recs


# ═══════════════════════════════════════════════════════════════════════
# 9. HTML REPORT
# ═══════════════════════════════════════════════════════════════════════

def generate_report(df: pd.DataFrame, recs: pd.DataFrame, output_dir: str,
                    kw_emb: np.ndarray = None,
                    briefs_html: str = "", serp_gaps_html: str = "",
                    visibility_html: str = ""):
    print("\n    → Generating HTML report...")

    # ── Keyword scatter map: PCA of embeddings → x/y coordinates
    # Each keyword = one dot, coloured by research area, symbol by gap type,
    # size by impressions. Much more visually informative than a treemap —
    # you can see semantic proximity, area clusters, and gap distribution at once.
    try:
        import plotly.express as px
        import plotly.io as pio
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import normalize as sk_normalize

        plot_df = df.copy()
        plot_df = plot_df[plot_df["research_area"] != "Unclassified / General Methods"]

        # PCA on keyword embeddings for x/y — deterministic, fast, no stochastic noise
        # Use the kw_emb passed in; fall back to recomputing from enriched_text if absent
        kw_emb_plot = kw_emb if kw_emb is not None else None

        if kw_emb_plot is not None and len(kw_emb_plot) == len(df):
            # Filter to classified rows only — keep original df index alignment
            classified_mask = df["research_area"] != "Unclassified / General Methods"
            emb_subset = kw_emb_plot[classified_mask.values]
            emb_norm   = sk_normalize(emb_subset)

            n_samples = len(emb_norm)
            n_comp    = min(2, n_samples - 1, emb_norm.shape[1])

            if n_comp == 2:
                pca = PCA(n_components=2, random_state=42)
                coords = pca.fit_transform(emb_norm)
                plot_df = plot_df.reset_index(drop=True)
                plot_df["pca_x"] = coords[:, 0]
                plot_df["pca_y"] = coords[:, 1]
                has_coords = True
            else:
                has_coords = False
        else:
            has_coords = False

        if has_coords:
            plot_df["impressions_sz"] = (
                pd.to_numeric(plot_df.get("impressions", 0), errors="coerce")
                .fillna(0).clip(lower=1)
            )
            plot_df["gap"]  = plot_df.get("primary_gap", "None").fillna("None")
            plot_df["pos"]  = pd.to_numeric(
                plot_df.get("position", 999), errors="coerce"
            ).fillna(999).round(1)
            plot_df["cluster"] = plot_df.get("primary_cluster_label", "").fillna("")

            # Cap marker size to keep small-impression keywords visible
            plot_df["marker_sz"] = (
                plot_df["impressions_sz"].clip(upper=5000) / 500 + 3
            ).clip(3, 15)

            AREA_COLORS = [
                "#0077b6","#e63946","#2a9d8f","#e9c46a","#f4a261",
                "#6a4c93","#43aa8b","#f77f00","#023e8a","#d62828",
                "#606c38","#bc6c25","#7b2d8b","#1d7874","#ee6c4d",
            ]
            areas = sorted(plot_df["research_area"].unique())
            area_color_map = {a: AREA_COLORS[i % len(AREA_COLORS)]
                              for i, a in enumerate(areas)}

            GAP_SYMBOLS = {
                "Absence":     "circle",
                "Performance": "diamond",
                "PAA":         "square",
                "Format":      "triangle-up",
                "Snippet":     "cross",
                "None":        "circle-open",
            }

            fig = px.scatter(
                plot_df,
                x="pca_x", y="pca_y",
                color="research_area",
                symbol="gap",
                size="marker_sz",
                size_max=15,
                color_discrete_map=area_color_map,
                symbol_map=GAP_SYMBOLS,
                hover_name="keyword",
                hover_data={
                    "research_area": True,
                    "cluster":       True,
                    "gap":           True,
                    "pos":           True,
                    "impressions_sz":True,
                    "pca_x":         False,
                    "pca_y":         False,
                    "marker_sz":     False,
                },
                labels={
                    "pca_x":         "← Semantic Axis 1 →",
                    "pca_y":         "← Semantic Axis 2 →",
                    "research_area": "Research Area",
                    "gap":           "Gap Type",
                    "pos":           "Your Position",
                    "impressions_sz":"Impressions",
                    "cluster":       "Cluster",
                },
                title="Keyword Map — Semantic Space, Coloured by Research Area, Shaped by Gap Type",
                height=750,
                template="plotly_white",
            )
            fig.update_traces(
                marker=dict(line=dict(width=0.5, color="rgba(0,0,0,0.3)")),
            )
            fig.update_layout(
                margin=dict(t=60, l=10, r=10, b=10),
                legend=dict(
                    orientation="v", x=1.01, y=1,
                    font=dict(size=11),
                ),
                xaxis=dict(showticklabels=False, showgrid=True,
                           gridcolor="#f0f0f0", zeroline=False),
                yaxis=dict(showticklabels=False, showgrid=True,
                           gridcolor="#f0f0f0", zeroline=False),
            )
            scatter_html = pio.to_html(fig, full_html=False, include_plotlyjs="cdn")
        else:
            # Fallback treemap if embeddings not available
            plot_df["impressions_sz"] = (
                pd.to_numeric(plot_df.get("impressions", 0), errors="coerce")
                .fillna(0).clip(lower=1)
            )
            plot_df["area"]    = plot_df["research_area"].fillna("Unclassified")
            plot_df["cluster"] = plot_df.get("primary_cluster_label", "").fillna("Unknown")
            plot_df["kw"]      = plot_df["keyword"]
            plot_df["gap"]     = plot_df.get("primary_gap", "Unknown").fillna("Unknown")
            fig = px.treemap(
                plot_df, path=["area", "cluster", "kw"],
                values="impressions_sz", color="gap",
                title="Keyword Map — Research Area → Cluster → Keyword",
                height=750,
            )
            scatter_html = pio.to_html(fig, full_html=False, include_plotlyjs="cdn")

        # Gap breakdown bar chart (unchanged)
        gap_data = pd.DataFrame({
            "Gap Type": ["Absence", "Performance", "Format", "PAA", "Snippet"],
            "Count":    [
                int(df.get("gap_absence",     pd.Series(0)).sum()),
                int(df.get("gap_performance", pd.Series(0)).sum()),
                int(df.get("gap_format",      pd.Series(0)).sum()),
                int(df.get("gap_paa",         pd.Series(0)).sum()),
                int(df.get("gap_snippet",     pd.Series(0)).sum()),
            ]
        })
        fig2 = px.bar(
            gap_data, x="Gap Type", y="Count",
            title="Gap Type Distribution Across All Keywords",
            color="Gap Type",
            color_discrete_map={
                "Absence":     "#dc3545",
                "Performance": "#fd7e14",
                "Format":      "#6610f2",
                "PAA":         "#0077b6",
                "Snippet":     "#198754",
            },
            template="plotly_white", height=350
        )
        bar_html = pio.to_html(fig2, full_html=False, include_plotlyjs=False)

    except ImportError:
        scatter_html = "<p><em>pip install plotly for charts</em></p>"
        bar_html     = ""

    # ── Recommendations table
    col_order = [
        "research_area", "keyword_count", "absence_gap_pct",
        "performance_gap_pct", "winnability", "opportunity_score",
        "priority", "recommended_action", "top_competitors", "sub_topics"
    ]
    col_order = [c for c in col_order if c in recs.columns]

    rec_rows = ""
    for rank, row in recs.iterrows():
        rec_rows += f"""
        <tr>
          <td style='text-align:center;font-weight:700'>{rank}</td>
          <td>{row.get('tier_label','')}</td>
          <td><strong>{row['research_area']}</strong></td>
          <td style='text-align:center'>{row.get('keyword_count','')}</td>
          <td style='text-align:center'>{row.get('absence_gap_pct','')}%</td>
          <td style='text-align:center'>{row.get('performance_gap_pct','')}%</td>
          <td style='text-align:center'>{row.get('avg_academic_lock','')}</td>
          <td style='text-align:center'>{row.get('winnability','')}</td>
          <td style='text-align:center'>{row.get('opportunity_score','')}</td>
          <td>{row.get('priority','')}</td>
          <td>{row.get('recommended_action','')}</td>
          <td style='font-size:0.8em;color:#444'>{row.get('sub_topics','')}</td>
        </tr>"""

    # ── Summary stats
    n_kw      = len(df)
    n_areas   = (df["research_area"] != "Unclassified / General Methods").nunique() - 1
    n_absence = int(df["gap_absence"].sum())
    n_perf    = int(df["gap_performance"].sum())
    n_unclass = int((df["research_area"] == "Unclassified / General Methods").sum())

    html = f"""<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Life Science Content Gap Analysis</title>
  <style>
    body   {{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',sans-serif;
            margin:0;padding:24px 44px;background:#f0f4f8;color:#212529;}}
    h1     {{color:#0d1b2a;border-bottom:3px solid #0077b6;padding-bottom:10px;}}
    h2     {{color:#0077b6;margin-top:36px;}}
    .stats {{display:flex;gap:16px;flex-wrap:wrap;margin:20px 0;}}
    .card  {{background:white;border-radius:10px;padding:14px 22px;
            box-shadow:0 2px 8px rgba(0,0,0,.08);min-width:130px;}}
    .card .num {{font-size:1.9em;font-weight:700;color:#0077b6;}}
    .card .lbl {{font-size:0.82em;color:#6c757d;margin-top:3px;}}
    table  {{width:100%;border-collapse:collapse;background:white;
            border-radius:10px;overflow:hidden;
            box-shadow:0 2px 8px rgba(0,0,0,.08);font-size:0.86em;}}
    th     {{background:#0077b6;color:white;padding:10px 8px;text-align:left;}}
    td     {{padding:9px 8px;border-bottom:1px solid #e9ecef;}}
    tr:hover td {{background:#e8f4fd;}}
    .plot  {{background:white;border-radius:10px;padding:16px;
            box-shadow:0 2px 8px rgba(0,0,0,.08);margin:16px 0;}}
    .note  {{background:#d1ecf1;border-left:4px solid #0077b6;
            padding:12px 16px;border-radius:4px;margin:16px 0;font-size:0.88em;}}
    .legend {{display:flex;gap:24px;flex-wrap:wrap;margin:12px 0;font-size:0.85em;}}
    .leg-item {{display:flex;align-items:center;gap:6px;}}
  </style>
</head>
<body>
  <h1>🔬 Life Science Content Gap Analysis</h1>

  <div class="stats">
    <div class="card"><div class="num">{n_kw}</div><div class="lbl">Total Keywords</div></div>
    <div class="card"><div class="num">{n_areas}</div><div class="lbl">Research Areas</div></div>
    <div class="card"><div class="num">{n_absence}</div><div class="lbl">Absence Gaps</div></div>
    <div class="card"><div class="num">{n_perf}</div><div class="lbl">Performance Gaps</div></div>
    <div class="card"><div class="num">{n_unclass}</div><div class="lbl">Unclassified</div></div>
  </div>

  <div class="note">
    <strong>How to read this:</strong>
    <strong>Absence gap</strong> = zero GSC impressions — no content presence at all (weight: 40%).
    <strong>Performance gap</strong> = impressions exist but position &gt;10 or CTR &lt;2% (weight: 30%).
    <strong>Format gap</strong> = SERP rewards guides/overviews, academic lock is low, you're not ranking (weight: 15%).
    <strong>PAA gap</strong> = People Also Ask questions exist but you're not in top 10 (weight: 10%).
    <strong>Snippet gap</strong> = featured snippet exists that you don't own (weight: 5%).
    <strong>Winnability</strong> = (1 − academic lock) × SERP diversity — higher means easier to compete.
    <strong>Opportunity Score</strong> = 45% demand + 35% gap score + 20% winnability.
    Only gaps with score ≥ 0.15 appear in gaps.csv.
  </div>

  <h2>📊 Keyword Map</h2>
  <div class="plot">{scatter_html}</div>

  <h2>📉 Gap Distribution</h2>
  <div class="plot">{bar_html}</div>

  <h2>🎯 Research Area Recommendations</h2>
  <table>
    <thead><tr>
      <th>#</th><th>Tier</th><th>Research Area</th><th>Keywords</th>
      <th>Absence Gap%</th><th>Perf Gap%</th>
      <th>Academic Lock</th><th>Winnability</th><th>Opp Score</th>
      <th>Priority</th><th>Action</th><th>Key Sub-topics</th>
    </tr></thead>
    <tbody>{rec_rows}</tbody>
  </table>

  {briefs_html}

  {serp_gaps_html}

  <h2>📈 Visibility Improvement Stages</h2>
  <p style='font-size:0.87em;color:#555;margin:0 0 20px 0'>
    The following analyses focus on moving existing and new content from
    page 2 to page 1 — covering intent alignment, fast optimisation wins,
    freshness priorities, and internal linking structure.
  </p>
  {visibility_html}

  <p style="margin-top:40px;color:#aaa;font-size:0.78em">
    Model: all-MiniLM-L6-v2 &nbsp;|&nbsp;
    {len(RESEARCH_AREAS)} research areas (Tier 1: Neuroscience · Oncology · Immunology) &nbsp;|&nbsp;
    Agglomerative clustering + t-SNE &nbsp;|&nbsp;
    Gap types: Absence · Performance · Freshness · Format · PAA · Snippet
  </p>
</body>
</html>"""

    path = os.path.join(output_dir, "report.html")
    with open(path, "w", encoding="utf-8") as f:
        f.write(html)
    print(f"    → report.html saved")


# ═══════════════════════════════════════════════════════════════════════
# 9. CONTENT BRIEF GENERATOR
# ═══════════════════════════════════════════════════════════════════════

CONTENT_TYPE_MAP = {
    "clinical":   "Clinical Deep-Dive",
    "guide":      "Practical Guide",
    "overview":   "Educational Overview",
    "listicle":   "Curated Explainer",
    "comparison": "Comparative Analysis",
    "news":       "Research Spotlight",
    "other":      "Research Article",
    "unknown":    "Research Article",
}

# Keywords that should never be used as a cluster label
LABEL_BLOCKLIST = {
    # Generic lab terms
    "kit", "panel", "assay", "protocol", "reagent", "antibody", "buffer",
    "solution", "product", "catalog", "item", "sample", "control", "standard",
    "test", "tool", "service", "data", "analysis", "method", "technique",
    "research", "study", "review", "paper", "article", "publication",
    # Short noise
    "a", "an", "the", "of", "in", "for", "and", "or", "with",
    # Numeric / ID patterns handled separately via regex
}

def _is_valid_label(kw: str) -> bool:
    """Return False if keyword looks like a product ID or generic noise."""
    kw = kw.strip().lower()
    # Too short
    if len(kw) < 5:
        return False
    # Blocklisted
    if kw in LABEL_BLOCKLIST:
        return False
    # Looks like a product code: has digits mixed with letters, or is all caps short
    if re.match(r"^[a-z]{0,3}\d+", kw):           # e.g. ab12345, 9876
        return False
    if re.match(r"^[A-Z]{2,6}-?\d{3,}", kw):       # e.g. AB-1234, FLWPNL007
        return False
    if re.match(r"^\d", kw):                         # starts with digit
        return False
    # Contains suspicious patterns
    if re.search(r"\d{4,}", kw):                     # long number sequence
        return False
    return True


def _best_cluster_label(cluster: pd.DataFrame, c_emb: np.ndarray,
                         centroid: np.ndarray) -> str:
    """
    Pick the cluster label keyword using centroid similarity,
    but skip any keyword that fails the quality filter.
    Falls back to the longest valid keyword if all centroid candidates fail.
    """
    sims  = c_emb @ centroid
    order = sims.argsort()[::-1]   # highest similarity first
    kws   = cluster["keyword"].tolist()

    for idx in order:
        kw = kws[idx]
        if _is_valid_label(kw):
            return kw

    # Fallback: longest valid keyword in cluster
    valid = [k for k in kws if _is_valid_label(k)]
    if valid:
        return max(valid, key=len)

    # Last resort: just return the most common word in the cluster
    all_words = " ".join(kws).split()
    counts    = Counter(w for w in all_words if len(w) > 4)
    if counts:
        return counts.most_common(1)[0][0]
    return kws[0]


def _extract_topic_phrase(keywords: list[str]) -> str:
    """
    Extract the most informative 2-3 word phrase from a list of keywords
    to use as the content angle. Prefers multi-word, biology-specific phrases.
    """
    # Score each keyword: longer multi-word phrases score higher,
    # penalise single words and product-like strings
    scored = []
    for kw in keywords:
        if not _is_valid_label(kw):
            continue
        words  = kw.split()
        n      = len(words)
        score  = n * 2                          # reward multi-word
        score += min(len(kw), 30)               # reward longer strings
        score -= sum(1 for w in words           # penalise generic words
                     if w.lower() in LABEL_BLOCKLIST) * 3
        scored.append((score, kw))

    if not scored:
        return keywords[0] if keywords else "Research Topic"

    scored.sort(reverse=True)
    return scored[0][1]


def _generate_title(topic: str, paa_questions: list[str], primary_gap: str,
                    content_type: str, research_area: str,
                    top_keywords: list[str], used_titles: set,
                    rep_titles: list[str] = None) -> str:
    """
    Derive a suggested article title from actual SERP evidence.

    The title is NOT invented from templates or PAA questions.
    It is extracted/synthesised from the real titles that are already ranking
    in the SERP for this keyword cluster — so it reflects what Google is
    actually rewarding for this topic.

    Strategy (in priority order):
    1. Best rep_title — the most centroid-representative actual SERP title,
       stripped of publisher suffix/boilerplate (already cleaned by _clean_title)
    2. _extract_cluster_theme on rep_titles — if the best single title is too
       generic or already used, extract the shared phrase across the top titles
    3. Structured fallback — topic + content_type signal, no generic templates

    PAA questions are NEVER used as titles. They belong in paa_questions column.
    """
    area        = research_area.split("/")[0].strip()
    topic_title = topic.title()

    # ── Strategy 1: best real SERP title from the cluster
    if rep_titles:
        for t in rep_titles:
            t_clean = _clean_title(t)
            if not t_clean:
                continue
            if t_clean not in used_titles and len(t_clean) > 15:
                used_titles.add(t_clean)
                return t_clean

    # ── Strategy 2: shared theme across cluster titles
    if rep_titles and len(rep_titles) >= 2:
        theme = _extract_cluster_theme(rep_titles)
        if theme and theme not in used_titles and len(theme) > 10:
            used_titles.add(theme)
            return theme

    # ── Strategy 3: structured fallback based on gap type
    # Only reached if rep_titles is empty (keyword-only clusters with no SERP data)
    # Uses content_type to pick the right angle — still grounded in data signal.
    gap_angles = {
        "Absence":     f"{topic_title}: {area} Research Overview",
        "Performance": f"{topic_title}: Updated Analysis for {area}",
        "PAA":         f"{topic_title}: Key Questions in {area}",
        "Format":      f"{topic_title}: {content_type} for {area}",
    }
    candidate = gap_angles.get(primary_gap, f"{topic_title}: {area} Perspective")
    if candidate not in used_titles:
        used_titles.add(candidate)
        return candidate

    # ── Last resort: topic with uniqueness suffix
    fallback = topic_title
    suffix   = 2
    while fallback in used_titles:
        fallback = f"{topic_title} ({suffix})"
        suffix  += 1
    used_titles.add(fallback)
    return fallback


def _current_year() -> str:
    from datetime import datetime
    return str(datetime.now().year)


def _tone_from_lock(academic_lock: float) -> str:
    if academic_lock >= 0.7:
        return "Technical — written for active researchers and scientists"
    if academic_lock >= 0.4:
        return "Balanced — research-aware professionals and clinicians"
    return "Accessible — broader scientific and clinical audience"


def _wordcount(fmt: str, academic_lock: float) -> str:
    if fmt == "clinical":       return "2,500–3,500"
    if academic_lock >= 0.6:    return "2,000–3,000"
    if fmt in ("guide","listicle"): return "1,500–2,500"
    return "1,800–2,500"


def _recommend_structure(paa_questions: list[str], content_type: str,
                          top_keywords: list[str]) -> str:
    """
    Derive article H2 structure from actual evidence — PAA questions and
    keyword themes — rather than hardcoded content-type templates.

    Logic:
    1. PAA questions (already relevance-gated) become the core H2s.
       They represent real questions Google users are asking about this topic.
    2. Gaps in coverage are filled from keyword themes: if the top keywords
       suggest a mechanism, clinical, or methods angle not covered by PAA,
       add it as a section.
    3. A minimal frame (intro + conclusion) is added around the evidence-based
       sections. These are the only templated parts.

    Result: every brief has a unique structure driven by its actual data,
    not a generic 'Clinical Deep-Dive' template applied identically to
    every immunotherapy article.
    """
    sections = ["Introduction & Overview"]

    # ── Core sections from PAA questions (clean the question into an H2)
    paa_sections = []
    for q in paa_questions[:5]:
        q = q.strip().rstrip("?")
        if not q or len(q) < 10:
            continue
        # Capitalise as a heading
        paa_sections.append(q[0].upper() + q[1:])

    sections.extend(paa_sections)

    # ── Fill remaining slots from keyword themes
    # Extract the most specific 2-3 word phrases from top_keywords to infer
    # angles the PAA questions haven't already covered.
    n_slots = max(0, 5 - len(paa_sections))
    if n_slots > 0 and top_keywords:
        # Group keywords by shared first meaningful word → infer angle
        angle_candidates = []
        kw_stop = {"the","a","an","of","in","and","or","for","with","is","are",
                   "how","what","why","when","does","do","can","will"}
        seen_first = set()
        for kw in top_keywords[:20]:
            words = [w for w in kw.lower().split() if w not in kw_stop and len(w) > 3]
            if not words:
                continue
            first = words[0]
            if first in seen_first:
                continue
            seen_first.add(first)
            # Build a heading from the 2-3 most informative words
            heading_words = words[:3]
            heading = " ".join(heading_words).title()
            # Only add if not already covered by a PAA section
            if not any(heading.lower() in s.lower() for s in paa_sections):
                angle_candidates.append(heading)
            if len(angle_candidates) >= n_slots:
                break

        sections.extend(angle_candidates[:n_slots])

    # ── Clinical/methods framing based on content type — only if no PAA covers it
    paa_text = " ".join(paa_sections).lower()
    if content_type == "Clinical Deep-Dive" and "clinical" not in paa_text:
        sections.append("Clinical Evidence & Current Guidelines")
    elif content_type == "Practical Guide" and "method" not in paa_text:
        sections.append("Methods, Protocols & Best Practices")

    sections.append("Key Takeaways & Future Directions")

    # Deduplicate while preserving order
    seen = set()
    unique_sections = []
    for s in sections:
        key = s.lower()[:30]
        if key not in seen:
            seen.add(key)
            unique_sections.append(s)

    return " → ".join(unique_sections[:7])


def _cluster_relevance_score(text: str, area_emb: np.ndarray, model) -> float:
    """
    Score how relevant a text (cluster label, title, or PAA question) is
    to any of the life science research areas.
    Returns max cosine similarity to any area embedding — 0.0 to 1.0.
    Anything below ~0.20 is unlikely to be life science content.
    """
    emb = model.encode(text, normalize_embeddings=True)
    sims = emb @ area_emb.T
    return float(sims.max())


# Minimum cosine similarity to research area taxonomy for a cluster
# or PAA question to be included in briefs / gaps.
# 0.22 was too permissive — allowed "Did Ted Bundy have MAOA gene" type
# queries through because "gene" bumped similarity above the floor.
# 0.28 eliminates these while keeping genuine edge-case life-sci queries.
RELEVANCE_THRESHOLD = 0.28


def _is_relevant(text: str, area_emb: np.ndarray, model,
                 threshold: float = RELEVANCE_THRESHOLD) -> bool:
    """Return True if text is semantically relevant to life science."""
    if not text or len(text.strip()) < 8:
        return False
    return _cluster_relevance_score(text, area_emb, model) >= threshold


def _cluster_is_high_quality(cluster: pd.DataFrame, area_name: str,
                              min_classified_pct: float = 0.5) -> bool:
    """
    Secondary quality gate applied to each cluster before generating a brief.

    Rules:
    1. At least 50% of the cluster's keywords must be classified into a
       real research area (not Unclassified / General Methods).
       Prevents junk clusters built from oddly-matched low-quality keywords.
    2. At least 1 keyword in the cluster must belong to the correct area.
       Prevents cross-area bleed where the brief topic doesn't match the
       area heading it appears under.
    """
    total = len(cluster)
    if total == 0:
        return False

    # Rule 1: majority must be classified
    classified = (cluster["research_area"] != "Unclassified / General Methods").sum()
    if classified / total < min_classified_pct:
        return False

    # Rule 2: at least one keyword must actually belong to this area
    in_area = (cluster["research_area"] == area_name).sum()
    if in_area == 0:
        return False

    return True


def generate_content_briefs(df: pd.DataFrame, kw_emb: np.ndarray,
                             area_emb: np.ndarray, model,
                             recs: pd.DataFrame, output_dir: str,
                             n_briefs: int = 25) -> pd.DataFrame:
    """
    Stage 9: Generate prioritised, specific content briefs.

    Relevance gating:
    - Every cluster topic is scored against the research area taxonomy
    - Clusters below RELEVANCE_THRESHOLD are dropped entirely
    - PAA questions are individually filtered the same way
    - This eliminates junk like "Is ELISA a girl's name" or
      unrelated religious/cultural content that bleeds into SERP data
    """
    print("\n[9] Generating content briefs...")

    from sklearn.preprocessing import normalize

    try:
        import umap as _umap; _HAVE_UMAP = True
    except ImportError:
        _HAVE_UMAP = False
    try:
        import hdbscan as _hdbscan; _HAVE_HDBSCAN = True
    except ImportError:
        _HAVE_HDBSCAN = False

    # Only classified, meaningfully gapped keywords
    work = df[
        (df["research_area"] != "Unclassified / General Methods") &
        (df["gap_score"] >= 0.15) &
        (df["keyword"].apply(_is_valid_label))
    ].copy()

    if len(work) == 0:
        print("    → No valid gap keywords above threshold")
        return pd.DataFrame()

    tier_lookup   = {a["name"]: a.get("tier", 2) for a in RESEARCH_AREAS}
    all_briefs    = []
    used_titles   = set()
    used_paa      = set()   # global PAA dedup — same question won't appear in 2 briefs
    rejected      = 0

    areas_ordered = (
        recs.sort_values(["tier", "opportunity_score"], ascending=[True, False])
        ["research_area"].tolist()
    )

    for area in areas_ordered:
        area_mask = work["research_area"] == area
        area_df   = work[area_mask].copy()

        if len(area_df) < 3:
            continue

        # ── Reduce + cluster within this area
        area_indices = area_df.index.tolist()
        a_emb        = kw_emb[area_indices]
        n            = len(area_df)

        reduced = _reduce_embeddings(a_emb, n_components=min(10, n-2),
                                     have_umap=_HAVE_UMAP)
        labels  = _cluster_embeddings(reduced, min_size=2,
                                      have_hdbscan=_HAVE_HDBSCAN)

        # Assign noise points to nearest cluster
        if -1 in labels:
            from sklearn.metrics import pairwise_distances_argmin_min
            noise_mask = labels == -1
            valid_mask = labels != -1
            if valid_mask.sum() > 0:
                nearest, _ = pairwise_distances_argmin_min(
                    reduced[noise_mask], reduced[valid_mask]
                )
                labels[noise_mask] = labels[valid_mask][nearest]

        area_df = area_df.copy()
        area_df["area_cluster"] = labels

        for cid in np.unique(labels):
            c_mask  = area_df["area_cluster"] == cid
            cluster = area_df[c_mask]

            if len(cluster) < 2:
                continue

            c_emb    = reduced[c_mask.values]
            centroid = c_emb.mean(axis=0)

            # ── Quality-filtered cluster label
            label_kw = _best_cluster_label(cluster, c_emb, centroid)

            # ── Topic phrase (best multi-word angle from cluster keywords)
            valid_kws = [k for k in cluster["keyword"].tolist() if _is_valid_label(k)]
            topic     = _extract_topic_phrase(valid_kws)

            # ── Relevance gate — reject clusters not related to life science
            if not _is_relevant(topic, area_emb, model):
                rejected += 1
                continue

            # ── Quality gate — reject clusters where majority of keywords
            # are unclassified or don't actually belong to this area
            if not _cluster_is_high_quality(cluster, area):
                rejected += 1
                continue

            # ── Gap signals
            avg_gap   = cluster["gap_score"].mean()
            top_gap   = cluster["primary_gap"].value_counts().index[0] \
                        if len(cluster["primary_gap"].value_counts()) else "Absence"

            # ── PAA questions — deduplicated locally AND globally, relevance-gated
            # PAA questions come from raw SERP data and frequently contain off-topic
            # questions that happen to co-occur with life science keywords.
            # Junk pattern and relevance gate are applied before adding to all_paa.
            all_paa = []
            for paa_str in cluster["paa_questions"].dropna():
                for q in str(paa_str).split("|"):
                    q = q.strip()
                    if not q or len(q) <= 10:
                        continue
                    norm_q = _normalise_for_dedup(q)
                    if norm_q in used_paa or q in all_paa:
                        continue
                    # Hard pattern block — biographical/crime junk
                    if _JUNK_PAA_PATTERNS.search(q):
                        continue
                    # Semantic relevance gate — question must relate to life science
                    if not _is_relevant(q, area_emb, model):
                        continue
                    all_paa.append(q)
            # Register these PAAs as used globally
            for q in all_paa:
                used_paa.add(_normalise_for_dedup(q))

            # ── Format
            fmt_counts   = cluster["dominant_format"].value_counts()
            dom_fmt      = fmt_counts.index[0] if len(fmt_counts) else "other"
            content_type = CONTENT_TYPE_MAP.get(dom_fmt, "Research Article")

            # ── Target keywords: scored by relevance to cluster topic,
            # not just impressions. A keyword that's semantically central
            # to this cluster AND has high impressions AND has a real gap
            # is far more valuable than a high-impression keyword that's
            # only loosely related to the cluster theme.
            #
            # Score = semantic_centrality × demand_weight × gap_weight
            #   semantic_centrality: cosine similarity of keyword embedding
            #                        to cluster centroid (0-1)
            #   demand_weight:       log-normalised impressions (0-1)
            #   gap_weight:          gap_score (0-1, already impression-weighted)
            #
            # This ensures the primary_keyword and secondary_keywords in the
            # brief are the ones the article should actually target — not just
            # whatever happened to rank highest in GSC.

            c_emb_norm   = c_emb / (np.linalg.norm(c_emb, axis=1, keepdims=True) + 1e-9)
            centroid_norm = centroid / (np.linalg.norm(centroid) + 1e-9)
            centrality    = (c_emb_norm @ centroid_norm).clip(0, 1)

            kw_df = cluster[cluster["keyword"].apply(_is_valid_label)].copy()
            kw_df = kw_df.reset_index(drop=True)

            if len(kw_df) == 0:
                top_kws = valid_kws[:15]
            else:
                # Re-index centrality to match kw_df rows
                valid_mask = cluster["keyword"].apply(_is_valid_label).values
                cent_valid = centrality[valid_mask]

                max_imp = kw_df["impressions"].max() if kw_df["impressions"].max() > 0 else 1
                demand  = np.log1p(kw_df["impressions"].values) / np.log1p(max_imp)
                gap_w   = kw_df["gap_score"].values.clip(0, 1)

                kw_score = cent_valid * 0.45 + demand * 0.35 + gap_w * 0.20

                kw_df["_kw_score"] = kw_score
                top_kws = (
                    kw_df.sort_values("_kw_score", ascending=False)
                    ["keyword"].head(15).tolist()
                )

            # ── Representative SERP titles for this cluster
            # Pull from cluster_labels (all title clusters spanning these keywords)
            # — these are real SERP titles already cleaned by Stage 5.
            # Used as the primary source for suggested_title.
            rep_titles_raw = []
            for label_str in cluster["cluster_labels"].dropna():
                for lbl in str(label_str).split("||"):
                    lbl = lbl.strip()
                    if lbl and lbl not in rep_titles_raw and lbl != "No title data":
                        rep_titles_raw.append(lbl)
            # Also add primary_cluster_label for each keyword
            for lbl in cluster["primary_cluster_label"].dropna().unique():
                if lbl and lbl not in rep_titles_raw and lbl != "No title data":
                    rep_titles_raw.append(lbl)
            # Deduplicate and take top 6 most distinct
            rep_titles = list(dict.fromkeys(rep_titles_raw))[:6]

            # ── Generate title from SERP evidence
            title = _generate_title(
                topic, all_paa, top_gap, content_type,
                area, top_kws, used_titles,
                rep_titles=rep_titles
            )

            # ── Suggested article structure
            structure = _recommend_structure(all_paa, content_type, top_kws)

            # ── Brief score
            paa_bonus  = min(0.15, len(all_paa) * 0.015)
            tier_bonus = 0.10 if tier_lookup.get(area, 2) == 1 else 0.0
            brief_score = avg_gap + paa_bonus + tier_bonus

            all_briefs.append({
                "brief_score":         round(brief_score, 3),
                "research_area":       area,
                "tier":                tier_lookup.get(area, 2),
                "topic":               topic,
                "suggested_title":     title,
                "serp_evidence":       " | ".join(rep_titles[:3]),
                "content_type":        content_type,
                "primary_gap":         top_gap,
                "keyword_count":       len(cluster),
                "primary_keyword":     top_kws[0] if top_kws else topic,
                "secondary_keywords":  " | ".join(top_kws[1:8]),
                "all_target_keywords": " | ".join(top_kws),
                "paa_questions":       " | ".join(all_paa[:8]),
                "paa_count":           len(all_paa),
                "suggested_structure": structure,
                "avg_gap_score":       round(avg_gap, 3),
                "recommended_action":  (
                    "Create"              if top_gap == "Absence"     else
                    "🔧 Optimise"        if top_gap == "Performance" else
                    "❓ Add PAA content" if top_gap == "PAA"         else
                    "📋 Reformat"        if top_gap == "Format"      else
                    "⭐ Win Snippet"
                ),
            })

    if not all_briefs:
        print("    → No briefs generated — check gap threshold or keyword quality")
        return pd.DataFrame()

    print(f"    → {rejected} clusters rejected (below relevance threshold)")

    briefs_df = pd.DataFrame(all_briefs)

    # ── Select top n_briefs: guarantee 1 per area, then fill by score
    selected    = []
    seen_areas  = set()

    for _, row in briefs_df.sort_values(
        ["tier", "brief_score"], ascending=[True, False]
    ).iterrows():
        if row["research_area"] not in seen_areas:
            selected.append(row.to_dict())
            seen_areas.add(row["research_area"])

    selected_titles = {r["suggested_title"] for r in selected}
    for _, row in briefs_df.sort_values("brief_score", ascending=False).iterrows():
        if len(selected) >= n_briefs:
            break
        if row["suggested_title"] not in selected_titles:
            selected.append(row.to_dict())
            selected_titles.add(row["suggested_title"])

    briefs_out = (
        pd.DataFrame(selected)
        .sort_values(["tier", "brief_score"], ascending=[True, False])
        .reset_index(drop=True)
    )
    briefs_out.index += 1
    briefs_out.index.name = "brief_rank"

    briefs_path = os.path.join(output_dir, "content_briefs.csv")
    briefs_out.to_csv(briefs_path)
    print(f"    → {len(briefs_out)} content briefs generated → {briefs_path}")

    print("\n📝  Content Briefs:\n")
    for rank, row in briefs_out.iterrows():
        tier_tag = "⭐" if row["tier"] == 1 else "  "
        print(f"  {rank:>2}. {tier_tag} [{row['research_area']}]")
        print(f"       {row['suggested_title']}")
        print(f"       {row['content_type']} · {row['recommended_action']} · "
              f"{row['keyword_count']} keywords")
        if row["paa_count"] > 0:
            first_q = str(row["paa_questions"]).split("|")[0].strip()
            print(f"       PAA ({row['paa_count']} Qs): {first_q}...")
        print()

    return briefs_out


def generate_briefs_html(briefs_df: pd.DataFrame, output_dir: str,
                          total_briefs: int = None) -> str:
    if briefs_df is None or len(briefs_df) == 0:
        return ""

    cards = ""
    for rank, row in briefs_df.iterrows():
        tier_badge = (
            "<span style='background:#0077b6;color:white;border-radius:4px;"
            "padding:2px 8px;font-size:0.75em;margin-right:6px'>⭐ Tier 1</span>"
            if row["tier"] == 1 else
            "<span style='background:#6c757d;color:white;border-radius:4px;"
            "padding:2px 8px;font-size:0.75em;margin-right:6px'>Tier 2</span>"
        )

        # PAA questions as structured list
        paa_html = ""
        if row.get("paa_questions"):
            items = "".join(
                f"<li style='margin:3px 0'>{q.strip()}</li>"
                for q in str(row["paa_questions"]).split("|")[:6]
                if q.strip()
            )
            if items:
                paa_html = (
                    f"<div style='margin-top:12px'>"
                    f"<strong style='font-size:0.84em;color:#495057'>"
                    f"PAA Questions to Answer ({row['paa_count']} total):</strong>"
                    f"<ul style='margin:4px 0 0 0;padding-left:18px;"
                    f"font-size:0.82em;color:#555'>{items}</ul></div>"
                )

        # Suggested structure
        structure_html = ""
        if row.get("suggested_structure"):
            steps = str(row["suggested_structure"]).split(" → ")
            step_items = "".join(
                f"<span style='background:#f0f4f8;border-radius:4px;padding:3px 8px;"
                f"font-size:0.78em;margin:2px;display:inline-block'>"
                f"<strong>{i+1}.</strong> {s}</span>"
                for i, s in enumerate(steps)
            )
            structure_html = (
                f"<div style='margin-top:12px'>"
                f"<strong style='font-size:0.84em;color:#495057'>Suggested Structure:</strong>"
                f"<div style='margin-top:5px;line-height:1.8'>{step_items}</div></div>"
            )

        # Target keywords as tags
        kws    = str(row.get("all_target_keywords", "")).split("|")
        kw_tags = "".join(
            f"<span style='background:#e8f4fd;border-radius:3px;padding:2px 7px;"
            f"font-size:0.78em;margin:2px;display:inline-block'>{k.strip()}</span>"
            for k in kws[:12] if k.strip()
        )

        gap_colour = {
            "Absence":     "#dc3545",
            "Performance": "#fd7e14",
            "PAA":         "#0077b6",
            "Format":      "#6610f2",
            "Snippet":     "#198754",
        }.get(row["primary_gap"], "#6c757d")

        cards += f"""
        <div style='background:white;border-radius:12px;padding:22px 26px;
                    box-shadow:0 2px 10px rgba(0,0,0,.08);margin-bottom:24px;
                    border-left:5px solid {gap_colour}'>

          <div style='display:flex;align-items:center;justify-content:space-between;
                      flex-wrap:wrap;gap:8px;margin-bottom:8px'>
            <div>
              {tier_badge}
              <span style='font-size:0.83em;color:#495057;font-weight:600'>
                {row['research_area']}</span>
            </div>
            <div style='display:flex;gap:6px;flex-wrap:wrap'>
              <span style='background:{gap_colour};color:white;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['primary_gap']} Gap</span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['content_type']}</span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;border-radius:4px;
                           padding:2px 9px;font-size:0.78em'>{row['recommended_action']}</span>
            </div>
          </div>

          <h3 style='margin:0 0 6px 0;color:#0d1b2a;font-size:1.08em;line-height:1.4'>
            <span style='color:{gap_colour};margin-right:6px'>#{rank}</span>
            {row['suggested_title']}
          </h3>

          <div style='font-size:0.82em;color:#6c757d;margin-bottom:14px'>
            {row['keyword_count']} target keywords
          </div>

          <div>
            <strong style='font-size:0.84em;color:#495057'>Target Keywords:</strong><br>
            <div style='margin-top:5px'>{kw_tags}</div>
          </div>

          {structure_html}
          {paa_html}
        </div>"""

    return f"""
  <h2>📝 Content Briefs (showing top {len(briefs_df)}{f" of {total_briefs}" if total_briefs and total_briefs > len(briefs_df) else ""} Recommendations)</h2>
  <div style='margin-bottom:16px;font-size:0.87em;color:#555;
              background:#f8f9fa;border-radius:8px;padding:12px 16px'>
    Briefs are ranked by opportunity score within each tier.
    Each brief includes a suggested article structure derived from PAA questions and content type.
    Border colour = primary gap:
    <span style='color:#dc3545'>■ Absence</span> &nbsp;
    <span style='color:#fd7e14'>■ Performance</span> &nbsp;
    <span style='color:#0077b6'>■ PAA</span> &nbsp;
    <span style='color:#6610f2'>■ Format</span> &nbsp;
    <span style='color:#198754'>■ Snippet</span>
  </div>
  {cards}"""

# ═══════════════════════════════════════════════════════════════════════
# 10. SERP TITLE MINING — CONTENT ANGLE GAP ANALYSIS
# ═══════════════════════════════════════════════════════════════════════

def mine_serp_title_gaps(df: pd.DataFrame, output_dir: str,
                          model, area_emb: np.ndarray,
                          n_gaps: int = 25) -> tuple:
    """
    Stage 10: Data-mine SERP titles to find content angles that are either:
      A) Heavily represented in top-ranking titles but you have no presence
      B) Raised in PAA questions but addressed by NO top-ranking title

    This is fundamentally different from keyword gap analysis (Stage 7-9).
    Stage 7-9 asks: "where are you absent from search results?"
    Stage 10 asks: "what do the search results reveal that nobody has written well?"

    Approach per research area:
      1. Explode enriched_text back into individual SERP titles
      2. Embed all titles as a corpus (not as keyword enrichment — as content units)
      3. Cluster the title corpus to find what themes Google is rewarding
      4. For each title cluster:
           - Summarise what the cluster covers (pattern from titles)
           - Check your GSC presence (avg position for mapped keywords)
           - Find PAA questions that don't match any title cluster centroid
             → these are the uncontested angles
      5. Score each gap by: search demand × your absence × PAA signal strength
      6. Output structured gap briefs: what exists, what's missing, why it wins

    Proxy for "your content": GSC position ≤ 10 on mapped keywords = you have coverage.
    Position > 20 or no impressions = you're absent from that topic.
    """
    print("\n[10/10] Mining SERP titles for content angle gaps...")

    from sklearn.decomposition import TruncatedSVD
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.preprocessing import normalize
    from sklearn.metrics.pairwise import cosine_similarity

    # ── Build the SERP title corpus
    # Each row in df has an enriched_text = "keyword | title1 | title2 | ..."
    # Explode back to individual titles, tagged with their keyword's research area

    title_rows = []

    for _, row in df.iterrows():
        area     = row.get("research_area", "")
        keyword  = row.get("keyword", "")
        position = float(row.get("position", 999))
        impressions = float(row.get("impressions", 0))

        if area == "Unclassified / General Methods" or not area:
            continue

        # Get titles from enriched_text (pipe-separated) or title_N columns
        enriched = str(row.get("enriched_text", ""))
        if "|" in enriched:
            parts  = [p.strip() for p in enriched.split("|")]
            titles = parts[1:]
        else:
            title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])
            titles = [str(row.get(c, "")).strip()
                      for c in title_cols if pd.notna(row.get(c)) and str(row.get(c)).strip()]

        # Get PAA questions
        paa_str = str(row.get("paa_questions", ""))
        paas    = [q.strip() for q in paa_str.split("|") if q.strip() and len(q.strip()) > 8]

        for title in titles:
            cleaned = _clean_title(title)
            if cleaned and cleaned.lower() != keyword:
                title_rows.append({
                    "title":         cleaned,
                    "keyword":       keyword,
                    "research_area": area,
                    "position":      position,
                    "impressions":   impressions,
                    "paa_questions": paa_str,
                    "paas":          paas,
                })

    if not title_rows:
        print("    → No SERP titles available for mining")
        return pd.DataFrame(), ""

    titles_df = pd.DataFrame(title_rows)
    print(f"    → {len(titles_df):,} raw title instances across "
          f"{titles_df['research_area'].nunique()} research areas")

    # ── Embed all unique titles in ONE batch, then batch-score relevance
    # This replaces the old per-title _is_relevant() calls which encoded
    # one string at a time — 111k individual model calls vs. one batched encode.
    unique_titles = titles_df["title"].drop_duplicates().tolist()
    title_embs    = cached_encode(
        unique_titles, model, _EMB_CACHE_DIR,
        label=f"SERP titles ({len(unique_titles):,} unique)",
        batch_size=256,
    )
    title_emb_map  = {t: e for t, e in zip(unique_titles, title_embs)}

    # Batch relevance score: max cosine sim to any research area
    # area_emb shape: (n_areas, 384) — one row per research area description
    unique_emb_matrix = np.array([title_emb_map[t] for t in unique_titles])
    # (n_unique_titles, n_areas) — one sim score per title per area
    rel_sims    = unique_emb_matrix @ area_emb.T
    max_rel_sim = rel_sims.max(axis=1)   # best match to any area

    relevant_titles = {
        t for t, sim in zip(unique_titles, max_rel_sim)
        if sim >= RELEVANCE_THRESHOLD
    }
    n_before = len(titles_df)
    titles_df = titles_df[titles_df["title"].isin(relevant_titles)].copy()
    n_removed = n_before - len(titles_df)
    print(f"    → {n_removed:,} titles removed by relevance gate "
          f"({len(titles_df):,} remain, threshold={RELEVANCE_THRESHOLD})")

    if len(titles_df) == 0:
        print("    → No relevant titles remain after filtering")
        return pd.DataFrame(), ""

    titles_df["emb_idx"] = titles_df["title"].map(
        {t: i for i, t in enumerate(unique_titles)}
    )

    all_gaps    = []
    used_titles = set()   # global dedup across all areas and gap types

    for area in titles_df["research_area"].unique():
        area_titles = titles_df[titles_df["research_area"] == area].copy()
        n = len(area_titles)

        if n < 5:
            continue

        # ── Cluster the title corpus for this area
        embs = np.array([title_emb_map[t] for t in area_titles["title"]])
        n_comp   = min(20, embs.shape[1] - 1, n - 1)
        reduced  = normalize(
            TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(embs)
        )

        k = max(3, min(12, n // 8))
        labels = AgglomerativeClustering(
            n_clusters=k, metric="cosine", linkage="average"
        ).fit_predict(reduced)

        area_titles = area_titles.copy()
        area_titles["title_cluster"] = labels

        # ── Collect PAA questions — deduplicated, then batch relevance-score
        raw_paa = []
        for paa_list in area_titles["paas"].tolist():
            for q in paa_list:
                if q and q not in raw_paa:
                    raw_paa.append(q)

        # Batch-encode all PAA questions once, then filter by relevance
        # (avoids one model call per question — could be thousands)
        all_area_paa = []
        paa_embs     = None
        if raw_paa:
            paa_embs_all = cached_encode(
                raw_paa, model, _EMB_CACHE_DIR,
                label=f"PAA ({area[:30]})",
                batch_size=256,
                show_progress_bar=False,
            )
            paa_rel = (paa_embs_all @ area_emb.T).max(axis=1)
            all_area_paa = [q for q, s in zip(raw_paa, paa_rel)
                            if s >= RELEVANCE_THRESHOLD]
            if all_area_paa:
                keep_idx = [i for i, s in enumerate(paa_rel)
                            if s >= RELEVANCE_THRESHOLD]
                paa_embs = paa_embs_all[keep_idx]

        # ── Analyse each title cluster
        for cid in np.unique(labels):
            c_mask   = area_titles["title_cluster"] == cid
            cluster  = area_titles[c_mask]
            c_embs   = embs[c_mask.values]
            centroid = c_embs.mean(axis=0)
            centroid = centroid / (np.linalg.norm(centroid) + 1e-9)

            # What do these titles cover? Extract representative titles
            sims        = c_embs @ centroid
            top_idx     = sims.argsort()[::-1][:5]
            rep_titles  = cluster["title"].iloc[top_idx].tolist()

            # Pattern: find common words/phrases across rep titles
            def extract_pattern(titles: list) -> str:
                """Find what themes recur across the top titles in a cluster."""
                all_words = Counter()
                for t in titles:
                    words = re.findall(r"\b[a-zA-Z]{4,}\b", t.lower())
                    all_words.update(words)
                # Remove noise words
                noise = {"with", "from", "that", "this", "have", "been",
                         "their", "they", "were", "what", "which", "when",
                         "after", "before", "about", "into", "your", "more"}
                top_words = [w for w, _ in all_words.most_common(8)
                             if w not in noise]
                return ", ".join(top_words[:5]) if top_words else "mixed"

            pattern       = extract_pattern(rep_titles)
            cluster_label = rep_titles[0] if rep_titles else f"Cluster {cid}"

            # ── Relevance gate — reject clusters not related to life science
            if not _is_relevant(cluster_label, area_emb, model):
                continue

            # ── Your presence: avg position for keywords mapping to this cluster
            # Keywords "map" to this cluster if their position is tracked
            cluster_kws     = cluster.drop_duplicates("keyword")
            your_positions  = cluster_kws[cluster_kws["position"] < 999]["position"]
            your_impressions= cluster_kws["impressions"].sum()

            if len(your_positions) > 0:
                avg_pos = your_positions.mean()
            else:
                avg_pos = 999   # no presence

            you_are_absent = avg_pos > 20 or your_impressions == 0

            # ── Find uncontested PAA angles
            # PAA questions that are semantically distant from ALL title cluster centroids
            uncontested_paa = []
            if paa_embs is not None and len(all_area_paa) > 0:
                # Compute all cluster centroids for this area
                all_centroids = []
                for other_cid in np.unique(labels):
                    other_mask = area_titles["title_cluster"] == other_cid
                    other_embs = embs[other_mask.values]
                    c = other_embs.mean(axis=0)
                    c = c / (np.linalg.norm(c) + 1e-9)
                    all_centroids.append(c)
                all_centroids = np.array(all_centroids)

                # For each PAA question, find its max similarity to any title cluster
                paa_cluster_sims = paa_embs @ all_centroids.T
                max_sims         = paa_cluster_sims.max(axis=1)

                # PAA questions with low similarity to all clusters = uncontested
                for q, max_sim in zip(all_area_paa, max_sims):
                    if max_sim < 0.45:   # no cluster strongly covers this question
                        uncontested_paa.append(q)

            # ── Score this gap
            # Higher score = more valuable to create content for
            absence_score  = 1.0 if avg_pos == 999 else max(0, (avg_pos - 10) / 90)
            demand_score   = min(1.0, your_impressions / 5000) if your_impressions > 0 else 0.3
            paa_score      = min(1.0, len(uncontested_paa) * 0.2)
            title_coverage = len(cluster) / max(len(area_titles), 1)  # how big this theme is

            gap_score = (
                absence_score  * 0.40 +
                title_coverage * 0.25 +
                paa_score      * 0.20 +
                demand_score   * 0.15
            )

            # ── Determine gap type
            if not you_are_absent:
                gap_type = "Angle Gap"   # topic exists, specific angle missing
                gap_explanation = (
                    f"You rank for some keywords in this area (avg position {avg_pos:.0f}) "
                    f"but the SERP shows {len(cluster)} titles on this theme — "
                    f"a more targeted piece could outrank broader coverage."
                )
            elif uncontested_paa:
                gap_type = "Uncontested Angle"
                gap_explanation = (
                    f"This theme has {len(cluster)} competing titles but "
                    f"{len(uncontested_paa)} PAA questions that none of them answer well. "
                    f"Content that directly addresses these questions has a clear opening."
                )
            else:
                gap_type = "Topic Void"
                gap_explanation = (
                    f"You have no presence on this theme ({len(cluster)} SERP titles exist). "
                    f"Creating foundational content here captures demand with no direct competition "
                    f"from your existing pages."
                )

            # ── Generate suggested title from the cluster theme
            # Use _extract_cluster_theme on rep_titles for a specific, shared phrase
            # rather than the top-frequency-words fallback which produces generic titles.
            cluster_theme = _extract_cluster_theme(rep_titles)

            if uncontested_paa:
                # Title comes from the best uncontested PAA question
                raw_title = uncontested_paa[0].rstrip("?").strip()
            else:
                # Title comes from the cluster theme — a concrete shared phrase
                raw_title = cluster_theme

            # Dedup: if this exact title has already been used, try the next
            # PAA question, then try theme + area qualifier, then skip
            if raw_title in used_titles:
                alternate = None
                # Try remaining PAA questions first
                for q in uncontested_paa[1:4]:
                    candidate = q.rstrip("?").strip()
                    if candidate not in used_titles and len(candidate) > 10:
                        alternate = candidate
                        break
                if alternate is None and cluster_theme not in used_titles:
                    alternate = cluster_theme
                if alternate is None:
                    # Last resort: theme + area qualifier
                    short_area = area.split("/")[0].strip().split()[-1]
                    candidate = f"{cluster_theme}: {short_area} Perspective"
                    if candidate not in used_titles:
                        alternate = candidate
                if alternate is None:
                    continue   # truly duplicate, skip this gap
                raw_title = alternate

            title_suggestion = raw_title
            used_titles.add(title_suggestion)

            # What existing titles cover (shown in brief)
            existing_coverage = " · ".join(
                t[:60] + "..." if len(t) > 60 else t
                for t in rep_titles[:4]
            )

            all_gaps.append({
                "gap_score":          round(gap_score, 3),
                "research_area":      area,
                "gap_type":           gap_type,
                "gap_explanation":    gap_explanation,
                "suggested_title":    title_suggestion,
                "theme_pattern":      pattern,
                "serp_title_count":   len(cluster),
                "your_avg_position":  round(avg_pos, 1) if avg_pos < 999 else "No presence",
                "uncontested_paa":    " | ".join(uncontested_paa[:5]),
                "uncontested_paa_count": len(uncontested_paa),
                "existing_coverage":  existing_coverage,
                "rep_titles":         " · ".join(rep_titles[:3]),
                "keywords_in_theme":  " | ".join(cluster["keyword"].drop_duplicates().head(10).tolist()),
            })

    if not all_gaps:
        print("    → No content angle gaps identified")
        return pd.DataFrame(), ""

    gaps_df = (
        pd.DataFrame(all_gaps)
        .sort_values("gap_score", ascending=False)
        .head(n_gaps)
        .reset_index(drop=True)
    )
    gaps_df.index += 1
    gaps_df.index.name = "gap_rank"

    # ── Save
    gaps_path = os.path.join(output_dir, "serp_content_gaps.csv")
    gaps_df.to_csv(gaps_path)
    print(f"    → {len(gaps_df)} content angle gaps identified → {gaps_path}")

    # ── Terminal summary
    print("\n🔍  SERP Content Angle Gaps:\n")
    for rank, row in gaps_df.iterrows():
        gap_icon = {"Uncontested Angle": "🎯", "Topic Void": "⬛",
                    "Angle Gap": "↗️"}.get(row["gap_type"], "•")
        print(f"  {rank:>2}. {gap_icon} [{row['research_area']}] {row['gap_type']}")
        print(f"       {row['suggested_title']}")
        print(f"       SERP coverage: {row['serp_title_count']} titles · "
              f"Your position: {row['your_avg_position']}")
        if row["uncontested_paa_count"] > 0:
            first = str(row["uncontested_paa"]).split("|")[0].strip()
            print(f"       Uncontested: {first}...")
        print()

    # ── Build HTML section (top 25 only — full 50 in serp_content_gaps.csv)
    html_gaps = gaps_df.head(25)
    gap_cards = ""
    for rank, row in html_gaps.iterrows():
        gap_colour = {
            "Uncontested Angle": "#0077b6",
            "Topic Void":        "#dc3545",
            "Angle Gap":         "#fd7e14",
        }.get(row["gap_type"], "#6c757d")

        gap_icon = {
            "Uncontested Angle": "🎯",
            "Topic Void":        "⬛",
            "Angle Gap":         "↗️",
        }.get(row["gap_type"], "•")

        # Uncontested PAA as list
        paa_html = ""
        if row.get("uncontested_paa"):
            items = "".join(
                f"<li style='margin:3px 0;color:#0077b6'>{q.strip()}</li>"
                for q in str(row["uncontested_paa"]).split("|")[:5]
                if q.strip()
            )
            if items:
                paa_html = (
                    f"<div style='margin-top:12px'>"
                    f"<strong style='font-size:0.84em;color:#495057'>"
                    f"❓ Uncontested questions — not answered by any top-ranking page:</strong>"
                    f"<ul style='margin:4px 0 0 0;padding-left:18px;"
                    f"font-size:0.82em'>{items}</ul></div>"
                )

        # What currently ranks
        existing_html = ""
        if row.get("rep_titles"):
            items = "".join(
                f"<li style='margin:2px 0;color:#6c757d'>{t.strip()}</li>"
                for t in str(row["rep_titles"]).split("·")[:3]
                if t.strip()
            )
            existing_html = (
                f"<div style='margin-top:12px'>"
                f"<strong style='font-size:0.84em;color:#495057'>"
                f"📄 What currently ranks on this theme:</strong>"
                f"<ul style='margin:4px 0 0 0;padding-left:18px;"
                f"font-size:0.82em;font-style:italic'>{items}</ul></div>"
            )

        # Keyword tags
        kws = str(row.get("keywords_in_theme", "")).split("|")
        kw_tags = "".join(
            f"<span style='background:#f0f4f8;border-radius:3px;padding:2px 7px;"
            f"font-size:0.78em;margin:2px;display:inline-block'>{k.strip()}</span>"
            for k in kws[:10] if k.strip()
        )

        pos_display = row["your_avg_position"]
        pos_colour  = "#dc3545" if pos_display == "No presence" else (
            "#198754" if isinstance(pos_display, float) and pos_display <= 10 else "#fd7e14"
        )

        gap_cards += f"""
        <div style='background:white;border-radius:12px;padding:22px 26px;
                    box-shadow:0 2px 10px rgba(0,0,0,.08);margin-bottom:24px;
                    border-left:5px solid {gap_colour}'>

          <div style='display:flex;align-items:center;justify-content:space-between;
                      flex-wrap:wrap;gap:8px;margin-bottom:10px'>
            <div>
              <span style='background:{gap_colour};color:white;border-radius:4px;
                           padding:2px 10px;font-size:0.8em;margin-right:8px'>
                {gap_icon} {row["gap_type"]}
              </span>
              <span style='font-size:0.83em;color:#495057;font-weight:600'>
                {row["research_area"]}
              </span>
            </div>
            <div style='display:flex;gap:6px;flex-wrap:wrap;font-size:0.78em'>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;
                           border-radius:4px;padding:2px 8px'>
                {row["serp_title_count"]} competing titles
              </span>
              <span style='background:#f8f9fa;border:1px solid #dee2e6;
                           border-radius:4px;padding:2px 8px;color:{pos_colour}'>
                Your position: {pos_display}
              </span>
            </div>
          </div>

          <h3 style='margin:0 0 6px 0;color:#0d1b2a;font-size:1.06em;line-height:1.4'>
            <span style='color:{gap_colour}'># {rank}</span> &nbsp;
            {row["suggested_title"]}
          </h3>

          <div style='font-size:0.83em;color:#555;background:#f8f9fa;
                      border-radius:6px;padding:8px 12px;margin:10px 0;
                      border-left:3px solid {gap_colour}'>
            {row["gap_explanation"]}
          </div>

          <div style='margin-top:10px'>
            <strong style='font-size:0.84em;color:#495057'>Related keywords:</strong>
            <div style='margin-top:5px'>{kw_tags}</div>
          </div>

          {existing_html}
          {paa_html}
        </div>"""

    serp_gaps_html = f"""
  <h2>🔍 SERP Content Angle Gaps (showing top {len(html_gaps)} of {len(gaps_df)})</h2>
  <div style='margin-bottom:16px;font-size:0.87em;color:#555;
              background:#f8f9fa;border-radius:8px;padding:12px 16px'>
    These gaps are derived from mining what top-ranking SERP titles cover —
    not from keyword demand alone. They represent topics where either
    no content exists, a specific angle is unaddressed, or PAA questions
    go unanswered by any current top-ranking page.
    <br><br>
    <span style='color:#dc3545'>■ Topic Void</span> — theme has SERP presence but you have none &nbsp;
    <span style='color:#0077b6'>■ Uncontested Angle</span> — PAA questions unanswered by top pages &nbsp;
    <span style='color:#fd7e14'>■ Angle Gap</span> — you rank but a deeper angle is available
  </div>
  {gap_cards}"""

    return gaps_df, serp_gaps_html


# ═══════════════════════════════════════════════════════════════════════
# 11. SEARCH INTENT CLASSIFIER
# ═══════════════════════════════════════════════════════════════════════

# Intent classification via embedding similarity — more accurate than regex
# for biomedical text where the same terms appear across intent types.
# Each prototype is a short sentence capturing the essential character of that intent.
# At startup, embed_intent_prototypes() encodes these and stores the matrix.
INTENT_PROTOTYPES = {
    "informational": (
        "how does this biological mechanism work, what is the pathway, "
        "explain the molecular function, review of the science, "
        "overview of the research, what role does this gene play"
    ),
    "clinical": (
        "clinical trial results, patient treatment outcomes, therapy efficacy, "
        "phase 2 phase 3 study, standard of care, diagnosis prognosis biomarker, "
        "FDA approved indication, dosing regimen, safety adverse events"
    ),
    "commercial": (
        "compare products, best antibody kit for this assay, "
        "which reagent is better, top rated alternatives, "
        "versus comparison, which should I choose"
    ),
    "transactional": (
        "buy reagent antibody kit, order catalog, price cost, "
        "recombinant protein supplier, assay kit manufacturer, custom conjugated"
    ),
    "navigational": (
        "NCBI PubMed database, UniProt entry, Ensembl gene page, "
        "KEGG pathway map, Reactome, protein data bank PDB entry"
    ),
}

# Will be populated by embed_intent_prototypes() at runtime
_INTENT_PROTOTYPE_MATRIX = None   # shape (5, embedding_dim)
_INTENT_LABELS            = list(INTENT_PROTOTYPES.keys())


def embed_intent_prototypes(model) -> None:
    """Encode intent prototypes once after model is loaded."""
    global _INTENT_PROTOTYPE_MATRIX
    texts = [INTENT_PROTOTYPES[k] for k in _INTENT_LABELS]
    _INTENT_PROTOTYPE_MATRIX = model.encode(
        texts, normalize_embeddings=True, show_progress_bar=False
    )

def classify_intent(text: str, model=None) -> str:
    """
    Classify search intent using embedding similarity to intent prototypes.
    Falls back to regex if model/prototypes not yet initialised.

    Embedding approach is significantly more accurate for biomedical text:
    - "KRAS G12C expression colorectal cancer prognosis" → clinical (not informational)
    - "sotorasib resistance mechanism" → informational (not transactional)
    - Regex would misclassify both of these.
    """
    # Fast navigational pre-filter — these are unambiguous by keyword alone
    _NAV_RE = re.compile(
        r'\b(ncbi|pubmed|uniprot|ensembl|pdb|kegg|reactome|'
        r'abcam|thermo|sigma|bio-techne|r&d systems|biolegend)\b',
        re.IGNORECASE
    )
    if _NAV_RE.search(text):
        return "navigational"

    # Embedding-based classification
    if _INTENT_PROTOTYPE_MATRIX is not None and model is not None:
        vec  = model.encode(text, normalize_embeddings=True, show_progress_bar=False)
        sims = _INTENT_PROTOTYPE_MATRIX @ vec
        return _INTENT_LABELS[int(sims.argmax())]

    # Regex fallback if prototypes not initialised
    _FALLBACK = {
        "transactional": re.compile(
            r'\b(buy|purchase|order|price|cost|kit|reagent|antibody|'
            r'catalog|supplier|manufacturer|recombinant|conjugated)\b', re.I),
        "commercial":    re.compile(
            r'\b(best|top|compare|vs\.?|versus|alternative|which is better)\b', re.I),
        "clinical":      re.compile(
            r'\b(clinical trial|phase [123]|patient|treatment|therapy|'
            r'efficacy|safety|fda approved|standard of care)\b', re.I),
        "informational": re.compile(
            r'\b(what is|how does|mechanism|overview|pathway|function|review)\b', re.I),
    }
    for intent, pat in _FALLBACK.items():
        if pat.search(text):
            return intent
    return "informational"


def run_intent_classification(df: pd.DataFrame, model=None) -> pd.DataFrame:
    """
    Stage 11: Classify search intent for every keyword.
    Uses embedding similarity to intent prototypes when model is available.
    """
    print("\n[11] Classifying search intent...")

    # Initialise intent prototypes with the shared model
    if model is not None:
        embed_intent_prototypes(model)
        print("    → Using embedding-based intent classification")
    else:
        print("    → Using regex fallback for intent classification")

    # Classify keyword intent
    df["keyword_intent"] = df["keyword"].apply(
        lambda t: classify_intent(t, model=model)
    )

    # Classify SERP intent from titles
    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])

    def serp_intent_from_titles(row):
        titles = " ".join(
            str(row.get(c, "")) for c in title_cols[:5]
            if pd.notna(row.get(c))
        )
        if "enriched_text" in row.index and "|" in str(row.get("enriched_text", "")):
            titles = " ".join(str(row["enriched_text"]).split("|")[1:6])
        return classify_intent(titles, model=model) if titles.strip() else "unknown"

    df["serp_intent"] = df.apply(serp_intent_from_titles, axis=1)

    # Flag mismatches
    df["intent_mismatch"] = (
        (df["serp_intent"] != "unknown") &
        (df["keyword_intent"] != df["serp_intent"])
    )

    def intent_note(row):
        if not row["intent_mismatch"]:
            return ""
        ki, si = row["keyword_intent"], row["serp_intent"]
        notes = {
            ("informational", "transactional"):
                "SERP is product/commercial — informational content unlikely to rank here",
            ("informational", "clinical"):
                "SERP favours clinical evidence pages — content needs trial data & citations",
            ("commercial", "informational"):
                "SERP is educational — product-focused content won't rank",
            ("transactional", "informational"):
                "SERP rewards explainer content — pure product pages won't rank",
            ("informational", "navigational"):
                "SERP is database/tool navigation — not rankable with editorial content",
        }
        return notes.get((ki, si),
            f"Intent shift: your content is {ki}, SERP rewards {si}")

    df["intent_note"] = df.apply(intent_note, axis=1)

    n_mismatch = df["intent_mismatch"].sum()
    print(f"    → {n_mismatch:,} intent mismatches detected "
          f"({n_mismatch/len(df)*100:.1f}% of keywords)")

    intent_dist = df["keyword_intent"].value_counts()
    print("    → Intent distribution:")
    for intent, cnt in intent_dist.items():
        bar = "█" * min(30, cnt * 30 // len(df))
        print(f"      {intent:<16} {cnt:>5}  {bar}")

    return df


# ═══════════════════════════════════════════════════════════════════════
# 12. PAGE 2 FAST-WIN IDENTIFIER
# ═══════════════════════════════════════════════════════════════════════

def find_page2_fast_wins(df: pd.DataFrame, output_dir: str) -> pd.DataFrame:
    """
    Stage 12: Identify keywords ranking 11-20 (page 2) where small
    content improvements could flip to page 1.

    These are your highest-ROI optimisation opportunities:
    - Content clearly exists (Google has indexed and ranked it)
    - You're close — position 11-20, not position 50+
    - Specific, actionable edits can push you over

    For each fast-win keyword, output:
      - What PAA questions are being asked that you may not answer
      - What format the top results use (that yours may not match)
      - Whether there's a snippet to win
      - Specific optimisation actions ranked by impact

    Grouped by cluster so optimisation briefs address multiple
    related keywords in one content edit.
    """
    print("\n[12] Identifying page 2 fast-wins...")

    # Page 2: position 11-20, has impressions (confirmed Google presence)
    page2 = df[
        (df["position"] >= 11) &
        (df["position"] <= 20) &
        (df["impressions"] > 0) &
        (df["research_area"] != "Unclassified / General Methods")
    ].copy()

    if len(page2) == 0:
        print("    → No page 2 keywords found")
        return pd.DataFrame()

    print(f"    → {len(page2):,} keywords on page 2 across "
          f"{page2['research_area'].nunique()} research areas")

    # Group by cluster for shared optimisation briefs
    fast_wins = []

    for (area, cluster), group in page2.groupby(
        ["research_area", "cluster_label"]
    ):
        if len(group) < 1:
            continue

        # PAA questions unanswered (gap_paa flagged)
        paa_needed = []
        for paa_str in group["paa_questions"].dropna():
            for q in str(paa_str).split("|"):
                q = q.strip()
                if q and len(q) > 8 and q not in paa_needed:
                    paa_needed.append(q)

        # Format signal
        fmt_counts  = group["dominant_format"].value_counts()
        serp_format = fmt_counts.index[0] if len(fmt_counts) else "unknown"

        # Snippet opportunity
        snippet_opp = bool(group.get("gap_snippet", pd.Series(False)).any())

        # Intent alignment
        intent_issues = group[group["intent_mismatch"]]["intent_note"].unique().tolist() \
                        if "intent_mismatch" in group.columns else []

        # Best position in cluster (closest to page 1)
        best_pos  = group["position"].min()
        avg_pos   = group["position"].mean()
        total_imp = group["impressions"].sum()
        top_kw    = group.sort_values("impressions", ascending=False)["keyword"].iloc[0]

        # Build prioritised action list
        actions = []
        if paa_needed:
            actions.append(
                f"Add a FAQ or dedicated section answering: "
                f"{'; '.join(paa_needed[:3])}"
            )
        if serp_format in ("guide", "overview") and not any(
            w in cluster.lower() for w in ["guide", "overview", "how", "what"]
        ):
            actions.append(
                f"Restructure as a {serp_format} — SERP rewards this format here"
            )
        if snippet_opp:
            actions.append(
                "Add a concise 40-60 word direct-answer paragraph near the top "
                "targeting the featured snippet"
            )
        if intent_issues:
            actions.append(f"Intent note: {intent_issues[0]}")
        if not actions:
            actions.append(
                "Deepen content coverage — add more specific sub-topics, "
                "mechanisms, or supporting data to match SERP depth"
            )

        # Win score: closer to p1 + higher impressions = better
        win_score = (
            ((20 - avg_pos) / 10) * 0.6 +
            min(1.0, total_imp / 1000) * 0.4
        )

        fast_wins.append({
            "win_score":          round(win_score, 3),
            "research_area":      area,
            "cluster":            cluster,
            "keyword_count":      len(group),
            "best_position":      round(best_pos, 1),
            "avg_position":       round(avg_pos, 1),
            "total_impressions":  int(total_imp),
            "primary_keyword":    top_kw,
            "all_keywords":       " | ".join(
                group.sort_values("impressions", ascending=False)
                ["keyword"].head(10).tolist()
            ),
            "serp_format":        serp_format,
            "snippet_opportunity":snippet_opp,
            "paa_to_add":         " | ".join(paa_needed[:5]),
            "optimisation_actions": " → ".join(actions),
            "effort":             "Low" if len(actions) <= 2 else "Medium",
        })

    if not fast_wins:
        print("    → No fast-win clusters found")
        return pd.DataFrame()

    wins_df = (
        pd.DataFrame(fast_wins)
        .sort_values("win_score", ascending=False)
        .reset_index(drop=True)
    )
    wins_df.index += 1
    wins_df.index.name = "priority"

    path = os.path.join(output_dir, "page2_fast_wins.csv")
    wins_df.to_csv(path)
    print(f"    → {len(wins_df)} fast-win clusters → {path}")
    print(f"    → Potential: {wins_df['keyword_count'].sum():,} keywords "
          f"could move from page 2 → page 1")

    return wins_df


# ═══════════════════════════════════════════════════════════════════════
# 13. FRESHNESS PRIORITY SCORER
# ═══════════════════════════════════════════════════════════════════════

FRESHNESS_SIGNALS = [
    r"\b(202[3-9]|20[3-9]\d)\b",                          # recent years
    r"\b(new|latest|recent|update[d]?|current|emerging)\b",
    r"\b(phase [23]|trial results|readout|approval|approved|fda|ema)\b",
    r"\b(breakthrough|advance[d]?|novel|first.in.class|next.gen)\b",
]

def score_freshness_priority(df: pd.DataFrame, output_dir: str) -> pd.DataFrame:
    """
    Stage 13: Identify keyword clusters where SERP titles signal
    recency demand — meaning stale content is actively losing ground.

    Logic:
    1. Detect recency signals in SERP titles (year mentions, "new", "latest",
       clinical readout language, approval announcements)
    2. Score each keyword by how strong those signals are
    3. Combine with your position — if you rank 5-15 on a
       freshness-sensitive topic, a content update is urgent
    4. Output a prioritised refresh list by cluster

    This is distinct from the gap analysis — these keywords already
    have content, it's just ageing out of relevance.
    """
    print("\n[13] Scoring freshness priority...")

    title_cols = sorted([c for c in df.columns if re.match(r"title_\d+", c)])

    def freshness_score(row) -> float:
        """Count recency signals across SERP titles for this keyword."""
        titles = " ".join(
            str(row.get(c, "")) for c in title_cols[:10]
            if pd.notna(row.get(c))
        )
        if "enriched_text" in row.index and "|" in str(row.get("enriched_text", "")):
            titles = " ".join(str(row["enriched_text"]).split("|")[1:])
        titles = titles.lower()
        hits = sum(
            len(re.findall(p, titles)) for p in FRESHNESS_SIGNALS
        )
        return min(1.0, hits / 5)   # normalise 0-1

    df["freshness_signal"] = df.apply(freshness_score, axis=1)

    # Freshness priority = high signal + you rank (content exists but may be stale)
    # Only flag keywords where you're ranking — no point refreshing content
    # you don't have
    df["freshness_priority"] = (
        df["freshness_signal"] >= 0.4
    ) & (
        df["position"] < 999
    )

    # Roll up to cluster level
    fresh_clusters = []

    for (area, cluster), group in df[df["freshness_priority"]].groupby(
        ["research_area", "cluster_label"]
    ):
        avg_signal = group["freshness_signal"].mean()
        avg_pos    = group["position"].replace(999, np.nan).mean()
        total_imp  = group["impressions"].sum()
        kw_count   = len(group)

        # Sample the freshness signals detected
        sample_titles = []
        for col in title_cols[:3]:
            vals = group[col].dropna().head(2).tolist()
            sample_titles.extend([str(v) for v in vals if str(v).strip()])

        refresh_urgency = (
            "🔴 Urgent" if avg_signal >= 0.7 and (avg_pos or 999) < 15
            else "🟡 Soon"   if avg_signal >= 0.5
            else "🟢 Watch"
        )

        fresh_clusters.append({
            "refresh_score":   round(avg_signal * 0.6 +
                                     (1 - min(1, (avg_pos or 50) / 50)) * 0.4, 3),
            "urgency":         refresh_urgency,
            "research_area":   area,
            "cluster":         cluster,
            "keyword_count":   kw_count,
            "avg_position":    round(avg_pos, 1) if avg_pos else "—",
            "total_impressions": int(total_imp),
            "freshness_signal":round(avg_signal, 2),
            "top_keywords":    " | ".join(
                group.sort_values("impressions", ascending=False)
                ["keyword"].head(8).tolist()
            ),
            "recency_evidence":"; ".join(sample_titles[:3]),
            "recommended_action": (
                "Update with latest clinical data, trial results, or recent publications. "
                "Add publication date and 'Last updated' tag. "
                "Refresh statistics and cited studies."
            ),
        })

    if not fresh_clusters:
        print("    → No freshness-sensitive clusters detected")
        return pd.DataFrame()

    fresh_df = (
        pd.DataFrame(fresh_clusters)
        .sort_values("refresh_score", ascending=False)
        .reset_index(drop=True)
    )
    fresh_df.index += 1
    fresh_df.index.name = "priority"

    path = os.path.join(output_dir, "freshness_priority.csv")
    fresh_df.to_csv(path)

    urgent = (fresh_df["urgency"] == "🔴 Urgent").sum()
    print(f"    → {len(fresh_df)} freshness-sensitive clusters | "
          f"{urgent} urgent → {path}")

    return fresh_df


# ═══════════════════════════════════════════════════════════════════════
# 14. TOPIC CLUSTER MAPPER (HUB & SPOKE)
# ═══════════════════════════════════════════════════════════════════════

def map_topic_clusters(df: pd.DataFrame, kw_emb: np.ndarray,
                        output_dir: str) -> pd.DataFrame:
    """
    Stage 14: Map keywords into hub-and-spoke topic structures
    for internal linking strategy.

    Why this matters for rankings:
    Internal links signal to Google which pages are authoritative
    on a topic. A hub page on "CAR-T cell therapy" that links to
    spokes on "CAR-T manufacturing", "CAR-T toxicity management",
    "CAR-T vs BiTE" etc. builds topical authority much faster
    than isolated pages.

    This stage identifies:
      HUB candidates — broad keywords with high search volume / impression
                        that could serve as pillar pages linking outward
      SPOKE candidates — specific sub-topic keywords that should link
                         back to the hub
      ORPHANS — keywords that have no natural hub in your current
                keyword set (content gaps for hub pages)

    Output:
      topic_clusters.csv — hub-spoke mapping for each research area
                           ready to inform site architecture and
                           internal linking decisions
    """
    print("\n[14] Mapping hub-and-spoke topic clusters...")

    from sklearn.metrics.pairwise import cosine_similarity

    classified = df[
        df["research_area"] != "Unclassified / General Methods"
    ].copy()

    topic_clusters = []

    for area in classified["research_area"].unique():
        area_df = classified[classified["research_area"] == area].copy()

        if len(area_df) < 5:
            continue

        area_indices = area_df.index.tolist()
        a_emb        = kw_emb[area_indices]

        # ── Identify hub candidates
        # Hubs are broad, high-volume, informational keywords — pillar topic pages.
        # Exclude transactional and navigational keywords (reagents, databases etc)
        # which would otherwise score high on impressions but make terrible hubs.
        area_df["word_count_kw"] = area_df["keyword"].str.split().str.len()

        # Only consider informational intent keywords as hub candidates
        if "keyword_intent" in area_df.columns:
            hub_candidates = area_df[
                area_df["keyword_intent"].isin(["informational", "clinical"])
            ].copy()
        else:
            hub_candidates = area_df.copy()

        # Fallback: if filtering leaves nothing, use all
        if len(hub_candidates) == 0:
            hub_candidates = area_df.copy()

        # Hub score: fewer words + more impressions = broader pillar candidate
        max_imp = hub_candidates["impressions"].max() or 1
        hub_candidates["hub_score"] = (
            (1 / hub_candidates["word_count_kw"].clip(1, 10)) * 0.5 +
            (hub_candidates["impressions"] / max_imp)           * 0.5
        )

        n_hubs   = max(1, min(5, len(area_df) // 10))
        hub_mask = hub_candidates.nlargest(n_hubs, "hub_score")
        spoke_mask = area_df[~area_df.index.isin(hub_mask.index)]

        hub_embs   = a_emb[[area_indices.index(i) for i in hub_mask.index]]

        # For each hub, find its spokes (closest non-hub keywords)
        for hub_idx, hub_row in hub_mask.iterrows():
            hub_emb_vec = kw_emb[hub_idx].reshape(1, -1)

            # Cosine sim from this hub to all spokes in area
            spoke_indices = [area_indices.index(i)
                             for i in spoke_mask.index if i in area_indices]
            if not spoke_indices:
                continue

            spoke_embs = a_emb[spoke_indices]
            sims       = cosine_similarity(hub_emb_vec, spoke_embs)[0]

            spoke_df = spoke_mask.copy()
            spoke_df["hub_similarity"] = sims

            # Direct spokes: sim > 0.6 (clearly related)
            direct_spokes = spoke_df[spoke_df["hub_similarity"] > 0.60]
            # Weak spokes: 0.4-0.6 (loosely related)
            weak_spokes   = spoke_df[
                (spoke_df["hub_similarity"] >= 0.40) &
                (spoke_df["hub_similarity"] <= 0.60)
            ]

            # Identify orphans: spokes with very low similarity to ANY hub
            orphan_threshold = 0.35
            orphan_mask = spoke_df["hub_similarity"] < orphan_threshold

            topic_clusters.append({
                "research_area":       area,
                "hub_keyword":         hub_row["keyword"],
                "hub_impressions":     int(hub_row["impressions"]),
                "hub_position":        round(hub_row["position"], 1)
                                       if hub_row["position"] < 999 else "Not ranking",
                "hub_recommendation":  (
                    "✓ Strengthen as pillar page — expand coverage and add internal links"
                    if hub_row["position"] < 20 else
                    "⚠ Hub not ranking — create or significantly expand this page first"
                ),
                "direct_spoke_count":  len(direct_spokes),
                "direct_spokes":       " | ".join(
                    direct_spokes.sort_values("impressions", ascending=False)
                    ["keyword"].head(10).tolist()
                ),
                "weak_spoke_count":    len(weak_spokes),
                "weak_spokes":         " | ".join(
                    weak_spokes.sort_values("impressions", ascending=False)
                    ["keyword"].head(5).tolist()
                ),
                "orphan_count":        int(orphan_mask.sum()),
                "orphan_keywords":     " | ".join(
                    spoke_df[orphan_mask]["keyword"].head(5).tolist()
                ),
                "internal_link_action": (
                    f"Link {len(direct_spokes)} spoke pages back to this hub. "
                    f"Ensure hub page explicitly covers: "
                    f"{', '.join(direct_spokes['keyword'].head(3).tolist())}."
                ) if len(direct_spokes) > 0 else
                    "No close spokes found — this may be an isolated topic",
            })

    if not topic_clusters:
        print("    → No topic cluster structure identified")
        return pd.DataFrame()

    tc_df = (
        pd.DataFrame(topic_clusters)
        .sort_values(["research_area", "hub_impressions"], ascending=[True, False])
        .reset_index(drop=True)
    )
    tc_df.index += 1
    tc_df.index.name = "cluster_rank"

    path = os.path.join(output_dir, "topic_clusters.csv")
    tc_df.to_csv(path)

    total_orphans = tc_df["orphan_count"].sum()
    print(f"    → {len(tc_df)} hub-spoke clusters mapped | "
          f"{total_orphans} orphaned keywords → {path}")

    print("\n    Hub-spoke summary by research area:")
    for area, grp in tc_df.groupby("research_area"):
        hubs    = len(grp)
        spokes  = grp["direct_spoke_count"].sum()
        orphans = grp["orphan_count"].sum()
        print(f"      {area:<35} {hubs} hubs · {spokes} spokes · {orphans} orphans")

    return tc_df


def generate_visibility_html(wins_df: pd.DataFrame, fresh_df: pd.DataFrame,
                              tc_df: pd.DataFrame, intent_df: pd.DataFrame) -> str:
    """Generate the HTML section for all visibility stages."""

    sections = []

    # ── Page 2 Fast Wins
    if len(wins_df) > 0:
        rows = ""
        for rank, row in wins_df.head(15).iterrows():
            snippet_badge = (
                "<span style='background:#198754;color:white;border-radius:3px;"
                "padding:1px 6px;font-size:0.75em;margin-left:4px'>⭐ Snippet</span>"
                if row.get("snippet_opportunity") else ""
            )
            rows += f"""
            <tr>
              <td style='font-weight:700;text-align:center'>{rank}</td>
              <td>{row['research_area']}</td>
              <td><strong>{row['primary_keyword']}</strong>{snippet_badge}</td>
              <td style='text-align:center'>{row['avg_position']}</td>
              <td style='text-align:center'>{row['keyword_count']}</td>
              <td style='text-align:center'>{row['total_impressions']:,}</td>
              <td style='font-size:0.8em;color:#495057'>{row['optimisation_actions']}</td>
              <td style='text-align:center'>{row['effort']}</td>
            </tr>"""

        sections.append(f"""
  <h2>⚡ Page 2 Fast Wins ({len(wins_df)} clusters)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Keywords ranking 11–20 where targeted content edits could move to page 1.
    Sorted by proximity to page 1 and impression volume.
  </p>
  <table>
    <thead><tr>
      <th>#</th><th>Area</th><th>Primary Keyword</th>
      <th>Avg Pos</th><th>Keywords</th><th>Impressions</th>
      <th>Optimisation Actions</th><th>Effort</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    # ── Freshness Priority
    if len(fresh_df) > 0:
        rows = ""
        for rank, row in fresh_df.head(15).iterrows():
            urgency_colour = {
                "🔴 Urgent": "#dc3545",
                "🟡 Soon":   "#fd7e14",
                "🟢 Watch":  "#198754",
            }.get(row["urgency"], "#6c757d")
            rows += f"""
            <tr>
              <td style='font-weight:700;text-align:center'>{rank}</td>
              <td><span style='color:{urgency_colour}'>{row['urgency']}</span></td>
              <td>{row['research_area']}</td>
              <td><strong>{row['cluster']}</strong></td>
              <td style='text-align:center'>{row['avg_position']}</td>
              <td style='text-align:center'>{row['keyword_count']}</td>
              <td style='font-size:0.78em;color:#555;font-style:italic'>
                {str(row['recency_evidence'])[:80]}...</td>
            </tr>"""

        sections.append(f"""
  <h2>🔄 Freshness Priority ({len(fresh_df)} clusters)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Topics where SERP titles show strong recency signals — existing content
    may be losing ground to newer pages. Refresh these to defend rankings.
  </p>
  <table>
    <thead><tr>
      <th>#</th><th>Urgency</th><th>Area</th><th>Topic</th>
      <th>Your Pos</th><th>Keywords</th><th>Recency Evidence</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    # ── Intent Mismatches
    if "intent_mismatch" in intent_df.columns:
        mismatches = intent_df[intent_df["intent_mismatch"]].copy()
        if len(mismatches) > 0:
            intent_summary = mismatches.groupby(
                ["keyword_intent", "serp_intent"]
            ).size().reset_index(name="count").sort_values("count", ascending=False)

            rows = ""
            for _, row in intent_summary.head(10).iterrows():
                rows += f"""
                <tr>
                  <td>{row['keyword_intent']}</td>
                  <td>→</td>
                  <td>{row['serp_intent']}</td>
                  <td style='text-align:center'>{row['count']:,}</td>
                  <td style='font-size:0.8em;color:#dc3545'>
                    Content targeting this query type unlikely to rank
                  </td>
                </tr>"""

            sections.append(f"""
  <h2>🎯 Intent Mismatches ({len(mismatches):,} keywords)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Keywords where your likely content type doesn't match what Google
    rewards. These are structural ranking barriers — content quality
    alone cannot overcome intent mismatch.
  </p>
  <table>
    <thead><tr>
      <th>Your Content Intent</th><th></th><th>SERP Rewards</th>
      <th>Keywords Affected</th><th>Implication</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    # ── Topic Clusters
    if len(tc_df) > 0:
        rows = ""
        for rank, row in tc_df.head(20).iterrows():
            hub_colour = (
                "#198754" if str(row["hub_position"]) != "Not ranking"
                and float(str(row["hub_position"]).replace("Not ranking","999")) < 20
                else "#dc3545"
            )
            rows += f"""
            <tr>
              <td style='text-align:center;font-weight:700'>{rank}</td>
              <td>{row['research_area']}</td>
              <td><strong style='color:{hub_colour}'>{row['hub_keyword']}</strong></td>
              <td style='text-align:center'>{row['hub_position']}</td>
              <td style='text-align:center'>{row['direct_spoke_count']}</td>
              <td style='text-align:center;color:#dc3545'>{row['orphan_count']}</td>
              <td style='font-size:0.79em;color:#495057'>
                {row['internal_link_action']}</td>
            </tr>"""

        sections.append(f"""
  <h2>🕸 Topic Cluster Map ({len(tc_df)} hubs)</h2>
  <p style='font-size:0.87em;color:#555;margin-bottom:16px'>
    Hub-and-spoke structure for internal linking strategy.
    Green hubs are ranking and can be strengthened.
    Red hubs need to be created or significantly expanded first.
    Orphans are keywords with no natural hub — consider whether a pillar page is missing.
  </p>
  <table>
    <thead><tr>
      <th>#</th><th>Area</th><th>Hub Keyword</th><th>Hub Position</th>
      <th>Spokes</th><th>Orphans</th><th>Action</th>
    </tr></thead>
    <tbody>{rows}</tbody>
  </table>""")

    return "\n".join(sections)


def main():
    args = parse_args()
    os.makedirs(args.output, exist_ok=True)

    # ── Embedding cache directory (shared across all stages)
    global _EMB_CACHE_DIR
    _EMB_CACHE_DIR = args.output

    # ── Stage 1: Load & merge
    df = load_and_merge(args.keywords, args.serp, args.kw_col)

    # ── Stage 2: Enrich text
    df = enrich_text(df)

    # ── Stage 3: Embed
    kw_emb, area_emb, model = embed_all(df, args.output)

    # ── Stage 4: Research area mapping
    df = map_research_areas(df, kw_emb, area_emb, args.threshold)

    # ── Stage 5: Title-first clustering + AIO clustering
    df, titles_df = cluster_keywords(df, kw_emb, args.min_cluster, model)

    # ── Stage 6: Competitor landscape
    df = analyze_competitor_landscape(df)

    # ── Stage 7: Gap analysis
    df = analyze_gaps(df)

    # ── Stage 8: Score & recommend
    area_df, recs = score_and_recommend(df, args.top_n)

    # ── Stage 9: Content briefs (per-area clustering, relevance-gated)
    briefs_df   = generate_content_briefs(
        df, kw_emb, area_emb, model, recs, args.output, n_briefs=50
    )
    # HTML view: top 25 only (full 50 in content_briefs.csv)
    briefs_html = generate_briefs_html(briefs_df.head(25), args.output,
                                        total_briefs=len(briefs_df))

    # ── Stage 10: SERP title mining — content angle gaps (relevance-gated)
    serp_gaps_df, serp_gaps_html = mine_serp_title_gaps(
        df, args.output, model, area_emb, n_gaps=50
    )

    # ── Stage 11: Search intent classification
    df = run_intent_classification(df, model=model)

    # ── Stage 12: Page 2 fast wins
    wins_df = find_page2_fast_wins(df, args.output)

    # ── Stage 13: Freshness priority
    fresh_df = score_freshness_priority(df, args.output)

    # ── Stage 14: Topic cluster mapper
    tc_df = map_topic_clusters(df, kw_emb, args.output)

    # ── Visibility HTML (stages 11-14)
    visibility_html = generate_visibility_html(
        wins_df   if len(wins_df)  > 0 else pd.DataFrame(),
        fresh_df  if len(fresh_df) > 0 else pd.DataFrame(),
        tc_df     if len(tc_df)    > 0 else pd.DataFrame(),
        df,
    )

    # ── Save outputs
    # Drop internal/noise columns from clusters.csv:
    # - enriched_text: large embedding input text, not useful post-pipeline
    # - gap_absence/performance/format/snippet: binary flags replaced by
    #   primary_gap + gap_score; keeping them just creates confusion
    # - academic_lock/serp_diversity: noisy intermediate metrics
    # - winnability: replaced by paa_coverage in opportunity scoring
    drop_cols = [
        "enriched_text",
        "gap_absence", "gap_performance", "gap_format", "gap_snippet",
        "academic_lock", "serp_diversity", "winnability",
    ]
    clusters_path           = os.path.join(args.output, "clusters.csv")
    gaps_path               = os.path.join(args.output, "gaps.csv")
    recs_path               = os.path.join(args.output, "recommendations.csv")
    briefs_path             = os.path.join(args.output, "content_briefs.csv")
    serp_gaps_path          = os.path.join(args.output, "serp_content_gaps.csv")
    wins_path               = os.path.join(args.output, "page2_fast_wins.csv")
    fresh_path              = os.path.join(args.output, "freshness_priority.csv")
    tc_path                 = os.path.join(args.output, "topic_clusters.csv")
    title_cluster_map_path  = os.path.join(args.output, "title_cluster_map.csv")
    title_kw_map_path       = os.path.join(args.output, "keyword_map_title_clusters.csv")
    aio_kw_map_path         = os.path.join(args.output, "keyword_map_aio_clusters.csv")

    # ── Keyword map 1: Title clusters
    # One row per keyword. Shows every title cluster the keyword belongs to,
    # ordered by title position (primary = highest-ranked title's cluster).
    # Useful for: understanding what topics Google associates with each keyword,
    # mapping keywords to content briefs, identifying multi-topic keywords.
    title_kw_map = df[[
        "keyword", "research_area", "impressions", "position",
        "primary_cluster_id", "primary_cluster_label",
        "cluster_ids", "cluster_labels",
    ]].copy()
    title_kw_map.columns = [
        "keyword", "research_area", "impressions", "position",
        "primary_title_cluster_id", "primary_title_cluster_label",
        "all_title_cluster_ids", "all_title_cluster_labels",
    ]
    title_kw_map["cluster_count"] = (
        title_kw_map["all_title_cluster_ids"]
        .str.count(r"\|\|")
        .add(1)
        .where(title_kw_map["all_title_cluster_ids"].str.strip() != "", 0)
    )
    title_kw_map = title_kw_map.sort_values(
        ["research_area", "impressions"], ascending=[True, False]
    )
    title_kw_map.to_csv(title_kw_map_path, index=False)
    print(f"    → keyword_map_title_clusters.csv: {len(title_kw_map):,} keywords")

    # ── Keyword map 2: AIO clusters
    # Only keywords that have AI Overview data.
    # One row per keyword. Shows primary AIO cluster and all AIO clusters
    # if the keyword's snippets span multiple themes.
    # Useful for: understanding what Google synthesises for each topic,
    # identifying where AI Overviews are displacing organic clicks,
    # finding AIO-specific content angles.
    aio_cols_present = [c for c in [
        "keyword", "research_area", "impressions", "position",
        "aio_cluster_id", "aio_cluster_label",
        "aio_cluster_ids", "aio_cluster_labels",
    ] if c in df.columns]

    if len(aio_cols_present) > 4:   # has AIO columns
        aio_kw_map = df[aio_cols_present].copy()
        aio_kw_map = aio_kw_map[
            aio_kw_map.get("aio_cluster_id", pd.Series(-1, index=aio_kw_map.index)) >= 0
        ]
        if len(aio_kw_map) > 0:
            aio_kw_map["aio_cluster_count"] = (
                aio_kw_map.get("aio_cluster_ids", pd.Series("", index=aio_kw_map.index))
                .str.count(r"\|\|")
                .add(1)
            )
            aio_kw_map = aio_kw_map.sort_values(
                ["research_area", "impressions"], ascending=[True, False]
            )
            aio_kw_map.to_csv(aio_kw_map_path, index=False)
            print(f"    → keyword_map_aio_clusters.csv: {len(aio_kw_map):,} keywords with AIO data")
        else:
            print("    → keyword_map_aio_clusters.csv: no AIO data present")
    else:
        print("    → keyword_map_aio_clusters.csv: skipped (no AIO columns)")

    # ── Save title-level cluster mapping (full detail: one row per title)
    if titles_df is not None and len(titles_df) > 0:
        titles_df[["title", "keyword", "cluster_id", "cluster_label"]].to_csv(
            title_cluster_map_path, index=False
        )

    df.drop(columns=drop_cols, errors="ignore").to_csv(clusters_path, index=False)

    gap_cols = ["keyword", "research_area", "research_area_2",
                "primary_cluster_id", "primary_cluster_label",
                "cluster_ids", "cluster_labels",
                "aio_cluster_id", "aio_cluster_label",
                "aio_cluster_ids", "aio_cluster_labels",
                "cluster_label",
                "primary_gap", "gap_score",
                "gap_paa", "paa_questions",
                "impressions", "clicks", "position", "ctr",
                "dominant_format"]
    gap_cols = [c for c in gap_cols if c in df.columns]
    (
        df[(df["primary_gap"] != "None") & (df["gap_score"] >= 0.15)][gap_cols]
        .sort_values("gap_score", ascending=False)
        .to_csv(gaps_path, index=False)
    )

    recs.to_csv(recs_path)
    generate_report(df, recs, args.output,
                    kw_emb=kw_emb,
                    briefs_html=briefs_html,
                    serp_gaps_html=serp_gaps_html,
                    visibility_html=visibility_html)

    print("\n" + "═" * 62)
    print("✅  Pipeline complete!")
    print(f"   clusters.csv                    → {clusters_path}")
    print(f"   title_cluster_map.csv           → {title_cluster_map_path}")
    print(f"   keyword_map_title_clusters.csv  → {title_kw_map_path}")
    print(f"   keyword_map_aio_clusters.csv    → {aio_kw_map_path}")
    print(f"   gaps.csv                        → {gaps_path}")
    print(f"   recommendations.csv             → {recs_path}")
    print(f"   content_briefs.csv              → {briefs_path}")
    print(f"   serp_content_gaps.csv           → {serp_gaps_path}")
    print(f"   page2_fast_wins.csv             → {wins_path}")
    print(f"   freshness_priority.csv          → {fresh_path}")
    print(f"   topic_clusters.csv              → {tc_path}")
    print(f"   report.html                     → {os.path.join(args.output, 'report.html')}")
    print("═" * 62)

    print("\n📋  Top Research Area Recommendations:\n")
    show_cols = ["research_area", "keyword_count", "absence_gap_pct",
                 "performance_gap_pct", "paa_coverage", "imp_weighted_gap",
                 "opportunity_score", "priority", "recommended_action",
                 "median_position", "pct_page2", "top3_imp_share", "aio_coverage_pct"]
    show_cols = [c for c in show_cols if c in recs.columns]
    print(recs[show_cols].to_string())
    print("═" * 62)


if __name__ == "__main__":
    main()


# ═══════════════════════════════════════════════════════════════════════
# EXPECTED INPUT FORMATS
# ═══════════════════════════════════════════════════════════════════════
#
# keywords.csv
# ────────────
# Required:  keyword, impressions, clicks, position
# Optional:  volume  (SEMrush/GSC search volume — improves demand scoring)
#
#   keyword,impressions,clicks,position
#   CAR-T cell therapy,3100,280,4.1
#   PD-1 checkpoint,1800,95,14.7
#
#
# serp_data.csv
# ─────────────
# Required:  keyword, title_1, title_2, title_3
# Optional:  paa_1, paa_2, paa_3   (People Also Ask questions)
#            snippet                (Featured snippet text)
#
# NOTE: URLs and dates are NOT needed and should be excluded.
#       Titles carry all the signal needed for academic lock-in
#       detection, format classification, and enriched embedding.
#
#   keyword,title_1,title_2,title_3,paa_1,paa_2,snippet
#   CAR-T cell therapy,CAR-T Cell Therapy Overview | NCI,How CAR-T Works,CAR-T Side Effects,What cancers does CAR-T treat?,How long does CAR-T last?,CAR-T therapy uses a patient's own T cells to fight cancer.
#
#
# Run:
#   python lifesci_gap_pipeline.py \
#     --keywords keywords.csv \
#     --serp     serp_data.csv \
#     --output   ./results
#
# Optional flags:
#   --threshold 0.20    min cosine similarity to assign a research area
#   --min-cluster 3     min keywords per sub-topic cluster
#   --top-n 9           show all 9 research areas in recommendations
#   --kw-col query      if your keyword column has a different name
# ═══════════════════════════════════════════════════════════════════════
